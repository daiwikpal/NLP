{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daiwikpal/NLP/blob/main/CS4650_p1_dpal32.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnnwupedFEIV"
      },
      "source": [
        "# Headline Classification with Neural Bag of Words\n",
        "**CS 4650 \"Natural Language Processing\" Project 1**  \n",
        "Georgia Tech, Spring 2025\n",
        "\n",
        "(Instructor: Prof. Wei Xu; TAs: Yao Dou,  Tarek Naous, Xiaofeng Wu, Jonathan Zheng)\n",
        "\n",
        "Welcome to the first full programming project for CS 4650! **To start, first make a copy of this notebook to your local drive, so you can edit it.**\n",
        "\n",
        "If you want GPUs (which will improve training times), you can always change your instance type by going to Runtime -> Change runtime type -> Hardware accelerator.\n",
        "\n",
        "**In this project, we will be using PyTorch.** If you are new to PyTorch, or simply want a refresher, we recommend you start by looking through these [Introduction to PyTorch](https://cocoxu.github.io/CS4650_spring2025/slides/PyTorch_tutorial.pdf) slides and this interactive [PyTorch basics notebook](http://bit.ly/pytorchbasics). Additionally, this [text sentiment](http://bit.ly/pytorchexample) notebook will provide some insight into working with PyTorch with a specific NLP task."
      ],
      "id": "JnnwupedFEIV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Honor Code  [1 points]\n",
        "\n",
        "**Honor Code:** I hereby agree to abide the Georgia Tech's Academic Honor Code, promise that the submitted assignment is my own work, and understand that my code is subject to plagiarism test.\n",
        "\n",
        "**Signature**: Daiwik Pal"
      ],
      "metadata": {
        "id": "BMgET_OMuuNZ"
      },
      "id": "BMgET_OMuuNZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRdPqeMMFEIY"
      },
      "source": [
        "## 1. Load and preprocess data [9 points]\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n",
        "\n",
        "This project will be modeling a *classification task* for headlines from [The Onion](https://www.theonion.com), a satirical news website. Our dataset contains headlines and whether they belong to The Onion or CNN. Given a headline, we want to predict whether it is Onion or not.\n",
        "\n",
        "The following cell loads, pre-processes and tokenizes our OnionOrNot dataset."
      ],
      "id": "ZRdPqeMMFEIY"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YmV_uknBJA-o"
      },
      "outputs": [],
      "source": [
        "!curl -so OnionOrNot.csv https://raw.githubusercontent.com/lukefeilberg/onion/master/OnionOrNot.csv"
      ],
      "id": "YmV_uknBJA-o"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "L3DkMDu7FEIZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0431973-e58c-44c8-a015-43d47de38598"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n"
          ]
        }
      ],
      "source": [
        "# ===========================================================================\n",
        "# Run some setup code for this notebook. Don't modify anything in this cell.\n",
        "# ===========================================================================\n",
        "\n",
        "import torch\n",
        "import random, sys\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "\n",
        "# ===========================================================================\n",
        "# A quick note on CUDA functionality (and `.to(model.device)`):\n",
        "# CUDA is a parallel GPU platform produced by NVIDIA and is used by most GPU\n",
        "# libraries in PyTorch. CUDA organizes GPUs into device IDs (i.e., \"cuda:X\" for GPU #X).\n",
        "# \"device\" will tell PyTorch which GPU (or CPU) to place an object in. Since\n",
        "# collab only uses one GPU, we will use 'cuda' as the device if a GPU is available\n",
        "# and the CPU if not. You will run into problems if your tensors are on different devices.\n",
        "# ===========================================================================\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Check what version of Python is running\n",
        "print(sys.version)"
      ],
      "id": "L3DkMDu7FEIZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Fulh0MZ8y8b"
      },
      "source": [
        "### 1.1 Dataset preprocessing functions\n",
        "The following cell define some methods to clean the dataset, but feel free to take a look to see some of the operations it's doing.\n"
      ],
      "id": "0Fulh0MZ8y8b"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ctNnE1Ui8oKw"
      },
      "outputs": [],
      "source": [
        "# ===========================================================================\n",
        "# Run some preprocessing code for our dataset. Don't modify anything in this cell.\n",
        "# This code was adapted from fast-bert.\n",
        "# ===========================================================================\n",
        "\n",
        "import re\n",
        "import html\n",
        "\n",
        "def spec_add_spaces(t: str) -> str:\n",
        "    \"Add spaces around / and # in `t`. \\n\"\n",
        "    return re.sub(r\"([/#\\n])\", r\" \\1 \", t)\n",
        "\n",
        "def rm_useless_spaces(t: str) -> str:\n",
        "    \"Remove multiple spaces in `t`.\"\n",
        "    return re.sub(\" {2,}\", \" \", t)\n",
        "\n",
        "def replace_multi_newline(t: str) -> str:\n",
        "    return re.sub(r\"(\\n(\\s)*){2,}\", \"\\n\", t)\n",
        "\n",
        "def fix_html(x: str) -> str:\n",
        "    \"List of replacements from html strings in `x`.\"\n",
        "    re1 = re.compile(r\"  +\")\n",
        "    x = (\n",
        "        x.replace(\"#39;\", \"'\")\n",
        "        .replace(\"amp;\", \"&\")\n",
        "        .replace(\"#146;\", \"'\")\n",
        "        .replace(\"nbsp;\", \" \")\n",
        "        .replace(\"#36;\", \"$\")\n",
        "        .replace(\"\\\\n\", \"\\n\")\n",
        "        .replace(\"quot;\", \"'\")\n",
        "        .replace(\"<br />\", \"\\n\")\n",
        "        .replace('\\\\\"', '\"')\n",
        "        .replace(\" @.@ \", \".\")\n",
        "        .replace(\" @-@ \", \"-\")\n",
        "        .replace(\" @,@ \", \",\")\n",
        "        .replace(\"\\\\\", \" \\\\ \")\n",
        "    )\n",
        "    return re1.sub(\" \", html.unescape(x))\n",
        "\n",
        "def clean_text(input_text):\n",
        "    text = fix_html(input_text)\n",
        "    text = replace_multi_newline(text)\n",
        "    text = spec_add_spaces(text)\n",
        "    text = rm_useless_spaces(text)\n",
        "    text = text.strip()\n",
        "    return text"
      ],
      "id": "ctNnE1Ui8oKw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiUlTSBB9Wx6"
      },
      "source": [
        "### 1.2 Tokenize using NLTK\n",
        "\n",
        "We will use our rule-based `clean_text` function to clean our raw text, then use the popular NLTK [punkt tokenizer](https://www.nltk.org/_modules/nltk/tokenize/punkt.html) to convert text to individual sub-words. This will take a while because you have to download the pre-trained punkt tokenizer.\n",
        "\n",
        "*If you are interested: There's a [long and diverse history of converting raw text to \"tokens\"](https://arxiv.org/abs/2112.10508), and many available methods/algorithms (you can experiment with some recently trained ones, trained on a dynamic programming-based method called BPE, [here](https://huggingface.co/spaces/Xenova/the-tokenizer-playground)).*"
      ],
      "id": "MiUlTSBB9Wx6"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vqtdrhF8FEIZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f50c1f2-f90d-4ac3-e24a-37f795fd5292"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "# ===========================================================================\n",
        "# Tokenize using punkt. Don't modify anything in this cell.\n",
        "# ===========================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from tqdm import tqdm\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "df = pd.read_csv(\"OnionOrNot.csv\")\n",
        "df[\"tokenized\"] = df[\"text\"].apply(lambda x: nltk.word_tokenize(clean_text(x.lower())))"
      ],
      "id": "vqtdrhF8FEIZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBBdVOYxFEIa"
      },
      "source": [
        "We will use `pandas`, a popular library for data analysis and table manipulation, in this project to manage the dataset. For more information on usage, please refer to the [Pandas documentation](https://pandas.pydata.org/docs/).\n",
        "\n",
        "The primary data structure in Pandas is a `DataFrame`. The following cell will print out the basic information contained in our `DataFrame` structure, and the first few rows of our dataset."
      ],
      "id": "qBBdVOYxFEIa"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "sJjScqV3FEIb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "3cf5274a-e897-41a5-d662-62b748a413bd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  label  \\\n",
              "0  Entire Facebook Staff Laughs As Man Tightens P...      1   \n",
              "1  Muslim Woman Denied Soda Can for Fear She Coul...      0   \n",
              "2  Bold Move: Hulu Has Announced That They’re Gon...      1   \n",
              "3  Despondent Jeff Bezos Realizes He’ll Have To W...      1   \n",
              "4  For men looking for great single women, online...      1   \n",
              "\n",
              "                                           tokenized  \n",
              "0  [entire, facebook, staff, laughs, as, man, tig...  \n",
              "1  [muslim, woman, denied, soda, can, for, fear, ...  \n",
              "2  [bold, move, :, hulu, has, announced, that, th...  \n",
              "3  [despondent, jeff, bezos, realizes, he, ’, ll,...  \n",
              "4  [for, men, looking, for, great, single, women,...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7eea0c2d-ad3c-4cd6-8308-2e10be02b67a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>tokenized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Entire Facebook Staff Laughs As Man Tightens P...</td>\n",
              "      <td>1</td>\n",
              "      <td>[entire, facebook, staff, laughs, as, man, tig...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Muslim Woman Denied Soda Can for Fear She Coul...</td>\n",
              "      <td>0</td>\n",
              "      <td>[muslim, woman, denied, soda, can, for, fear, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Bold Move: Hulu Has Announced That They’re Gon...</td>\n",
              "      <td>1</td>\n",
              "      <td>[bold, move, :, hulu, has, announced, that, th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Despondent Jeff Bezos Realizes He’ll Have To W...</td>\n",
              "      <td>1</td>\n",
              "      <td>[despondent, jeff, bezos, realizes, he, ’, ll,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>For men looking for great single women, online...</td>\n",
              "      <td>1</td>\n",
              "      <td>[for, men, looking, for, great, single, women,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7eea0c2d-ad3c-4cd6-8308-2e10be02b67a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7eea0c2d-ad3c-4cd6-8308-2e10be02b67a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7eea0c2d-ad3c-4cd6-8308-2e10be02b67a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0cb47e71-7ef4-49ea-8294-0ba20da7060b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0cb47e71-7ef4-49ea-8294-0ba20da7060b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0cb47e71-7ef4-49ea-8294-0ba20da7060b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 24000,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 23998,\n        \"samples\": [\n          \"Freeloading Refugee Children Taking Up Thousands Of Prison Cells Meant For Real Americans\",\n          \"Life: Finally Bought A Jackhammer? Don\\u2019t Make These 6 Rookie Mistakes\",\n          \"Apple 1 computer worth $200K left at recycling centre\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tokenized\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# View the first few entries of our dataset\n",
        "df.head()"
      ],
      "id": "sJjScqV3FEIb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9b4W9z1XhgS"
      },
      "source": [
        "Try to guess some examples! Is the task more difficult than you expected?\n",
        "\n",
        "DataFrames can be indexed using [`.iloc[]`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html). `iloc` uses interger based indexing and supports a single integer (`df.iloc[42]`), a list of integers (`df.iloc[[1, 5, 42]]`), or a slice (`df.iloc[7:42]`)."
      ],
      "id": "D9b4W9z1XhgS"
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "Ntm8laX6FEIb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16bec74c-ca37-4d84-bcbc-299daeb19b77"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24000, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "# E.g., get row 42 of our dataset\n",
        "df.iloc[42]\n",
        "df.shape"
      ],
      "id": "Ntm8laX6FEIb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQVT6HUA9htQ"
      },
      "source": [
        "### 1.3 Split the dataset into training, validation, and testing"
      ],
      "id": "TQVT6HUA9htQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDI72x8XFEIc"
      },
      "source": [
        "**Train/Test/Val Split** - Now that we've loaded this dataset, we need to split the data into train, validation, and test sets.\n",
        "\n",
        "A good explanation of why we need these different sets can be found in $\\S$2.2.5 of [Eisenstein](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf) but our high-level goal is to have a generalized model and have confidence in our results.\n",
        "\n",
        "\n",
        "The *training set* is used to fit our model's learned parameters (weights and biases) to the task. The *validation  set* (sometimes called development set) is used to verify our training jobs are minimizing loss on an unseen subset of the data and can also be used to help choose hyperparameters for our training setup. The *test set* is used to provide a final evaluation of our trained model (unbiased by development or training decisions), ideally providing some insight into how the model will perform in a scenario we cannot perfectly represent in our data (i.e., the real world). *Each of these sets should be disjoint from the others*, to prevent any leackage that could introduce bias in our evaluation metrics (in this case accuracy).\n",
        "\n",
        "**Model Vocabulary** - We cannot directly feed sub-word token strings into a model! We need to create a \"vocab map\", which contains an ID for each unique token in our Onion dataset. This will be used as a \"lookup\" in the next few sections, since your PyTorch implementation will require first converting your Onion token representations to a list of sub-word IDs.\n",
        "\n",
        "**In the following cell, please implement `split_train_val_test` and `generate_vocab_map`.**"
      ],
      "id": "GDI72x8XFEIc"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zeo9kX6i9pbH"
      },
      "outputs": [],
      "source": [
        "# ===========================================================================\n",
        "# Set constants for PAD and UNK. You will use these values, but DO NOT change\n",
        "# them, or import additional packages.\n",
        "\n",
        "from collections import Counter\n",
        "PADDING_VALUE = 0\n",
        "UNK_VALUE     = 1\n",
        "\n",
        "# ===========================================================================\n",
        "\n",
        "\n",
        "def split_train_val_test(df, props=[.8, .1, .1]):\n",
        "    \"\"\"\n",
        "    This method takes a dataframe and splits it into train/val/test splits.\n",
        "    It uses the props argument to split the dataset appropriately.\n",
        "\n",
        "    Args:\n",
        "      df (pd.DataFrame): A dataset as a Pandas DataFrame\n",
        "      props (list): Proportions for each split in the order of [train, validation, test].\n",
        "                    the last value of the props array is repetitive, but we've kept it for clarity.\n",
        "\n",
        "    Returns:\n",
        "      train_df (pd.DataFrame): Train DataFrame split.\n",
        "      val_df (pd.DataFrame): Validation DataFrame split.\n",
        "      test_df (pd.DataFrame): Test DataFramem split.\n",
        "    \"\"\"\n",
        "    assert round(sum(props), 2) == 1 and len(props) >= 2\n",
        "    train_df, test_df, val_df = None, None, None\n",
        "\n",
        "    ### BEGIN YOUR CODE (~3-5 lines) ###\n",
        "    ### Hint: You can use df.iloc to slice into specific indexes or ranges.\n",
        "\n",
        "    # Get length of dataframe\n",
        "    df_len = len(df)\n",
        "\n",
        "    # Get end location for training and validation (don't need test since that's the rest of the dataframe after val)\n",
        "    train_end = int(df_len * props[0])\n",
        "    val_end = int(df_len * (props[0] + props[1]))\n",
        "\n",
        "    # Split the dataframe based on the stored locations\n",
        "    train_df = df.iloc[:train_end]\n",
        "    val_df = df.iloc[train_end:val_end]\n",
        "    test_df = df.iloc[val_end:]\n",
        "\n",
        "    ### END YOUR CODE ###\n",
        "\n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "\n",
        "def generate_vocab_map(df, cutoff=2):\n",
        "    \"\"\"\n",
        "    This method takes a dataframe and builds a vocabulary to unique number map.\n",
        "    It uses the cutoff argument to remove rare words occuring <= cutoff times.\n",
        "    *NOTE*: \"\" and \"UNK\" are reserved tokens in our vocab that will be useful\n",
        "    later. You'll also find the Counter imported for you to be useful as well.\n",
        "\n",
        "    Args:\n",
        "      df (pd.DataFrame): The entire dataset this mapping is built from\n",
        "      cutoff (int): We exclude words from the vocab that appear less than or\n",
        "                    eq to cutoff\n",
        "\n",
        "    Returns:\n",
        "      vocab (dict[str, int]):\n",
        "        In vocab, each str is a unique token, and each dict[str] is a\n",
        "        unique integer ID. Only elements that appear > cutoff times appear\n",
        "        in vocab.\n",
        "\n",
        "      reversed_vocab (dict[int, str]):\n",
        "        A reversed version of vocab, which allows us to retrieve\n",
        "        words given their unique integer ID. This map will\n",
        "        allow us to \"decode\" integer sequences we'll encode using\n",
        "        vocab!\n",
        "    \"\"\"\n",
        "\n",
        "    vocab          = {\"\": PADDING_VALUE, \"UNK\": UNK_VALUE}\n",
        "    reversed_vocab = {}\n",
        "\n",
        "    ### BEGIN YOUR CODE (~5-15 lines) ###\n",
        "    ### Hint: Start by iterating over df[\"tokenized\"]\n",
        "\n",
        "    allTokens = []\n",
        "\n",
        "    # Make a dictionary of words and their counts\n",
        "    for tokens in df[\"tokenized\"]:\n",
        "      for token in tokens:\n",
        "        allTokens.append(token)\n",
        "\n",
        "    tokenCounter = Counter(allTokens)\n",
        "\n",
        "    # Start iterator for unique word IDs\n",
        "    idCounter = 2\n",
        "\n",
        "    # For each word in the dictionary, if it meets the minimum frequency add it and increase the word ID counter\n",
        "    for token in tokenCounter:\n",
        "      if tokenCounter[token] > cutoff:\n",
        "        vocab[token] = idCounter\n",
        "        idCounter += 1\n",
        "\n",
        "    # Create a new dictionary of the flipped values\n",
        "\n",
        "    for token, wordID in vocab.items():\n",
        "       reversed_vocab[wordID] = token\n",
        "\n",
        "    ### END YOUR CODE ###\n",
        "\n",
        "    return vocab, reversed_vocab"
      ],
      "id": "zeo9kX6i9pbH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9LEk83hRFgT"
      },
      "source": [
        "With the methods you have implemented above, we can now split the dataset into training, validation, and testing sets and generate our dictionaries mapping from word tokens to IDs (and vice versa).\n",
        "\n",
        "*Note: The props list currently being used splits the dataset so that 80% of samples are used to train, and the remaining 20% are evenly split between training and validation. How you split your dataset is itself a major choice and something you would need to consider in your own projects. Can you think of why?*"
      ],
      "id": "w9LEk83hRFgT"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rcmX931OFEId"
      },
      "outputs": [],
      "source": [
        "df                         = df.sample(frac=1)\n",
        "train_df, val_df, test_df  = split_train_val_test(df, props=[.8, .1, .1])\n",
        "train_vocab, reverse_vocab = generate_vocab_map(train_df, 1)"
      ],
      "id": "rcmX931OFEId"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "CAACzA8YFEId",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70dd6e7e-4996-4c5f-ed76-6d9565f0ee97"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8, 0.1, 0.1)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# ===========================================================================\n",
        "# This line of code will help test your implementation, the expected output is\n",
        "# the same distribution used in 'props' in the above cell. Try out some\n",
        "# different values to ensure it works, but for submission ensure you use\n",
        "# [.8, .1, .1]\n",
        "# ===========================================================================\n",
        "\n",
        "(len(train_df) / len(df)), (len(val_df) / len(df)), (len(test_df) / len(df))"
      ],
      "id": "CAACzA8YFEId"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fCFfEHv1hnI"
      },
      "source": [
        "### 1.4 Building a Dataset Class"
      ],
      "id": "5fCFfEHv1hnI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-qTQQa2FEIe"
      },
      "source": [
        "PyTorch has custom Dataset Classes that have very useful extentions, we want to turn our current pandas DataFrame into a subclass of Dataset so that we can iterate and sample through it for minibatch updates. **In the following cell, fill out the `HeadlineDataset` class.** Refer to PyTorch documentation on [Dataset Classes](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n",
        "for help."
      ],
      "id": "8-qTQQa2FEIe"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tqt9q92J1QKK"
      },
      "outputs": [],
      "source": [
        "# ===========================================================================\n",
        "# Please do not change, or import additional packages.\n",
        "from torch.utils.data import Dataset\n",
        "# ===========================================================================\n",
        "\n",
        "class HeadlineDataset(Dataset):\n",
        "  \"\"\"\n",
        "  This class takes a Pandas DataFrame and wraps in a PyTorch Dataset.\n",
        "  Read more about Torch Datasets here:\n",
        "  https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, vocab, df, max_length=50):\n",
        "    \"\"\"\n",
        "    Initialize this class with appropriate instance variables\n",
        "\n",
        "    We would *strongly* recommend storing the dataframe itself as an instance\n",
        "    variable, and keeping this method very simple. Leave processing to\n",
        "    __getitem__.\n",
        "\n",
        "    Sometimes, however, it does make sense to preprocess in __init__. If you\n",
        "    are curious as to why, read the aside at the bottom of this cell.\n",
        "    \"\"\"\n",
        "\n",
        "    ### BEGIN YOUR CODE (~3 lines) ###\\\n",
        "    self.vocab = vocab\n",
        "    self.df  = df\n",
        "    self.MAX_LEN = max_length\n",
        "\n",
        "    ### END YOUR CODE ###\n",
        "\n",
        "  def __len__(self):\n",
        "    \"\"\"\n",
        "    Return the length of the dataframe instance variable\n",
        "    \"\"\"\n",
        "    df_len = self.df.shape[0]\n",
        "\n",
        "    ### BEGIN YOUR CODE (~1 line) ###\n",
        "\n",
        "    ### END YOUR CODE ###\n",
        "\n",
        "    return df_len\n",
        "\n",
        "  def __getitem__(self, index: int):\n",
        "    \"\"\"\n",
        "    Converts a dataframe row (row[\"tokenized\"]) to an encoded torch LongTensor,\n",
        "    using our vocab map created using generate_vocab_map. Restricts the encoded\n",
        "    headline length to max_length.\n",
        "\n",
        "    The purpose of this method is to convert the row - a list of words - into\n",
        "    a corresponding list of numbers.\n",
        "\n",
        "    i.e. using a map of {\"hi\": 2, \"hello\": 3, \"UNK\": 0}\n",
        "    this list [\"hi\", \"hello\", \"NOT_IN_DICT\"] will turn into [2, 3, 0]\n",
        "\n",
        "    Returns:\n",
        "      tokenized_word_tensor (torch.LongTensor):\n",
        "        A 1D tensor of type Long, that has each token in the dataframe mapped to\n",
        "        a number. These numbers are retrieved from the vocab_map we created in\n",
        "        generate_vocab_map.\n",
        "\n",
        "        **IMPORTANT**: if we filtered out the word because it's infrequent (and\n",
        "        it doesn't exist in the vocab) we need to replace it w/ the UNK token.\n",
        "\n",
        "      curr_label (int):\n",
        "        Binary 0/1 label retrieved from the DataFrame.\n",
        "\n",
        "    \"\"\"\n",
        "    tokenized_word_tensor = None\n",
        "    curr_label            = None\n",
        "\n",
        "    ### BEGIN YOUR CODE (~3-7 lines) ###\n",
        "\n",
        "    # Get the first MAX_LENGTH words in the headline, if it doesn't exist in the vocab replace it with UNK\n",
        "    row = self.df.iloc[index]\n",
        "    headline_array = row[\"tokenized\"][:self.MAX_LEN]\n",
        "\n",
        "    for i, word in enumerate(headline_array):\n",
        "      if word not in self.vocab:\n",
        "        headline_array[i] = \"UNK\"\n",
        "\n",
        "\n",
        "    # Get the ID of each word (or UNK) in the cleaned list for the headline\n",
        "    tokens_array = []\n",
        "    for word in headline_array:\n",
        "      tokens_array.append(self.vocab[word])\n",
        "\n",
        "    tokenized_word_tensor = torch.tensor(tokens_array, dtype=torch.long)\n",
        "\n",
        "    # Grab the label for the current index\n",
        "    curr_label = int(row[\"label\"])\n",
        "\n",
        "    ### END YOUR CODE ###\n",
        "\n",
        "    return tokenized_word_tensor, curr_label\n",
        "\n",
        "\n",
        "# ===========================================================================\n",
        "# Completely optional aside on preprocessing in __init__.\n",
        "#\n",
        "# Sometimes the compute bottleneck actually ends up being in __getitem__.\n",
        "# In this case, you'd loop over your dataset in __init__, passing data\n",
        "# to __getitem__ and storing it in another instance variable. Then,\n",
        "# you can simply return the preprocessed data in __getitem__ instead of\n",
        "# doing the preprocessing.\n",
        "#\n",
        "# There is a tradeoff though: can you think of one?\n",
        "# ==========================================================================="
      ],
      "id": "tqt9q92J1QKK"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "KuLtIOAZFEIe"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import RandomSampler\n",
        "\n",
        "train_dataset = HeadlineDataset(train_vocab, train_df)\n",
        "val_dataset   = HeadlineDataset(train_vocab, val_df)\n",
        "test_dataset  = HeadlineDataset(train_vocab, test_df)\n",
        "\n",
        "\n",
        "# Now that we're wrapping our dataframes in PyTorch datsets, we can make use of\n",
        "# PyTorch Random Samplers, they'll define how our DataLoaders sample elements\n",
        "# from the HeadlineDatasets\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "val_sampler   = RandomSampler(val_dataset)\n",
        "test_sampler  = RandomSampler(test_dataset)"
      ],
      "id": "KuLtIOAZFEIe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9iBiSKF1yXA"
      },
      "source": [
        "### 1.5 Finalizing our DataLoader"
      ],
      "id": "n9iBiSKF1yXA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfXSbxoFFEIe"
      },
      "source": [
        "We can now use PyTorch `DataLoader` to batch our data for us. **In the following cell, please implement `collate_fn`.** Refer to PyTorch documentation on [`DataLoader`](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) for help."
      ],
      "id": "lfXSbxoFFEIe"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Zp1aQAvn1_mz"
      },
      "outputs": [],
      "source": [
        "# ===========================================================================\n",
        "# Please do not change, or import additional packages.\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "# ===========================================================================\n",
        "\n",
        "def collate_fn(batch, padding_value=PADDING_VALUE):\n",
        "  \"\"\"\n",
        "  This function is passed as a parameter to Torch DataSampler. collate_fn collects\n",
        "  batched rows, in the form of tuples, from a DataLoader and applies some final\n",
        "  pre-processing.\n",
        "\n",
        "  Objective:\n",
        "    In our case, we need to take the batched input array of 1D tokenized_word_tensors,\n",
        "    and create a 2D tensor that's padded to be the max length from all our tokenized_word_tensors\n",
        "    in a batch. We're moving from a Python array of tuples, to a padded 2D tensor.\n",
        "\n",
        "    *HINT*: you're allowed to use torch.nn.utils.rnn.pad_sequence (ALREADY IMPORTED)\n",
        "\n",
        "    Finally, you can read more about collate_fn here: https://pytorch.org/docs/stable/data.html\n",
        "\n",
        "  Args:\n",
        "    batch: PythonArray[tuple(tokenized_word_tensor: 1D Torch.LongTensor, curr_label: int)]\n",
        "           len(batch) == BATCH_SIZE\n",
        "\n",
        "  Returns:\n",
        "    padded_tokens: 2D LongTensor of shape (BATCH_SIZE, max len of all tokenized_word_tensor))\n",
        "    y_labels: 1D FloatTensor of shape (BATCH_SIZE)\n",
        "\n",
        "  \"\"\"\n",
        "  padded_tokens, y_labels = None, None\n",
        "\n",
        "  ### BEGIN YOUR CODE (~4-8 lines) ###\n",
        "\n",
        "  # Pad each sequence with the given value to the longest in the batch\n",
        "  sequences = [row[0] for row in batch]\n",
        "  padded_tokens = pad_sequence(sequences, batch_first=True, padding_value=padding_value)\n",
        "  padded_tokens.transpose(1, 0)\n",
        "  # Create a tensor of labels for the batch, ground truth\n",
        "  y_labels = [row[1] for row in batch]\n",
        "  y_labels = torch.tensor(y_labels, dtype=torch.float)\n",
        "\n",
        "  ### END YOUR CODE ###\n",
        "\n",
        "  return padded_tokens, y_labels"
      ],
      "id": "Zp1aQAvn1_mz"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "OayoJRTeFEIf"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, collate_fn=collate_fn)\n",
        "val_iterator   = DataLoader(val_dataset, batch_size=BATCH_SIZE, sampler=val_sampler, collate_fn=collate_fn)\n",
        "test_iterator  = DataLoader(test_dataset, batch_size=BATCH_SIZE, sampler=test_sampler, collate_fn=collate_fn)"
      ],
      "id": "OayoJRTeFEIf"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "pidbg12AFEIf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98a2d6a6-a667-4864-9ef0-63098b65ace5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1140,  1394,   250,  2440,    62,   237,    45,  9371,   604,    78,\n",
            "          6279,  9788,   423,   790,    32,   696,  3647,  3648,     0,     0,\n",
            "             0],\n",
            "        [ 1692,   106,    91,   699,   281,   523,   585,     1,  2492,    34,\n",
            "             1,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0],\n",
            "        [ 7269,    93,  1523,   261,  1655, 10959,  2832,    68,     1,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0],\n",
            "        [ 6018,   461,  7067,    71,    78,   384,    68,    51,    52,  6018,\n",
            "           461,    17,  5661,   183,     0,     0,     0,     0,     0,     0,\n",
            "             0],\n",
            "        [  390,   391,     1,   594,  3175,  8236,    68,   914,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0],\n",
            "        [10483,   184,  1482,    91,  1415,  3713,    71,  1047,   155,   222,\n",
            "            46,   229,   504,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0],\n",
            "        [  275,    45,   139,  8331,     1,    34,  1983,  9399,  1386,    62,\n",
            "           377,    32,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0],\n",
            "        [ 4307,    28,   522,    36,   434,   975,    46,  5078,   596,    78,\n",
            "          6778,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0],\n",
            "        [    1,  1134,    20,   803,  1357,    34,  5070,  1613,    62,  7410,\n",
            "            20,    46,   108, 12580,     0,     0,     0,     0,     0,     0,\n",
            "             0],\n",
            "        [    1,     1,  1182, 10472,    71,  7880,  9477,    20,  1323, 11858,\n",
            "           375,    52,  2058,    71,     1,     0,     0,     0,     0,     0,\n",
            "             0],\n",
            "        [  445,  1848,  1597,   883,   229,   335,   258,    34,    78,     1,\n",
            "           522,   514,    32,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0],\n",
            "        [  207,   423,   271,    20,   121,     1,   155,   126,    20,    36,\n",
            "          9276,  8997,    66,    23,   112,  1491,    46,   607,   258,    71,\n",
            "         10156],\n",
            "        [    1,  4696,   250,  4292,  1169,   908,    34,  3298, 11733,  1291,\n",
            "           803,    32,   233,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0],\n",
            "        [ 3136,  4041,  2210,     1,  3321,    20,   966,  7126,   155,   314,\n",
            "            52,  2111,  7432,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0],\n",
            "        [ 1216, 12223, 12224,  1413,     1,  3243,  1542,    17,  1947,   315,\n",
            "            46,  1303,  1971,    46,  2479,    34, 13188,     0,     0,     0,\n",
            "             0],\n",
            "        [ 2077,  4482,  5429,  1864,    46,   320,   458,   299,    62,   514,\n",
            "          1201,    34,  5599,  7284,     0,     0,     0,     0,     0,     0,\n",
            "             0]]) tensor([0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1.])\n",
            "x: torch.Size([16, 21])\n",
            "y: torch.Size([16])\n"
          ]
        }
      ],
      "source": [
        "# ===========================================================================\n",
        "# Use this to test your collate_fn implementation.\n",
        "#\n",
        "# You can look at the shapes of x and y or put print statements in collate_fn\n",
        "# while running this snippet\n",
        "# ===========================================================================\n",
        "\n",
        "for x, y in test_iterator:\n",
        "    print(x, y)\n",
        "    print(f'x: {x.shape}')\n",
        "    print(f'y: {y.shape}')\n",
        "    break\n",
        "test_iterator = DataLoader(test_dataset, batch_size=BATCH_SIZE, sampler=test_sampler, collate_fn=collate_fn)"
      ],
      "id": "pidbg12AFEIf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWLK7T1uFEIg"
      },
      "source": [
        "## 2. Modeling [10 points]\n",
        "Now that we have a clean dataset and a useful PyTorch `DataLoader` object, we can begin building a model for our task! In the following code block, you will build a feed-forward neural network implementing a neural bag-of-words baseline, `NBOW-RAND`, described in $\\S$2.1 of [this paper](https://www.aclweb.org/anthology/P15-1162.pdf). You may find [the PyTorch `torch.nn` docs](https://pytorch.org/docs/stable/nn.html) useful for understanding the different layers and [this PyTorch sequence models tutorial](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html) for how to put together `torch.nn` layers.\n",
        "\n",
        "The core intuition behind `NBOW-RAND` is that after we embed each word for our input, we average the embeddings to produce a single vector that hopefully averages the information across all embeddings. Formally, we first convert each document of length $n$ tokens into a matrix of $n\\times d$, where $d$ is the dimension of the token embedding. Then we average all embeddings to produce a vector of length $d$.\n",
        "\n",
        "If you are new to PyTorch, ensuring your matrix operations are correct is often the most common source of errors. Keep in mind how the dimensions change and what each axes represents. Your documents will be passed in as minibatches, so be careful when selecting which axes to apply certain operations. Feel free to experiment with the architecture of this network outside of the basic `NBOW-RAND` setup (such as adding in other layers) to see how this changes your results."
      ],
      "id": "BWLK7T1uFEIg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZDPs0Sf-H3V"
      },
      "source": [
        "### 2.1 Define the NBOW model class"
      ],
      "id": "pZDPs0Sf-H3V"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "jzGx2q0jLqyU"
      },
      "outputs": [],
      "source": [
        "# ===========================================================================\n",
        "# Please do not change, or import additional packages.\n",
        "import torch.nn as nn\n",
        "# ===========================================================================\n",
        "\n",
        "class NBOW(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim):\n",
        "    \"\"\"\n",
        "    Instantiate layers for your model.\n",
        "    Your model architecture will be a feed-forward neural network.\n",
        "\n",
        "    You will need 3 nn.Modules at minimum\n",
        "     1. An embeddings layer (see nn.Embedding)\n",
        "     2. A linear layer (see nn.Linear)\n",
        "     3. A sigmoid output (see nn.Sigmoid)\n",
        "\n",
        "    HINT: In the forward step, the BATCH_SIZE is the first dimension.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    ### BEGIN YOUR CODE (~4 lines) ###\n",
        "\n",
        "    # Initialize embedding layer, turn word ids into representative vectors\n",
        "    self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    # Initilize linear layer, will receive a single vector of length embedding_dim\n",
        "    self.linear = nn.Linear(embedding_dim, 1)\n",
        "\n",
        "    # Intialize sigmoid layer, turn output from linear layer into a probabilistic(ish) score\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    ### END YOUR CODE ###\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Complete the forward pass of the model.\n",
        "\n",
        "    Use the output of the embedding layer to create the average vector,\n",
        "    which will be input into the linear layer.\n",
        "\n",
        "    Args:\n",
        "      x: 2D LongTensor of shape (BATCH_SIZE, max len of all tokenized_word_tensor))\n",
        "         This is the same output that comes out of the collate_fn function you completed\n",
        "    \"\"\"\n",
        "    ### BEGIN YOUR CODE (~4-5 lines) ###\n",
        "\n",
        "    # Embed each document, this produces a tensor of (minibatch, max doc len, embedding size)\n",
        "    embed = self.word_embeddings(x)\n",
        "\n",
        "    # Average over the second dimension, producing a tensor of size (minibatch, embedding size)\n",
        "    average_embeds = torch.mean(embed, dim=1)\n",
        "    # Pass average vector through linear layer\n",
        "    out = self.linear(average_embeds)\n",
        "    out = self.sigmoid(out)\n",
        "\n",
        "    # Get a probabilistic score from sigmoid layer and return scores\n",
        "    return out.squeeze(1)\n",
        "    ### END YOUR CODE ###\n"
      ],
      "id": "jzGx2q0jLqyU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xltosIzM-SP2"
      },
      "source": [
        "### 2.2 Initialize the NBOW classification model\n",
        "\n",
        "Since the NBOW model is rather basic, there is only one meaningful hyperparameter w.r.t. model architecture: the size of the embedding dimension (`embedding_dim`). (We also see a `vocab_size` parameter here, but this only a by-product on our cutoff for infrequent tokens, there also may more hyperparameters if you modified the architecture, such as adding a linear layer). Adjust the embedding dimension below when you start training your model. How big should your embedding dimension be? Recall that the embedding is a way to \"condense\" the sparsely populated tokenized vocabulary, so we suggest starting with values that are much lower than the vocab size.\n",
        "\n",
        "Remember the CUDA discussion in the first cell of this notebook? Here the `.to(device)` is where that discussion becomes relevant (if `device=='cuda'`, PyTorch will perform the matrix operations on GPU). If you recieve a mismatch error, your tensors may be on different devices."
      ],
      "id": "xltosIzM-SP2"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "_HQWUu-ZFEIg"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_DIM = 10 # adjust here\n",
        "\n",
        "model = NBOW(\n",
        "  vocab_size    = len(train_vocab.keys()),\n",
        "  embedding_dim = EMBEDDING_DIM\n",
        ").to(device)"
      ],
      "id": "_HQWUu-ZFEIg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4CZnj1f-da-"
      },
      "source": [
        "### 2.3 Instantiate the loss function and optimizer"
      ],
      "id": "C4CZnj1f-da-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aXi8nA0FEIh"
      },
      "source": [
        "Please select and instantiate an appropriate loss function and optimizer.\n",
        "\n",
        "*Hint: What loss functions are availible for binary classification? Feel free to look at the [torch.nn docs on loss functions](https://pytorch.org/docs/stable/nn.html#loss-functions) for help!*"
      ],
      "id": "9aXi8nA0FEIh"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "w98UvlXxFEIh"
      },
      "outputs": [],
      "source": [
        "# While we import Adam for you, you may try / import other optimizers as well\n",
        "from torch.optim import Adam\n",
        "\n",
        "\n",
        "criterion, optimizer = nn.BCELoss(), Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "### BEGIN YOUR CODE ###\n",
        "\n",
        "### END YOUR CODE ###"
      ],
      "id": "w98UvlXxFEIh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUXBtqPEjiRe"
      },
      "source": [
        "Now that we have a NBOW model, a loss function, optimizer and dataset, we can begin training!"
      ],
      "id": "hUXBtqPEjiRe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVLeTa8wFEIh"
      },
      "source": [
        "## 3. Training and Evaluation [10 points]\n",
        "We will now instantiate a `train_loop`, and a `val_loop` to evaluate our model at each epoch.\n",
        "\n",
        "**Fill out the train and test loops below. Treat real headlines as `False`, and Onion headlines as `True`.**"
      ],
      "id": "bVLeTa8wFEIh"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "vganx5fCFEIh"
      },
      "outputs": [],
      "source": [
        "def train_loop(model, criterion, optim, iterator):\n",
        "  \"\"\"\n",
        "  Returns the total loss calculated from criterion\n",
        "  \"\"\"\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  for x, y in tqdm(iterator):\n",
        "    ### BEGIN YOUR CODE (~6 lines) ###\n",
        "\n",
        "    # Zero out the parameter gradients\n",
        "    optim.zero_grad()\n",
        "\n",
        "\n",
        "    # Do forward pass with current batch of input\n",
        "    classification_score = model(x)\n",
        "    # pred_y = (classification_score > 0.5).float()\n",
        "\n",
        "    # Get loss with model predictions and true labels\n",
        "    loss = criterion(classification_score, y)\n",
        "    # print(loss.shape)\n",
        "    loss.backward()\n",
        "    total_loss += loss.item()\n",
        "    optim.step()\n",
        "\n",
        "    ### END YOUR CODE ###\n",
        "\n",
        "  return total_loss\n",
        "\n",
        "\n",
        "def val_loop(model, iterator):\n",
        "  \"\"\"\n",
        "  Returns:\n",
        "    true (List[bool]): All the ground truth values taken from the dataset iterator\n",
        "    pred (List[bool]): All model predictions.\n",
        "  \"\"\"\n",
        "  true, pred = [], []\n",
        "\n",
        "  ### BEGIN YOUR CODE (~8 lines) ###\n",
        "\n",
        "  #put model into evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  # iterate over dataset and add predictions\n",
        "  for x, y in tqdm(iterator):\n",
        "    # print(\"input\", x)\n",
        "    classification_score = model(x)\n",
        "    pred_y = (classification_score > 0.5).float()\n",
        "\n",
        "    true.extend(y.tolist())\n",
        "    pred.extend(pred_y.tolist())\n",
        "\n",
        "  ### END YOUR CODE ###\n",
        "\n",
        "  return true, pred"
      ],
      "id": "vganx5fCFEIh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNXJevTu-tDZ"
      },
      "source": [
        "### 3.1 Define the evaluation metrics"
      ],
      "id": "JNXJevTu-tDZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IsZQs3rFEIi"
      },
      "source": [
        "We will also need evaluation metrics to tell us how well our model is doing on the validation set at each epoch and later how well the model does on the held-out test set. You may find $\\S$4.4.1 of Eisenstein useful for these questions.\n",
        "\n",
        "**Complete the functions in the following cell.**"
      ],
      "id": "7IsZQs3rFEIi"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "gMQDg9Vy-wY0"
      },
      "outputs": [],
      "source": [
        "# Note: You will not need to import anything to implement these functions.\n",
        "\n",
        "def accuracy(true, pred):\n",
        "  \"\"\"\n",
        "  Calculate the ratio of correct predictions.\n",
        "\n",
        "  Args:\n",
        "    true (List[bool]): ground truth\n",
        "    pred (List[bool]): model predictions\n",
        "\n",
        "  Returns:\n",
        "    acc (float): percent accuracy with range [0, 1]\n",
        "  \"\"\"\n",
        "  acc = None\n",
        "  ### BEGIN YOUR CODE (~2-5 lines) ###\n",
        "  correct = 0\n",
        "  for i in range(len(true)):\n",
        "    if true[i] == pred[i]:\n",
        "      correct += 1\n",
        "  acc = correct / len(true)\n",
        "\n",
        "\n",
        "  ### END YOUR CODE ###\n",
        "  return acc\n",
        "\n",
        "\n",
        "def binary_f1(true, pred, selected_class=True):\n",
        "  \"\"\"\n",
        "  Calculate F-1 scores for a binary classification task.\n",
        "\n",
        "  Args:\n",
        "    true (List[bool]): ground truth\n",
        "    pred (List[bool]): model predictions\n",
        "    selected_class (bool): the selected class the F-1 is being calculated for.\n",
        "\n",
        "  Returns:\n",
        "    f1 (float): F-1 score between [0, 1]\n",
        "  \"\"\"\n",
        "  f1 = None\n",
        "  ### BEGIN YOUR CODE (~10-15 lines) ###\n",
        "\n",
        "  # iterate over truths and predictions to calculate true positive, false positive, etc.\n",
        "  true_positive = 0\n",
        "  true_negative = 0\n",
        "  false_positive = 0\n",
        "  false_negative = 0\n",
        "\n",
        "  for i in range(len(true)):\n",
        "    if true[i] == selected_class:\n",
        "      if true[i] == pred[i]:\n",
        "        true_positive += 1\n",
        "      else:\n",
        "        false_negative += 1\n",
        "    else:\n",
        "      if true[i] == pred[i]:\n",
        "        true_negative += 1\n",
        "      else:\n",
        "        false_positive += 1\n",
        "  # Calculate precision and recall\n",
        "  precision = true_positive / (true_positive + false_positive)\n",
        "  recall = true_positive / (true_positive + false_negative)\n",
        "\n",
        "  # Calculate F1 score\n",
        "  f1 = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "  ### END YOUR CODE ###\n",
        "  return f1\n",
        "\n",
        "\n",
        "def binary_macro_f1(true, pred):\n",
        "  \"\"\"\n",
        "  Calculate averaged F-1 for all selected (true/false) classes.\n",
        "\n",
        "  Args:\n",
        "    true (List[bool]): ground truth\n",
        "    pred (List[bool]): model predictions\n",
        "  \"\"\"\n",
        "  averaged_macro_f1 = None\n",
        "  ### BEGIN YOUR CODE (~1 line) ###\n",
        "\n",
        "  # Simply call the binary f1 method with each class then average their f1 scores\n",
        "  averaged_macro_f1 = (binary_f1(true, pred, True) + binary_f1(true, pred, False)) / 2\n",
        "\n",
        "  ### END YOUR CODE ###\n",
        "  return averaged_macro_f1"
      ],
      "id": "gMQDg9Vy-wY0"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Yw79JFieFEIi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68461209-960d-4ad2-e0a4-032911284e3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 150/150 [00:00<00:00, 275.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'float'> <class 'float'>\n",
            "Binary Macro F1: 0.44919618467601097\n",
            "Accuracy: 0.4533333333333333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ===========================================================================\n",
        "# To test your eval implementation, we will evaluate the untrained model on our\n",
        "# dev dataset. It will do pretty poorly (it's untrained), but the exact performance\n",
        "# will be random since the initialization of the model parameters is random.\n",
        "# ===========================================================================\n",
        "\n",
        "true, pred = val_loop(model, val_iterator)\n",
        "print(type(true[0]), type(pred[0]))\n",
        "print(f'Binary Macro F1: {binary_macro_f1(true, pred)}')\n",
        "print(f'Accuracy: {accuracy(true, pred)}')"
      ],
      "id": "Yw79JFieFEIi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2to0kWVFEIi"
      },
      "source": [
        "## 4. Full Training Run [5 points]\n",
        "Now we can perform a full run and see the model fit our loss. If everything goes correctly, you should be able to achieve a validation F1 score of at least `0.80`.\n",
        "\n",
        "**Feel free to adjust the number of epochs to prevent overfitting or underfitting and to play with your model hyperparameters/optimizer & loss function.**"
      ],
      "id": "Q2to0kWVFEIi"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "N-iuqkKCFEIj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "073f2264-ef84-4601-e487-af733cbf2074"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:05<00:00, 203.50it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 503.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 0\n",
            "TRAIN LOSS: 768.0477433204651\n",
            "VAL F-1: 0.5644906778135842\n",
            "VAL ACC: 0.6958333333333333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:05<00:00, 221.72it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 326.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 1\n",
            "TRAIN LOSS: 675.1972255408764\n",
            "VAL F-1: 0.7092605265401255\n",
            "VAL ACC: 0.76375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:06<00:00, 197.95it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 497.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 2\n",
            "TRAIN LOSS: 553.3403358608484\n",
            "VAL F-1: 0.776720157755235\n",
            "VAL ACC: 0.80875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:04<00:00, 244.10it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 458.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 3\n",
            "TRAIN LOSS: 467.5034080147743\n",
            "VAL F-1: 0.8016147926087339\n",
            "VAL ACC: 0.825\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:06<00:00, 194.69it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 469.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 4\n",
            "TRAIN LOSS: 411.08610585331917\n",
            "VAL F-1: 0.8231940280961618\n",
            "VAL ACC: 0.8433333333333334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:05<00:00, 226.83it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 453.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 5\n",
            "TRAIN LOSS: 371.5997628085315\n",
            "VAL F-1: 0.8425052769291725\n",
            "VAL ACC: 0.85625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:06<00:00, 192.52it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 340.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 6\n",
            "TRAIN LOSS: 339.9100679680705\n",
            "VAL F-1: 0.8470800579877833\n",
            "VAL ACC: 0.85875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:05<00:00, 233.23it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 456.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 7\n",
            "TRAIN LOSS: 313.4886445067823\n",
            "VAL F-1: 0.8539307275142918\n",
            "VAL ACC: 0.8654166666666666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:05<00:00, 212.39it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 372.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 8\n",
            "TRAIN LOSS: 292.0538796503097\n",
            "VAL F-1: 0.8522904104259872\n",
            "VAL ACC: 0.865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:06<00:00, 196.22it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 430.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 9\n",
            "TRAIN LOSS: 273.1184643879533\n",
            "VAL F-1: 0.8568864589034133\n",
            "VAL ACC: 0.86875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "TOTAL_EPOCHS = 10\n",
        "for epoch in range(TOTAL_EPOCHS):\n",
        "    train_loss = train_loop(model, criterion, optimizer, train_iterator)\n",
        "    true, pred = val_loop(model, val_iterator)\n",
        "    print(f\"EPOCH: {epoch}\")\n",
        "    print(f\"TRAIN LOSS: {train_loss}\")\n",
        "    print(f\"VAL F-1: {binary_macro_f1(true, pred)}\")\n",
        "    print(f\"VAL ACC: {accuracy(true, pred)}\")"
      ],
      "id": "N-iuqkKCFEIj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_l91F4ooFEIj"
      },
      "source": [
        "We can also look at the models performance on the held-out test set, using the same `val_loop` from earlier."
      ],
      "id": "_l91F4ooFEIj"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "vs8Fy_ncFEIo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "769a3b89-829d-4375-f314-a1d422a7fbbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 150/150 [00:00<00:00, 469.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEST F-1: 0.8635706010993118\n",
            "TEST ACC: 0.8725\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "true, pred = val_loop(model, test_iterator)\n",
        "print(f\"TEST F-1: {binary_macro_f1(true, pred)}\")\n",
        "print(f\"TEST ACC: {accuracy(true, pred)}\")"
      ],
      "id": "vs8Fy_ncFEIo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMPWmorEFEIp"
      },
      "source": [
        "## 5. Analysis [5 points]\n",
        "While modeling and accuracy are a great signal that our model is working in our specific task setup, an inspection of what the model is classifying (particularly its errors), can allow us to hypothesize about what is going on, why it works, and how to improve.\n",
        "\n"
      ],
      "id": "rMPWmorEFEIp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnjKlKt352hQ"
      },
      "source": [
        "### 5.1 Impact of Vocab Size\n",
        "**Question:** *What happens to the vocab size as you change the cutoff in the cell below? Can you explain this in the context of [Zipf's Law](https://en.wikipedia.org/wiki/Zipf%27s_law)?*\n",
        "\n",
        "**Answer:** The vocab size decrease by a factor of ~40% when changing the cutoff from 1 to 2 and decresaes by a factor of ~20% when changing the cutoff frmo 2 to 3 as you can see below. This follows **Zipf's Law** which states that the frequency of a word is inversely proportional to its rank in the frequency table. In other words, a small number of words appear very frequently, while the majority occur rarely, so increasing the cutoff disproportionately removes less common words, leading to a sharp drop in vocabulary size as the cutoff incresease but less and less percent of words get cutoff as you incresae the cuttoff threshold."
      ],
      "id": "fnjKlKt352hQ"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "pI0fM4oMFEIp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e18d27c4-8356-4fd2-d3eb-30303d041aff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cutoff=1: 13247\n",
            "cutoff=2: 9568\n",
            "cutoff=3: 7646\n"
          ]
        }
      ],
      "source": [
        "tmp_vocab, _ = generate_vocab_map(train_df, cutoff = 1)\n",
        "print(f\"cutoff=1: {len(tmp_vocab)}\")\n",
        "\n",
        "tmp_vocab, _ = generate_vocab_map(train_df, cutoff = 2)\n",
        "print(f\"cutoff=2: {len(tmp_vocab)}\")\n",
        "\n",
        "tmp_vocab, _ = generate_vocab_map(train_df, cutoff = 3)\n",
        "print(f\"cutoff=3: {len(tmp_vocab)}\")"
      ],
      "id": "pI0fM4oMFEIp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0x54B1lFEIp"
      },
      "source": [
        "### 5.2 Error Analysis\n",
        "\n",
        "*Can you describe what cases the model is getting wrong in the witheld test-set?*\n",
        "\n",
        "To do this, you will need to create a new `val_train_loop_incorrect` which returns incorrect sequences **and** you will need to decode these sequences back into words. You have already created a map that can convert encoded sequences back to regular English (`reverse_vocab`)."
      ],
      "id": "d0x54B1lFEIp"
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "TfohtPF8FEIp"
      },
      "outputs": [],
      "source": [
        "def val_train_loop_incorrect(model, iterator):\n",
        "  \"\"\"\n",
        "  Implement this however you like! It should look very similar to val_loop.\n",
        "  Pass the test_iterator through this function to look at errors in the test set.\n",
        "  \"\"\"\n",
        "\n",
        "  # Make a list to hold the sequences the model mis-classified\n",
        "  incorrect_seqs = []\n",
        "\n",
        "  # Put model into evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  # Iterate over test data\n",
        "  for x, y in tqdm(iterator):\n",
        "    # print(\"input\", x)\n",
        "    classification_score = model(x)\n",
        "    pred_y = (classification_score > 0.5).float()\n",
        "\n",
        "    y = y.tolist()\n",
        "    pred_y = pred_y.tolist()\n",
        "\n",
        "    # print(type(y[0]), type(pred_y[0]))\n",
        "\n",
        "    for i in range(len(pred_y)):\n",
        "      if pred_y[i] != y[i]:\n",
        "        incorrect_seqs.append(x[i].tolist())\n",
        "\n",
        "\n",
        "\n",
        "  # Determine incorrect sequences\n",
        "\n",
        "  # Decode data back into English words\n",
        "  # (HINT: you should use the dictionary, reverse_vocab, defined earlier)\n",
        "  # (SECOND HINT: make sure you stop decoding once you hit padding tokens!)\n",
        "\n",
        "  for i, seq in enumerate(incorrect_seqs):\n",
        "    for j, token in enumerate(seq):\n",
        "      if token == PADDING_VALUE:\n",
        "        incorrect_seqs[i] = incorrect_seqs[i][:j]\n",
        "        break\n",
        "\n",
        "      incorrect_seqs[i][j] = reverse_vocab[int(token)]\n",
        "\n",
        "    incorrect_seqs[i] = \" \".join(incorrect_seqs[i])\n",
        "\n",
        "\n",
        "  return incorrect_seqs"
      ],
      "id": "TfohtPF8FEIp"
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "6-azPje88iU0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dc52070-410b-49b8-98e2-4993446332ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 150/150 [00:00<00:00, 433.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "202 303\n",
            "['leaving this UNK UNK with a bang - you can have your cremated ashes turned into UNK fireworks for a UNK display .', 'man finds car 20 years after forgetting where he parked it', \"right UNK complain that obama does n't have any white dogs\", 'UNK man does not maintain erection during national anthem', \"no one in gang has heart to tell police UNK his cover 's blown\", 'stephen miller UNK ice agent $ 50 bill in exchange for a little alone time with detained migrants', 'spineless democratic senator caves to demands of sick children', 'UNK meet with top pentagon leaders to talk about peace', 'epa promotes UNK black sludge to deputy director', 'nasa administrator resigns after leak of offensive UNK email', \"johnson & & johnson introduces UNK but tears ' UNK to UNK up UNK\", 'bell tells UNK she must wait UNK years to get cellphone credits UNK', 'the man who created a tiny country he can no longer enter', 'gay couple accepts responsibility for every major disaster of the last 50 years', 'literally a third of world bank policy reports have never , ever been read online , by anyone', '13-year-old drinking prodigy UNK to ohio state', 'man discovers end of UNK UNK after UNK hours of scrolling', 'venezuela issues travel warning to the u.s. in light of recent shootings', 'stanford offering free tuition to students whose families make less than $ UNK', 'UNK UNK apologize after cutting off head of wrong person', 'UNK family welcomes third child born on same day for third straight year', 'with the money the government spends to buy the army just one attack helicopter , it could buy me an attack helicopter', \"serial killer elizabeth UNK 's nursing licence could be revoked today\", \"supreme court on gay marriage : UNK , who cares '\", 'florida resort allows guests to swim with miami dolphins', 'man who spent 300 hours playing fantasy football this year rewarded with $ 30 UNK payout', 'UNK UNK b', 'mike pence visits small town hit hard by kids seeing r-rated movies', 'sessions argues justice department will not be UNK by political UNK outside private prison lobbyists , wall street donors , UNK christian activists', 'officials not sure how prisoner got UNK in half , with organs missing', 'local internet user completely unaware he a top content creator for UNK sports', 'facebook bans extremist figures after UNK them dangerous to its public reputation', 'amc reports record UNK', 'UNK college UNK designs diploma for student on verge of actually graduating', 'congress approves $ UNK million for evil', \"police officer does n't see a difference between black or UNK black suspects\", 'UNK chan UNK UNK UNK solar panel efficiency by a massive 22 %', 'bus tour takes fans down iconic ‘ joker ’ stairs', 'mood in car takes grim turn after dad misses exit', 'retired couple rage and destroy own house walls searching for source of loud UNK noise', 'state department warns americans traveling abroad to avoid lame amsterdam UNK tour', 'danish radio station criticised for killing baby rabbit on air', \"amazon completes new suspension tank to house psychic beings who UNK customers ' future orders\", 'united airlines cracking down on emotional support UNK', 'local UNK wants to work with kids', 'heartbreaking : kidnapper is forced to destroy his autographed michael jordan poster to get a ‘ d ’ for his ransom note', 'ancient UNK erection UNK in amber', 'texas to execute death row inmates with new UNK UNK cocktail', 'oscar UNK took acting lessons to ready for UNK performance at trial : report', 'olympic drug testing official left horribly UNK after coming into contact with russian urine', 'UNK outraged as u.s. plays nazi version of anthem', 'company president started out as UNK UNK', \"lawmaker 's war hero son would have wanted road bill passed\", 'mom declares garage her next big project', 'vagina has five UNK shadow', 'fcc chair unveils UNK comment line to UNK net neutrality complaints for $ UNK per month', '‘ UNK patrol ’ writers defend episode where german UNK cop shoots unarmed black lab 17 times in back', 'dancing , UNK rex tillerson slides across floor of empty state department', 'brazilian government UNK firefighters with flamethrowers to combat massive amazon rainforest', 'entire conversation with parents spent changing the subject', 'UNK to UNK', 'pfizer UNK puts down another UNK of trial patients', 'environmentalists warn swedish fish population being UNK by great pacific sour patch', 'harry potter books spark rise in UNK among children', \"bird hunted to near extinction due to UNK UNK you ' call\", 'pimp my ride : hamas proudly shows off a tank , turns out to just be a car', 'group of UNK cardinals appeals to pope to relax celibacy requirement', 'poll finds many voters would support equally UNK UNK candidate', 'usc insists lori loughlin ’ s daughter was admitted solely based on socioeconomic background', 'man with first name ‘ god ’ runs into UNK issues', 'couple , 89 , have to die by january or lose life insurance', 'florida state coaches award helmet sticker to UNK police officer | the onion', 'white house UNK changed to blame democrats', 'is your interior UNK putting your life at risk ?', 'mary cheney : if men can do drag , why can ’ t white people do UNK ?', 'these brave teens went UNK for 3 whole days and miraculously survived', 'UNK that UNK in box too fancy for dry food', 'the stories in ‘ the UNK ’ are powerful , but they probably didn ’ t literally happen', 'cnn interrupts coverage of biden on the nra with news of another school shooter', \"joe biden announces he 's not running for UNK\", 'la restaurant unveils UNK bottled water menu', 'michele telling the truth .', 'brussels UNK : UNK man forced daughter to stay at table for 13 hours', 'dozens of teenagers are now tweeting bomb jokes to american airlines', 'fema airdrops emergency UNK pills for residents stranded by hurricane florence', 'UNK restaurants to be UNK UNK', '2016 in technology', 'parents brawl during youth baseball game after UNK with 13-year-old umpire', 'white house aide said john mccain ’ s opinion doesn ’ t matter because ‘ he ’ s dying anyway ’ : reports', 'congressional aides withholding sex until budget compromise is UNK', 'clickhole is UNK its privacy policy', 'veteran told what offends him', 'man sends email to 246 women , hoping one of them is the UNK he met the night before', 'test', 'elon musk hires onion writers for project . ( not an onion headline )', 'trump spiritual adviser : jesus would have been UNK if he broke immigration laws', 'world ’ s greatest soccer stars arrive in brazil for UNK coca-cola ad', 'worth the wait : after UNK out their UNK conflicts , over 400 UNK are ready to play a relief concert for the victims of hurricane andrew', 'heavy lifting could affect fertility', 'UNK center director : we don ’ t spy on americans , just UNK americans', 'UNK find 99 % of people not related to anyone cool', 'warren buffett offers $ 1 billion for dick UNK to shut up', 'life : ultimate UNK : this former holocaust denier who claimed that only 1,000 jews died in the holocaust is making up for it by saying that 30 trillion jews died in the holocaust', 'kettle that looks like hitler brews trouble for UNK', 'pilots who fly drones into wildfires are idiots . punish them', 'woman nervous for boyfriend to meet person she becomes around parents', 'trump puts bannon on security council , dropping joint chiefs', 'new mit study suggests sonic the hedgehog might be living in computer simulation', 'phoenix leads nation in UNK shining lasers at airplanes', 'russian exposed friend to radiation to UNK him UNK', 'comet which looks like a skull to fly past earth on halloween', 'children may be UNK vulnerable to UNK pressure from robots', 'professional stone UNK makes it all the way across lake', 'intelligence setback : the cia is in crisis mode after isis made its instagram private', \"dog dumped by roadside with her dead puppies in a bag UNK tears in her eyes '\", 'how have we UNK without biscuits ?', 'UNK goal', 'internet dating picking up sex UNK UNK', 'white house UNK military draft registration for women', 'mixed-race family asks blue bell ice cream to change flavor name to be more inclusive', 'experts say earliest warning signs of mental health issues usually crossing eyes while UNK finger on lips , saying ‘ UNK , UNK ’', 'UNK on cnn : transgender girls will walk around bathrooms with their genitals exposed', '70-year-old woman decides it time to start dressing entirely in purple', 'reddit UNK in bringing UNK to nascar', 'woman ’ s head feared lost forever inside infinity UNK', \"caption to this photo on UNK : `` UNK protect the entrance to the new york stock exchange ''\", \"woman sues parents of cyclist she killed with her suv , complaining `` her UNK of life has been be UNK ''\", 'pbs called out for showing old fireworks footage on fourth of july', 'UNK fines post office $ UNK for exposing mail UNK to august heat', 'the onions tips for nailing a job interview', \"'you 're right ' host changes abortion stance after realizing he wouldn ’ t have show if he was UNK\", \"crazy UNK wants to create 'master race '\", \"copy of 'the UNK letter ' ca n't believe the notes high schooler writing in UNK\", 'panicked , UNK pope UNK UNK ban on abortion', \"'can i put my hand there ? ' : new york law changes the rules of the college UNK\", 'nervous steve bannon UNK entire class of interns amid calls for removal', 'UNK crashes UNK drone into nuclear plant to prove point', 'astronaut UNK cargo ship leaves note on side of iss after accidentally knocking off solar UNK', 'driving instructor has own gas UNK in case student total pussy', '‘ there are no good options in syria , ’ sighs man who has devoted 12 minutes of research to topic', 'betsy devos confirmed as education secretary', 'attention , UNK : clickhole will now audition to take over as admin of the official ‘ UNK ’ facebook and twitter accounts', \"dea claims UNK ' will get stoned if utah passes medical marijuana bill ( x-post from r / UNK )\", 'man always gets little rush out of telling people john lennon beat wife', \"UNK fraternity at yale university accused of hosting 'white girls only ' party\", 'nobel committee awards self peace prize for once', 'obama not ruling out u.s. military action in congress', 'public masturbator to cops : wait ’ til i ’ m finished', \"UNK musician to u.n. : UNK done me wrong '\", \"chinese factory worker ca n't believe the shit he makes for americans\", 'new epa regulations would force power plants to find 30 % more loopholes by UNK', 'clash of UNK : the san diego zoo is locked in a tense legal dispute with the UNK over who gets to keep a monkey holding a van gogh painting', 'guinea pig to become father to 400 after breaking into female enclosure', 'amputee inspires others not to lose limbs', 'epa reveals 37 % of water waste nationwide caused by UNK kids doing UNK into country club pool', 'sight of 400 war elephants on UNK marks hillary clinton ’ s arrival in swing state', 'progressive UNK school doesn ’ t have students', 'one dead in hair UNK UNK', 'white house honors aretha franklin by not releasing official statement on her death', 'president of news network : we ’ re not the place to turn for breaking news', 'tips for surviving a blizzard', 'congressman who UNK secret service was rejected by secret service', \"dakota access pipeline has first leak before it 's fully UNK\", 'UNK fills bodies with foam to save lives', 'fox news debuts bizarre , giant tablets in its UNK new newsroom', '10-year-old first responders rush to bike crash scene to check out tyler ’ s fucked-up leg', 'sephora makeup artist helping woman create the perfect pink eye', 'unstable man plots to bring guns to schools', 'UNK circles UNK from glowing red dot', \"sometimes it feels like i 'm in prison too , but then i go home\", 'air force uncovers lsd use among UNK guarding nuclear missiles', 'we did not have the time to UNK UNK the frog . here he is normal .', 'beer UNK talk man out of jumping off bridge by offering him UNK light', \"report : yelling UNK UNK ! ' will get you kicked out of the masters\", 'swat team heroically ends 6 hour standoff with empty apartment', 'world populace actually fine with rich people dying on mount everest', 'study : nearly 80 percent of roommates got so drunk last night', 'reality check : trip to UNK cost less than UNK stadium', 'pink jersey proves that woman is sports fan , yet also UNK a certain UNK', 'rocket city trash pandas chosen as new madison baseball team ’ s name', 'federal judge pencils blocking trump ’ s unconstitutional executive orders into monthly schedule', \"madden UNK predicts patriots will beat the seahawks UNK in the super bowl UNK that 's exactly what happened\", 'hackers access children ’ s names , photos in UNK breach', 'UNK dig uncovers ancient race of skeleton people', 'jesus is back in the passion of the christ 2', 'english teacher already armed with deadly weapon called shakespeare', 'rio 2016 : officials forced to cut their way into olympic stadium after losing keys to gate', 'cam girl has ash on forehead', 'cia admits role in UNK coup to UNK david lee UNK from van UNK', \"scientist invents spray that 'll get you UNK UNK just for a few seconds\", 'sean spicer given own press secretary to answer media ’ s questions about his controversial UNK', 'james UNK , chris paul deny rumors of UNK , say they are fully committed to team at state farm', 'pr exec tweets , ‘ going to africa . hope i don ’ t get aids . just UNK . i ’ m white ! ’', 'youtube video containing UNK receives 5 different copyright violations', '‘ game of thrones ’ creators plan to be ‘ very drunk and far from the internet ’ when series finale airs', \"UNK differences set aside for congress 's annual erotic UNK ball\", 'kkk member indicted for trying to build anti-muslim x-ray cannon', 'gingrich : possible top trump aide worked in finance , so not anti-semitic', 'glasgow gym set to introduce fitness class UNK of nothing but sleeping for 45 minutes', 'climate change is no joke jim', 'teen on UNK of UNK incredible journey of UNK instead asks boyfriend to use condom', 'study : 11 % of americans think UNK is an std', 'UNK irving debuts signature shoe inspired by UNK chips government secretly implants in UNK patients', '‘ 1984 ’ tops amazon UNK list', 'disgusting couple always UNK in public', 'indiana governor insists new law has nothing to do with thing it UNK intended to do', 'cat lives double life with second family', 'north koreans go to polls : UNK will be 100 percent , results UNK', '’ possible UNK ’ was man putting up his UNK', 'george h.w . bush still hates broccoli', 'crew called to own station : response time UNK ’', 'civilians in abandoned mcdonalds seize control of wandering space satellite', 'ron harper defends himself against an onion column', 'UNK cord on stage steals UNK from jeb bush during campaign rally', 'tgi fridays is a human right', 'last two speakers of dying language refuse to talk to each other', 'some people are too UNK . i swear people just want to be UNK about something', 'UNK old medieval UNK found after 72 years .', \"world urges israelis , palestinians to focus UNK hatred on region 's UNK ' UNK UNK\", 'company to experiment with UNK employees', 'new hampshire man loses life savings on carnival game', 'UNK chickens wear UNK , egg production UNK', 'facebook said it messed up again and UNK millions of instagram passwords in plain text', '‘ i faked it all ’ : influencer UNK UNK faked entire trip to UNK', 'UNK castle salesman from london now chief UNK for isis', 'pruitt staffers worried about toxic chemical in his desk', 'in the name of free speech , adblock serves up ads , just for a day', 'report : would-be suicide UNK pushed off bridge', 'UNK government UNK students from UNK in showers by handing out digital UNK to limit shower time', 'obama turns 50 despite republican opposition', 'big pharma ceo : ‘ we ’ re in business of shareholder profit , not helping the sick ’', 'minnesota senate keeps rule UNK eye contact', 'ferry mcferryface has ferry UNK itself into a UNK', 'hundreds of children terrified when movie theatre plays la UNK instead of detective pikachu', 'two years ago , this man was 500 pounds . now he is two men who weigh 250 pounds .', 'devos backlash sees parents threatening to UNK kids', 'should the government stop dumping money into a giant hole ? ( youtube )', 'UNK parade float covered in tickets after parking on 5th UNK over holiday weekend', '“ one of the biggest UNK in UNK . history , ” bureau spokesman UNK UNK told reporters . unlocked iphone worthless after UNK . spills glass of water on it', 'new study finds most of earth ’ s oxygen used for complaining', \"hillary 's top donor buys the onion and starts publishing propaganda immediately .\", 'life UNK to change UNK of critical UNK', 'new york lawmakers carve out UNK exemption for struggling yacht UNK', 'taco bell unveils new taco with shell made from doritos bags', 'UNK may pull manning in the 2nd half , needs qb healthy for run at perfect record next season', 'news : just like us : this UNK shot himself in the head after the irs uncovered his tax fraud scheme', 'ap : UNK wasted his money on healthcare when he could have built UNK skyscrapers', 'china news confuses UNK for special mushroom ( UNK ) ( not the onion )', 'news flash : science shows half of what dr. oz says is UNK', 'designers think color of balls is what puts women off sports', 'UNK UNK green : it ’ s only rape if the person is conscious', 'UNK state employees being called back to staff unemployment compensation centers', 'nearly 40 % of 2019 farm income will come from federal aid and insurance', 'kellyanne conway : those on UNK who will lose health insurance can always get jobs', 'this guy sent sierra leone 4,000 bottles of holy water to cure ebola', 'alaska congressional candidate has never visited the state : ap', 'man constantly blaming his problems on fact that he ’ s on fire', 'depressed UNK director issues poll asking whether anyone would care whether he lives or dies', 'breaking bad creator : stop throwing pizzas at walter white ’ s house', 'epa removes climate change information from website', \"UNK koch : i 'm fighting against special interests\", 'pharmaceutical industry UNK as more moms making vaccines at home', 'the house from the windows 95 maze UNK is up for sale', 'earth ’ s last male northern white rhino gets personal armed UNK', 'system seller : pornhub pledges to fully support the UNK', 'unconscious amazon employee UNK for not filing UNK request', '‘ you know , i UNK it too , ’ bradley cooper says out loud again to no one in particular', 'listen , area boss gets it', 'health scare prompts man to start UNK healthier', 'video games cause mass shootings', 'sleeping man UNK by laptop , phone , UNK like egyptian UNK buried with all his UNK', '8-year-old accidentally UNK second amendment rights', 'number of billionaires doubles since financial crisis', 'fun toy banned because of three stupid dead kids', 'money really does lead to a more UNK life', 'soaring gas prices forcing more americans to drink less gas', 'UNK phil indicted for false informing', \"the onion looks back at ' UNK . '\", 'wwe staff forced to shoot aggressive wrestler after child climbs into steel cage', 'high integrity , moral UNK has cost idiot man millions', 'UNK more likely to have health issues', 'woman held for 8 days in psych ward for saying obama follows her on UNK even though he does', 'naked UNK will soon be sweeping through houston', 'park rangers lance old UNK in effort to pop clogged , inflamed UNK', 'i am an UNK to god ( by a hot air balloon )', 'UNK UNK : ‘ UNK UNK ’ to hurt god ’ s feelings by not using oil', 'UNK UNK recalls having to torture more prisoners than male colleagues to prove herself', '57 % percent of americans think only kids who win should get trophies', 'adele hit wakes child from coma', 'patrons thought stabbing was part of art UNK show', 'toilet UNK , please . perfect , onion or real news headline issue . seriously ?', 'new discovery channel chief promises no more UNK bullshit', 'indians worship a UNK believed to be the reincarnation of lord UNK', 'democrats call for convincing amount of UNK for al UNK', 'study : dolphins not so intelligent on land', 'six flags adds UNK cars to its roller UNK for passengers who prefer more UNK ride', 'patriothole : sorry liberals , but gun control won ’ t stop UNK chan if he wants to take out 30 people with a single mop', 'kavanaugh on sexual assault allegations : ‘ i miss high school ’', 'slightly upset woman declared insane', 'seattle installing UNK tables in public parks to deter crime', \"humanity still producing new art as though UNK 's UNK in peace ' does n't already exist\", 'finland to pay citizens UNK income', '911 UNK informs black caller that death is on the way']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "incorrect_seqs = val_train_loop_incorrect(model, test_iterator)\n",
        "counter = 0\n",
        "for seq in incorrect_seqs:\n",
        "  if \"UNK\" in seq:\n",
        "    counter += 1\n",
        "print(counter, len(incorrect_seqs))\n",
        "print(incorrect_seqs)"
      ],
      "id": "6-azPje88iU0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUoLTGAScre7"
      },
      "source": [
        "Now that we have our incorrect sequences:   \n",
        "**Question:** *Can you describe what cases the model is getting wrong in the witheld test-set?*\n",
        "\n",
        "**Answer:** The model is having trouble recognizing unknown vocabulary since a common theme in all the sequences this model is getting wrong is the appearence of the \"UNK\" token. This means that the vocab cutoff removed many lower frequency words, which may have lead to a loss of important context and meaning in these test cases. This causes the data to have sentences that do not make coherent sense or are semintically incomplete because the cutoff threshold caused us to remove important aspects of the sentnce like proper nouns, domain specific language, or even slang.\n",
        "\n",
        "Some examples of this include, the sentence \"cops bust filthy, UNK mark zuckerberg for selling personal data on street corner,\" where it is not clear what \"UNK\" is replacing. Given the setence, it can be an important adjective or proper noun that can change the meaning of the sentence completely. Similarly, in the example \"'game of thrones' UNK demands trial by combat in new york supreme court case\" the missing word could refer to domain specific (in this case game of thrones specific) language or it could be another word that could provide context on the humor in the sentence. Those examples show that the model is struggling with unknown vocab especially with proper nouns and domain specific language."
      ],
      "id": "mUoLTGAScre7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mY8S9ZK9zuVs"
      },
      "source": [
        "## 6. Submit Your Homework\n",
        "This is the end of Project 1. Congratulations!  \n",
        "\n",
        "Now, follow the steps below to submit your homework in [Gradescope](https://www.gradescope.com/courses/939466):\n",
        "\n",
        "1. Rename this ipynb file to `CS4650_p1_GTusername.ipynb`. Make sure all cells have been run. We recommend ensuring you have removed any extraneous cells & print statements, clearing all outputs, and using the Runtime --> Run all tool to make sure all output is update to date.\n",
        "2. Click on the menu 'File' --> 'Download' --> 'Download .py'.\n",
        "3. Click on the menu 'File' --> 'Download' --> 'Download .ipynb'.\n",
        "4. Download the notebook as a .pdf document. Make sure the output from Part 4 are captured so we can see how the loss, F1, & accuracy changes while training.\n",
        "5. **Double check to make sure you answered the cell text questions (sections 5.1 and 5.2)!** Every year some students forget to answer them and get points taken off.\n",
        "5. Upload all 3 files to Gradescope. Double check the files start with `CS4650_p1_*`, capitalization matters."
      ],
      "id": "mY8S9ZK9zuVs"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}