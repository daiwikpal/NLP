{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daiwikpal/NLP/blob/main/CS4650_p1_dpal32.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnnwupedFEIV"
      },
      "source": [
        "# Headline Classification with Neural Bag of Words\n",
        "**CS 4650 \"Natural Language Processing\" Project 1**  \n",
        "Georgia Tech, Spring 2025\n",
        "\n",
        "(Instructor: Prof. Wei Xu; TAs: Yao Dou,  Tarek Naous, Xiaofeng Wu, Jonathan Zheng)\n",
        "\n",
        "Welcome to the first full programming project for CS 4650! **To start, first make a copy of this notebook to your local drive, so you can edit it.**\n",
        "\n",
        "If you want GPUs (which will improve training times), you can always change your instance type by going to Runtime -> Change runtime type -> Hardware accelerator.\n",
        "\n",
        "**In this project, we will be using PyTorch.** If you are new to PyTorch, or simply want a refresher, we recommend you start by looking through these [Introduction to PyTorch](https://cocoxu.github.io/CS4650_spring2025/slides/PyTorch_tutorial.pdf) slides and this interactive [PyTorch basics notebook](http://bit.ly/pytorchbasics). Additionally, this [text sentiment](http://bit.ly/pytorchexample) notebook will provide some insight into working with PyTorch with a specific NLP task."
      ],
      "id": "JnnwupedFEIV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Honor Code  [1 points]\n",
        "\n",
        "**Honor Code:** I hereby agree to abide the Georgia Tech's Academic Honor Code, promise that the submitted assignment is my own work, and understand that my code is subject to plagiarism test.\n",
        "\n",
        "**Signature**: Daiwik Pal"
      ],
      "metadata": {
        "id": "BMgET_OMuuNZ"
      },
      "id": "BMgET_OMuuNZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRdPqeMMFEIY"
      },
      "source": [
        "## 1. Load and preprocess data [9 points]\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n",
        "\n",
        "This project will be modeling a *classification task* for headlines from [The Onion](https://www.theonion.com), a satirical news website. Our dataset contains headlines and whether they belong to The Onion or CNN. Given a headline, we want to predict whether it is Onion or not.\n",
        "\n",
        "The following cell loads, pre-processes and tokenizes our OnionOrNot dataset."
      ],
      "id": "ZRdPqeMMFEIY"
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "YmV_uknBJA-o"
      },
      "outputs": [],
      "source": [
        "!curl -so OnionOrNot.csv https://raw.githubusercontent.com/lukefeilberg/onion/master/OnionOrNot.csv"
      ],
      "id": "YmV_uknBJA-o"
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "L3DkMDu7FEIZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dd25de1-212d-4366-aa99-87dae3c60245"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n"
          ]
        }
      ],
      "source": [
        "# ===========================================================================\n",
        "# Run some setup code for this notebook. Don't modify anything in this cell.\n",
        "# ===========================================================================\n",
        "\n",
        "import torch\n",
        "import random, sys\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "\n",
        "# ===========================================================================\n",
        "# A quick note on CUDA functionality (and `.to(model.device)`):\n",
        "# CUDA is a parallel GPU platform produced by NVIDIA and is used by most GPU\n",
        "# libraries in PyTorch. CUDA organizes GPUs into device IDs (i.e., \"cuda:X\" for GPU #X).\n",
        "# \"device\" will tell PyTorch which GPU (or CPU) to place an object in. Since\n",
        "# collab only uses one GPU, we will use 'cuda' as the device if a GPU is available\n",
        "# and the CPU if not. You will run into problems if your tensors are on different devices.\n",
        "# ===========================================================================\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Check what version of Python is running\n",
        "print(sys.version)"
      ],
      "id": "L3DkMDu7FEIZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Fulh0MZ8y8b"
      },
      "source": [
        "### 1.1 Dataset preprocessing functions\n",
        "The following cell define some methods to clean the dataset, but feel free to take a look to see some of the operations it's doing.\n"
      ],
      "id": "0Fulh0MZ8y8b"
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "ctNnE1Ui8oKw"
      },
      "outputs": [],
      "source": [
        "# ===========================================================================\n",
        "# Run some preprocessing code for our dataset. Don't modify anything in this cell.\n",
        "# This code was adapted from fast-bert.\n",
        "# ===========================================================================\n",
        "\n",
        "import re\n",
        "import html\n",
        "\n",
        "def spec_add_spaces(t: str) -> str:\n",
        "    \"Add spaces around / and # in `t`. \\n\"\n",
        "    return re.sub(r\"([/#\\n])\", r\" \\1 \", t)\n",
        "\n",
        "def rm_useless_spaces(t: str) -> str:\n",
        "    \"Remove multiple spaces in `t`.\"\n",
        "    return re.sub(\" {2,}\", \" \", t)\n",
        "\n",
        "def replace_multi_newline(t: str) -> str:\n",
        "    return re.sub(r\"(\\n(\\s)*){2,}\", \"\\n\", t)\n",
        "\n",
        "def fix_html(x: str) -> str:\n",
        "    \"List of replacements from html strings in `x`.\"\n",
        "    re1 = re.compile(r\"  +\")\n",
        "    x = (\n",
        "        x.replace(\"#39;\", \"'\")\n",
        "        .replace(\"amp;\", \"&\")\n",
        "        .replace(\"#146;\", \"'\")\n",
        "        .replace(\"nbsp;\", \" \")\n",
        "        .replace(\"#36;\", \"$\")\n",
        "        .replace(\"\\\\n\", \"\\n\")\n",
        "        .replace(\"quot;\", \"'\")\n",
        "        .replace(\"<br />\", \"\\n\")\n",
        "        .replace('\\\\\"', '\"')\n",
        "        .replace(\" @.@ \", \".\")\n",
        "        .replace(\" @-@ \", \"-\")\n",
        "        .replace(\" @,@ \", \",\")\n",
        "        .replace(\"\\\\\", \" \\\\ \")\n",
        "    )\n",
        "    return re1.sub(\" \", html.unescape(x))\n",
        "\n",
        "def clean_text(input_text):\n",
        "    text = fix_html(input_text)\n",
        "    text = replace_multi_newline(text)\n",
        "    text = spec_add_spaces(text)\n",
        "    text = rm_useless_spaces(text)\n",
        "    text = text.strip()\n",
        "    return text"
      ],
      "id": "ctNnE1Ui8oKw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiUlTSBB9Wx6"
      },
      "source": [
        "### 1.2 Tokenize using NLTK\n",
        "\n",
        "We will use our rule-based `clean_text` function to clean our raw text, then use the popular NLTK [punkt tokenizer](https://www.nltk.org/_modules/nltk/tokenize/punkt.html) to convert text to individual sub-words. This will take a while because you have to download the pre-trained punkt tokenizer.\n",
        "\n",
        "*If you are interested: There's a [long and diverse history of converting raw text to \"tokens\"](https://arxiv.org/abs/2112.10508), and many available methods/algorithms (you can experiment with some recently trained ones, trained on a dynamic programming-based method called BPE, [here](https://huggingface.co/spaces/Xenova/the-tokenizer-playground)).*"
      ],
      "id": "MiUlTSBB9Wx6"
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "vqtdrhF8FEIZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eabab3b7-375a-4a45-de32-5fc20f06b7c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# ===========================================================================\n",
        "# Tokenize using punkt. Don't modify anything in this cell.\n",
        "# ===========================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from tqdm import tqdm\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "df = pd.read_csv(\"OnionOrNot.csv\")\n",
        "df[\"tokenized\"] = df[\"text\"].apply(lambda x: nltk.word_tokenize(clean_text(x.lower())))"
      ],
      "id": "vqtdrhF8FEIZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBBdVOYxFEIa"
      },
      "source": [
        "We will use `pandas`, a popular library for data analysis and table manipulation, in this project to manage the dataset. For more information on usage, please refer to the [Pandas documentation](https://pandas.pydata.org/docs/).\n",
        "\n",
        "The primary data structure in Pandas is a `DataFrame`. The following cell will print out the basic information contained in our `DataFrame` structure, and the first few rows of our dataset."
      ],
      "id": "qBBdVOYxFEIa"
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "sJjScqV3FEIb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "2d45dca2-a884-4b1b-9151-a8078f4afcf8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  label  \\\n",
              "0  Entire Facebook Staff Laughs As Man Tightens P...      1   \n",
              "1  Muslim Woman Denied Soda Can for Fear She Coul...      0   \n",
              "2  Bold Move: Hulu Has Announced That They’re Gon...      1   \n",
              "3  Despondent Jeff Bezos Realizes He’ll Have To W...      1   \n",
              "4  For men looking for great single women, online...      1   \n",
              "\n",
              "                                           tokenized  \n",
              "0  [entire, facebook, staff, laughs, as, man, tig...  \n",
              "1  [muslim, woman, denied, soda, can, for, fear, ...  \n",
              "2  [bold, move, :, hulu, has, announced, that, th...  \n",
              "3  [despondent, jeff, bezos, realizes, he, ’, ll,...  \n",
              "4  [for, men, looking, for, great, single, women,...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0a2b08d0-46dc-4316-ad16-e4572dd4a31b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>tokenized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Entire Facebook Staff Laughs As Man Tightens P...</td>\n",
              "      <td>1</td>\n",
              "      <td>[entire, facebook, staff, laughs, as, man, tig...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Muslim Woman Denied Soda Can for Fear She Coul...</td>\n",
              "      <td>0</td>\n",
              "      <td>[muslim, woman, denied, soda, can, for, fear, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Bold Move: Hulu Has Announced That They’re Gon...</td>\n",
              "      <td>1</td>\n",
              "      <td>[bold, move, :, hulu, has, announced, that, th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Despondent Jeff Bezos Realizes He’ll Have To W...</td>\n",
              "      <td>1</td>\n",
              "      <td>[despondent, jeff, bezos, realizes, he, ’, ll,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>For men looking for great single women, online...</td>\n",
              "      <td>1</td>\n",
              "      <td>[for, men, looking, for, great, single, women,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0a2b08d0-46dc-4316-ad16-e4572dd4a31b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0a2b08d0-46dc-4316-ad16-e4572dd4a31b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0a2b08d0-46dc-4316-ad16-e4572dd4a31b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-759afea2-c4bc-4f94-a97c-c0c357da183e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-759afea2-c4bc-4f94-a97c-c0c357da183e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-759afea2-c4bc-4f94-a97c-c0c357da183e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 24000,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 23998,\n        \"samples\": [\n          \"Freeloading Refugee Children Taking Up Thousands Of Prison Cells Meant For Real Americans\",\n          \"Life: Finally Bought A Jackhammer? Don\\u2019t Make These 6 Rookie Mistakes\",\n          \"Apple 1 computer worth $200K left at recycling centre\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tokenized\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "# View the first few entries of our dataset\n",
        "df.head()"
      ],
      "id": "sJjScqV3FEIb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9b4W9z1XhgS"
      },
      "source": [
        "Try to guess some examples! Is the task more difficult than you expected?\n",
        "\n",
        "DataFrames can be indexed using [`.iloc[]`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html). `iloc` uses interger based indexing and supports a single integer (`df.iloc[42]`), a list of integers (`df.iloc[[1, 5, 42]]`), or a slice (`df.iloc[7:42]`)."
      ],
      "id": "D9b4W9z1XhgS"
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "Ntm8laX6FEIb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19d653f8-7fbc-40fc-9e15-0acf6fda4ee0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24000, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "# E.g., get row 42 of our dataset\n",
        "df.iloc[42]\n",
        "df.shape"
      ],
      "id": "Ntm8laX6FEIb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQVT6HUA9htQ"
      },
      "source": [
        "### 1.3 Split the dataset into training, validation, and testing"
      ],
      "id": "TQVT6HUA9htQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDI72x8XFEIc"
      },
      "source": [
        "**Train/Test/Val Split** - Now that we've loaded this dataset, we need to split the data into train, validation, and test sets.\n",
        "\n",
        "A good explanation of why we need these different sets can be found in $\\S$2.2.5 of [Eisenstein](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf) but our high-level goal is to have a generalized model and have confidence in our results.\n",
        "\n",
        "\n",
        "The *training set* is used to fit our model's learned parameters (weights and biases) to the task. The *validation  set* (sometimes called development set) is used to verify our training jobs are minimizing loss on an unseen subset of the data and can also be used to help choose hyperparameters for our training setup. The *test set* is used to provide a final evaluation of our trained model (unbiased by development or training decisions), ideally providing some insight into how the model will perform in a scenario we cannot perfectly represent in our data (i.e., the real world). *Each of these sets should be disjoint from the others*, to prevent any leackage that could introduce bias in our evaluation metrics (in this case accuracy).\n",
        "\n",
        "**Model Vocabulary** - We cannot directly feed sub-word token strings into a model! We need to create a \"vocab map\", which contains an ID for each unique token in our Onion dataset. This will be used as a \"lookup\" in the next few sections, since your PyTorch implementation will require first converting your Onion token representations to a list of sub-word IDs.\n",
        "\n",
        "**In the following cell, please implement `split_train_val_test` and `generate_vocab_map`.**"
      ],
      "id": "GDI72x8XFEIc"
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "zeo9kX6i9pbH"
      },
      "outputs": [],
      "source": [
        "# ===========================================================================\n",
        "# Set constants for PAD and UNK. You will use these values, but DO NOT change\n",
        "# them, or import additional packages.\n",
        "\n",
        "from collections import Counter\n",
        "PADDING_VALUE = 0\n",
        "UNK_VALUE     = 1\n",
        "\n",
        "# ===========================================================================\n",
        "\n",
        "\n",
        "def split_train_val_test(df, props=[.8, .1, .1]):\n",
        "    \"\"\"\n",
        "    This method takes a dataframe and splits it into train/val/test splits.\n",
        "    It uses the props argument to split the dataset appropriately.\n",
        "\n",
        "    Args:\n",
        "      df (pd.DataFrame): A dataset as a Pandas DataFrame\n",
        "      props (list): Proportions for each split in the order of [train, validation, test].\n",
        "                    the last value of the props array is repetitive, but we've kept it for clarity.\n",
        "\n",
        "    Returns:\n",
        "      train_df (pd.DataFrame): Train DataFrame split.\n",
        "      val_df (pd.DataFrame): Validation DataFrame split.\n",
        "      test_df (pd.DataFrame): Test DataFramem split.\n",
        "    \"\"\"\n",
        "    assert round(sum(props), 2) == 1 and len(props) >= 2\n",
        "    train_df, test_df, val_df = None, None, None\n",
        "\n",
        "    ### BEGIN YOUR CODE (~3-5 lines) ###\n",
        "    ### Hint: You can use df.iloc to slice into specific indexes or ranges.\n",
        "\n",
        "    # Get length of dataframe\n",
        "    df_len = len(df)\n",
        "\n",
        "    # Get end location for training and validation (don't need test since that's the rest of the dataframe after val)\n",
        "    train_end = int(df_len * props[0])\n",
        "    val_end = int(df_len * (props[0] + props[1]))\n",
        "\n",
        "    # Split the dataframe based on the stored locations\n",
        "    train_df = df.iloc[:train_end]\n",
        "    val_df = df.iloc[train_end:val_end]\n",
        "    test_df = df.iloc[val_end:]\n",
        "\n",
        "    ### END YOUR CODE ###\n",
        "\n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "\n",
        "def generate_vocab_map(df, cutoff=2):\n",
        "    \"\"\"\n",
        "    This method takes a dataframe and builds a vocabulary to unique number map.\n",
        "    It uses the cutoff argument to remove rare words occuring <= cutoff times.\n",
        "    *NOTE*: \"\" and \"UNK\" are reserved tokens in our vocab that will be useful\n",
        "    later. You'll also find the Counter imported for you to be useful as well.\n",
        "\n",
        "    Args:\n",
        "      df (pd.DataFrame): The entire dataset this mapping is built from\n",
        "      cutoff (int): We exclude words from the vocab that appear less than or\n",
        "                    eq to cutoff\n",
        "\n",
        "    Returns:\n",
        "      vocab (dict[str, int]):\n",
        "        In vocab, each str is a unique token, and each dict[str] is a\n",
        "        unique integer ID. Only elements that appear > cutoff times appear\n",
        "        in vocab.\n",
        "\n",
        "      reversed_vocab (dict[int, str]):\n",
        "        A reversed version of vocab, which allows us to retrieve\n",
        "        words given their unique integer ID. This map will\n",
        "        allow us to \"decode\" integer sequences we'll encode using\n",
        "        vocab!\n",
        "    \"\"\"\n",
        "\n",
        "    vocab          = {\"\": PADDING_VALUE, \"UNK\": UNK_VALUE}\n",
        "    reversed_vocab = {}\n",
        "\n",
        "    ### BEGIN YOUR CODE (~5-15 lines) ###\n",
        "    ### Hint: Start by iterating over df[\"tokenized\"]\n",
        "\n",
        "    allTokens = []\n",
        "\n",
        "    # Make a dictionary of words and their counts\n",
        "    for tokens in df[\"tokenized\"]:\n",
        "      for token in tokens:\n",
        "        allTokens.append(token)\n",
        "\n",
        "    tokenCounter = Counter(allTokens)\n",
        "\n",
        "    # Start iterator for unique word IDs\n",
        "    idCounter = 2\n",
        "\n",
        "    # For each word in the dictionary, if it meets the minimum frequency add it and increase the word ID counter\n",
        "    for token in tokenCounter:\n",
        "      if tokenCounter[token] > cutoff:\n",
        "        vocab[token] = idCounter\n",
        "        idCounter += 1\n",
        "\n",
        "    # Create a new dictionary of the flipped values\n",
        "\n",
        "    for token, wordID in vocab.items():\n",
        "       reversed_vocab[wordID] = token\n",
        "\n",
        "    ### END YOUR CODE ###\n",
        "\n",
        "    return vocab, reversed_vocab"
      ],
      "id": "zeo9kX6i9pbH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9LEk83hRFgT"
      },
      "source": [
        "With the methods you have implemented above, we can now split the dataset into training, validation, and testing sets and generate our dictionaries mapping from word tokens to IDs (and vice versa).\n",
        "\n",
        "*Note: The props list currently being used splits the dataset so that 80% of samples are used to train, and the remaining 20% are evenly split between training and validation. How you split your dataset is itself a major choice and something you would need to consider in your own projects. Can you think of why?*"
      ],
      "id": "w9LEk83hRFgT"
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "rcmX931OFEId"
      },
      "outputs": [],
      "source": [
        "df                         = df.sample(frac=1)\n",
        "train_df, val_df, test_df  = split_train_val_test(df, props=[.8, .1, .1])\n",
        "train_vocab, reverse_vocab = generate_vocab_map(train_df, 1)"
      ],
      "id": "rcmX931OFEId"
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "CAACzA8YFEId",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de33d086-3fbc-4e15-a2e4-66d79edf90ba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8, 0.1, 0.1)"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "# ===========================================================================\n",
        "# This line of code will help test your implementation, the expected output is\n",
        "# the same distribution used in 'props' in the above cell. Try out some\n",
        "# different values to ensure it works, but for submission ensure you use\n",
        "# [.8, .1, .1]\n",
        "# ===========================================================================\n",
        "\n",
        "(len(train_df) / len(df)), (len(val_df) / len(df)), (len(test_df) / len(df))"
      ],
      "id": "CAACzA8YFEId"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fCFfEHv1hnI"
      },
      "source": [
        "### 1.4 Building a Dataset Class"
      ],
      "id": "5fCFfEHv1hnI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-qTQQa2FEIe"
      },
      "source": [
        "PyTorch has custom Dataset Classes that have very useful extentions, we want to turn our current pandas DataFrame into a subclass of Dataset so that we can iterate and sample through it for minibatch updates. **In the following cell, fill out the `HeadlineDataset` class.** Refer to PyTorch documentation on [Dataset Classes](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n",
        "for help."
      ],
      "id": "8-qTQQa2FEIe"
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "tqt9q92J1QKK"
      },
      "outputs": [],
      "source": [
        "# ===========================================================================\n",
        "# Please do not change, or import additional packages.\n",
        "from torch.utils.data import Dataset\n",
        "# ===========================================================================\n",
        "\n",
        "class HeadlineDataset(Dataset):\n",
        "  \"\"\"\n",
        "  This class takes a Pandas DataFrame and wraps in a PyTorch Dataset.\n",
        "  Read more about Torch Datasets here:\n",
        "  https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, vocab, df, max_length=50):\n",
        "    \"\"\"\n",
        "    Initialize this class with appropriate instance variables\n",
        "\n",
        "    We would *strongly* recommend storing the dataframe itself as an instance\n",
        "    variable, and keeping this method very simple. Leave processing to\n",
        "    __getitem__.\n",
        "\n",
        "    Sometimes, however, it does make sense to preprocess in __init__. If you\n",
        "    are curious as to why, read the aside at the bottom of this cell.\n",
        "    \"\"\"\n",
        "\n",
        "    ### BEGIN YOUR CODE (~3 lines) ###\\\n",
        "    self.vocab = vocab\n",
        "    self.df  = df\n",
        "    self.MAX_LEN = max_length\n",
        "\n",
        "    ### END YOUR CODE ###\n",
        "\n",
        "  def __len__(self):\n",
        "    \"\"\"\n",
        "    Return the length of the dataframe instance variable\n",
        "    \"\"\"\n",
        "    df_len = self.df.shape[0]\n",
        "\n",
        "    ### BEGIN YOUR CODE (~1 line) ###\n",
        "\n",
        "    ### END YOUR CODE ###\n",
        "\n",
        "    return df_len\n",
        "\n",
        "  def __getitem__(self, index: int):\n",
        "    \"\"\"\n",
        "    Converts a dataframe row (row[\"tokenized\"]) to an encoded torch LongTensor,\n",
        "    using our vocab map created using generate_vocab_map. Restricts the encoded\n",
        "    headline length to max_length.\n",
        "\n",
        "    The purpose of this method is to convert the row - a list of words - into\n",
        "    a corresponding list of numbers.\n",
        "\n",
        "    i.e. using a map of {\"hi\": 2, \"hello\": 3, \"UNK\": 0}\n",
        "    this list [\"hi\", \"hello\", \"NOT_IN_DICT\"] will turn into [2, 3, 0]\n",
        "\n",
        "    Returns:\n",
        "      tokenized_word_tensor (torch.LongTensor):\n",
        "        A 1D tensor of type Long, that has each token in the dataframe mapped to\n",
        "        a number. These numbers are retrieved from the vocab_map we created in\n",
        "        generate_vocab_map.\n",
        "\n",
        "        **IMPORTANT**: if we filtered out the word because it's infrequent (and\n",
        "        it doesn't exist in the vocab) we need to replace it w/ the UNK token.\n",
        "\n",
        "      curr_label (int):\n",
        "        Binary 0/1 label retrieved from the DataFrame.\n",
        "\n",
        "    \"\"\"\n",
        "    tokenized_word_tensor = None\n",
        "    curr_label            = None\n",
        "\n",
        "    ### BEGIN YOUR CODE (~3-7 lines) ###\n",
        "\n",
        "    # Get the first MAX_LENGTH words in the headline, if it doesn't exist in the vocab replace it with UNK\n",
        "    row = self.df.iloc[index]\n",
        "    headline_array = row[\"tokenized\"][:self.MAX_LEN]\n",
        "\n",
        "    for i, word in enumerate(headline_array):\n",
        "      if word not in self.vocab:\n",
        "        headline_array[i] = \"UNK\"\n",
        "\n",
        "\n",
        "    # Get the ID of each word (or UNK) in the cleaned list for the headline\n",
        "    tokens_array = []\n",
        "    for word in headline_array:\n",
        "      tokens_array.append(self.vocab[word])\n",
        "\n",
        "    tokenized_word_tensor = torch.tensor(tokens_array, dtype=torch.long)\n",
        "\n",
        "    # Grab the label for the current index\n",
        "    curr_label = int(row[\"label\"])\n",
        "\n",
        "    ### END YOUR CODE ###\n",
        "\n",
        "    return tokenized_word_tensor, curr_label\n",
        "\n",
        "\n",
        "# ===========================================================================\n",
        "# Completely optional aside on preprocessing in __init__.\n",
        "#\n",
        "# Sometimes the compute bottleneck actually ends up being in __getitem__.\n",
        "# In this case, you'd loop over your dataset in __init__, passing data\n",
        "# to __getitem__ and storing it in another instance variable. Then,\n",
        "# you can simply return the preprocessed data in __getitem__ instead of\n",
        "# doing the preprocessing.\n",
        "#\n",
        "# There is a tradeoff though: can you think of one?\n",
        "# ==========================================================================="
      ],
      "id": "tqt9q92J1QKK"
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "KuLtIOAZFEIe"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import RandomSampler\n",
        "\n",
        "train_dataset = HeadlineDataset(train_vocab, train_df)\n",
        "val_dataset   = HeadlineDataset(train_vocab, val_df)\n",
        "test_dataset  = HeadlineDataset(train_vocab, test_df)\n",
        "\n",
        "\n",
        "# Now that we're wrapping our dataframes in PyTorch datsets, we can make use of\n",
        "# PyTorch Random Samplers, they'll define how our DataLoaders sample elements\n",
        "# from the HeadlineDatasets\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "val_sampler   = RandomSampler(val_dataset)\n",
        "test_sampler  = RandomSampler(test_dataset)"
      ],
      "id": "KuLtIOAZFEIe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9iBiSKF1yXA"
      },
      "source": [
        "### 1.5 Finalizing our DataLoader"
      ],
      "id": "n9iBiSKF1yXA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfXSbxoFFEIe"
      },
      "source": [
        "We can now use PyTorch `DataLoader` to batch our data for us. **In the following cell, please implement `collate_fn`.** Refer to PyTorch documentation on [`DataLoader`](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) for help."
      ],
      "id": "lfXSbxoFFEIe"
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "Zp1aQAvn1_mz"
      },
      "outputs": [],
      "source": [
        "# ===========================================================================\n",
        "# Please do not change, or import additional packages.\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "# ===========================================================================\n",
        "\n",
        "def collate_fn(batch, padding_value=PADDING_VALUE):\n",
        "  \"\"\"\n",
        "  This function is passed as a parameter to Torch DataSampler. collate_fn collects\n",
        "  batched rows, in the form of tuples, from a DataLoader and applies some final\n",
        "  pre-processing.\n",
        "\n",
        "  Objective:\n",
        "    In our case, we need to take the batched input array of 1D tokenized_word_tensors,\n",
        "    and create a 2D tensor that's padded to be the max length from all our tokenized_word_tensors\n",
        "    in a batch. We're moving from a Python array of tuples, to a padded 2D tensor.\n",
        "\n",
        "    *HINT*: you're allowed to use torch.nn.utils.rnn.pad_sequence (ALREADY IMPORTED)\n",
        "\n",
        "    Finally, you can read more about collate_fn here: https://pytorch.org/docs/stable/data.html\n",
        "\n",
        "  Args:\n",
        "    batch: PythonArray[tuple(tokenized_word_tensor: 1D Torch.LongTensor, curr_label: int)]\n",
        "           len(batch) == BATCH_SIZE\n",
        "\n",
        "  Returns:\n",
        "    padded_tokens: 2D LongTensor of shape (BATCH_SIZE, max len of all tokenized_word_tensor))\n",
        "    y_labels: 1D FloatTensor of shape (BATCH_SIZE)\n",
        "\n",
        "  \"\"\"\n",
        "  padded_tokens, y_labels = None, None\n",
        "\n",
        "  ### BEGIN YOUR CODE (~4-8 lines) ###\n",
        "\n",
        "  # Pad each sequence with the given value to the longest in the batch\n",
        "  sequences = [row[0] for row in batch]\n",
        "  padded_tokens = pad_sequence(sequences, batch_first=True, padding_value=padding_value)\n",
        "  padded_tokens.transpose(1, 0)\n",
        "  # Create a tensor of labels for the batch, ground truth\n",
        "  y_labels = [row[1] for row in batch]\n",
        "  y_labels = torch.tensor(y_labels, dtype=torch.float)\n",
        "\n",
        "  ### END YOUR CODE ###\n",
        "\n",
        "  return padded_tokens, y_labels"
      ],
      "id": "Zp1aQAvn1_mz"
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "OayoJRTeFEIf"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, collate_fn=collate_fn)\n",
        "val_iterator   = DataLoader(val_dataset, batch_size=BATCH_SIZE, sampler=val_sampler, collate_fn=collate_fn)\n",
        "test_iterator  = DataLoader(test_dataset, batch_size=BATCH_SIZE, sampler=test_sampler, collate_fn=collate_fn)"
      ],
      "id": "OayoJRTeFEIf"
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "pidbg12AFEIf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8828a673-ed67-4123-c6b3-2f0f99b8436c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  277,    19,  3112,   104,   378,  3529,   290,  1568,   298,    56,\n",
            "           217,   848,  3054,   215,    65,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [   30,  3824,   106,  3825,    10,  8099,   456,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [ 6377,  2768,   764,  9650,    19,  1650,    39,     1,   923,  4781,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [    1,  2765,    15,  1887,   201,   226,     1,  3849,    10, 10836,\n",
            "          3459,  3351,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [ 1570,  5862,    15,  1164,  2866,    80,     1,    15,  9121,   686,\n",
            "            19, 11844,   729,   369,  1340,   231,  1570,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [ 3035,  8786,     1,   167,  6153,  1615,    19,  1044,  1616,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  676,    28,     1,  4186,   783,  2265,     1,    15,   424,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [ 2810,     1,     1,  2486,  6501,  1771,    19,  1027,  2894,   272,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [ 1193,    28,  4323,  4214,   369,  2262,  2192,     1,   237,    29,\n",
            "          4575,    40,  6842,    71,  5587,   104,    30,     1,   947,  2793,\n",
            "             1,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [    9,  2175,   165,  1422,  2711,    19,     1,  7185,    10,   444,\n",
            "           445,  8344,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [ 1148,  2758,   787,    68,    69,   292,   154,    40,  1973,   543,\n",
            "            19,  3057,  1151,    15,     1,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [ 8155,   586,  7905,  2159,  1406,  6834,    15,  1187,  4923,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [    1, 12178,   369,   464,  1159,    64,   232,    80,  3993,  6238,\n",
            "          2648,   519,  3330,   279,  2807,   533,    19,   133,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  205,    30,   929,    88,  3452,  7805,    88,  1332,  1985,    30,\n",
            "           929,    88,    68,  1660,  4668,  5477,  1995, 11486,  2187,  7805,\n",
            "           565,     1, 12102,     1,     0,     0,     0,     0],\n",
            "        [ 1761,  6842,    15,   640,   140,  6821,   124,  6614,   290,  6078,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  133,    28,   134,  2539,    19,  3328,   688,   960,   106,  4675,\n",
            "            30,   546,    88,  6578,   140,   177,  2514,     1,  1016,  4082,\n",
            "            19,   407,  1985,  8707,    94,    10,    75,     1]]) tensor([1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1.])\n",
            "x: torch.Size([16, 28])\n",
            "y: torch.Size([16])\n"
          ]
        }
      ],
      "source": [
        "# ===========================================================================\n",
        "# Use this to test your collate_fn implementation.\n",
        "#\n",
        "# You can look at the shapes of x and y or put print statements in collate_fn\n",
        "# while running this snippet\n",
        "# ===========================================================================\n",
        "\n",
        "for x, y in test_iterator:\n",
        "    print(x, y)\n",
        "    print(f'x: {x.shape}')\n",
        "    print(f'y: {y.shape}')\n",
        "    break\n",
        "test_iterator = DataLoader(test_dataset, batch_size=BATCH_SIZE, sampler=test_sampler, collate_fn=collate_fn)"
      ],
      "id": "pidbg12AFEIf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWLK7T1uFEIg"
      },
      "source": [
        "## 2. Modeling [10 points]\n",
        "Now that we have a clean dataset and a useful PyTorch `DataLoader` object, we can begin building a model for our task! In the following code block, you will build a feed-forward neural network implementing a neural bag-of-words baseline, `NBOW-RAND`, described in $\\S$2.1 of [this paper](https://www.aclweb.org/anthology/P15-1162.pdf). You may find [the PyTorch `torch.nn` docs](https://pytorch.org/docs/stable/nn.html) useful for understanding the different layers and [this PyTorch sequence models tutorial](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html) for how to put together `torch.nn` layers.\n",
        "\n",
        "The core intuition behind `NBOW-RAND` is that after we embed each word for our input, we average the embeddings to produce a single vector that hopefully averages the information across all embeddings. Formally, we first convert each document of length $n$ tokens into a matrix of $n\\times d$, where $d$ is the dimension of the token embedding. Then we average all embeddings to produce a vector of length $d$.\n",
        "\n",
        "If you are new to PyTorch, ensuring your matrix operations are correct is often the most common source of errors. Keep in mind how the dimensions change and what each axes represents. Your documents will be passed in as minibatches, so be careful when selecting which axes to apply certain operations. Feel free to experiment with the architecture of this network outside of the basic `NBOW-RAND` setup (such as adding in other layers) to see how this changes your results."
      ],
      "id": "BWLK7T1uFEIg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZDPs0Sf-H3V"
      },
      "source": [
        "### 2.1 Define the NBOW model class"
      ],
      "id": "pZDPs0Sf-H3V"
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "jzGx2q0jLqyU"
      },
      "outputs": [],
      "source": [
        "# ===========================================================================\n",
        "# Please do not change, or import additional packages.\n",
        "import torch.nn as nn\n",
        "# ===========================================================================\n",
        "\n",
        "class NBOW(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim):\n",
        "    \"\"\"\n",
        "    Instantiate layers for your model.\n",
        "    Your model architecture will be a feed-forward neural network.\n",
        "\n",
        "    You will need 3 nn.Modules at minimum\n",
        "     1. An embeddings layer (see nn.Embedding)\n",
        "     2. A linear layer (see nn.Linear)\n",
        "     3. A sigmoid output (see nn.Sigmoid)\n",
        "\n",
        "    HINT: In the forward step, the BATCH_SIZE is the first dimension.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    ### BEGIN YOUR CODE (~4 lines) ###\n",
        "\n",
        "    # Initialize embedding layer, turn word ids into representative vectors\n",
        "    self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    # Initilize linear layer, will receive a single vector of length embedding_dim\n",
        "    self.linear = nn.Linear(embedding_dim, 1)\n",
        "\n",
        "    # Intialize sigmoid layer, turn output from linear layer into a probabilistic(ish) score\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    ### END YOUR CODE ###\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Complete the forward pass of the model.\n",
        "\n",
        "    Use the output of the embedding layer to create the average vector,\n",
        "    which will be input into the linear layer.\n",
        "\n",
        "    Args:\n",
        "      x: 2D LongTensor of shape (BATCH_SIZE, max len of all tokenized_word_tensor))\n",
        "         This is the same output that comes out of the collate_fn function you completed\n",
        "    \"\"\"\n",
        "    ### BEGIN YOUR CODE (~4-5 lines) ###\n",
        "\n",
        "    # Embed each document, this produces a tensor of (minibatch, max doc len, embedding size)\n",
        "    embed = self.word_embeddings(x)\n",
        "\n",
        "    # Average over the second dimension, producing a tensor of size (minibatch, embedding size)\n",
        "    average_embeds = torch.mean(embed, dim=1)\n",
        "    # Pass average vector through linear layer\n",
        "    out = self.linear(average_embeds)\n",
        "    out = self.sigmoid(out)\n",
        "\n",
        "    # Get a probabilistic score from sigmoid layer and return scores\n",
        "    return out.squeeze(1)\n",
        "    ### END YOUR CODE ###\n"
      ],
      "id": "jzGx2q0jLqyU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xltosIzM-SP2"
      },
      "source": [
        "### 2.2 Initialize the NBOW classification model\n",
        "\n",
        "Since the NBOW model is rather basic, there is only one meaningful hyperparameter w.r.t. model architecture: the size of the embedding dimension (`embedding_dim`). (We also see a `vocab_size` parameter here, but this only a by-product on our cutoff for infrequent tokens, there also may more hyperparameters if you modified the architecture, such as adding a linear layer). Adjust the embedding dimension below when you start training your model. How big should your embedding dimension be? Recall that the embedding is a way to \"condense\" the sparsely populated tokenized vocabulary, so we suggest starting with values that are much lower than the vocab size.\n",
        "\n",
        "Remember the CUDA discussion in the first cell of this notebook? Here the `.to(device)` is where that discussion becomes relevant (if `device=='cuda'`, PyTorch will perform the matrix operations on GPU). If you recieve a mismatch error, your tensors may be on different devices."
      ],
      "id": "xltosIzM-SP2"
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "_HQWUu-ZFEIg"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_DIM = 10 # adjust here\n",
        "\n",
        "model = NBOW(\n",
        "  vocab_size    = len(train_vocab.keys()),\n",
        "  embedding_dim = EMBEDDING_DIM\n",
        ").to(device)"
      ],
      "id": "_HQWUu-ZFEIg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4CZnj1f-da-"
      },
      "source": [
        "### 2.3 Instantiate the loss function and optimizer"
      ],
      "id": "C4CZnj1f-da-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aXi8nA0FEIh"
      },
      "source": [
        "Please select and instantiate an appropriate loss function and optimizer.\n",
        "\n",
        "*Hint: What loss functions are availible for binary classification? Feel free to look at the [torch.nn docs on loss functions](https://pytorch.org/docs/stable/nn.html#loss-functions) for help!*"
      ],
      "id": "9aXi8nA0FEIh"
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "w98UvlXxFEIh"
      },
      "outputs": [],
      "source": [
        "# While we import Adam for you, you may try / import other optimizers as well\n",
        "from torch.optim import Adam\n",
        "\n",
        "\n",
        "criterion, optimizer = nn.BCELoss(), Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "### BEGIN YOUR CODE ###\n",
        "\n",
        "### END YOUR CODE ###"
      ],
      "id": "w98UvlXxFEIh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUXBtqPEjiRe"
      },
      "source": [
        "Now that we have a NBOW model, a loss function, optimizer and dataset, we can begin training!"
      ],
      "id": "hUXBtqPEjiRe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVLeTa8wFEIh"
      },
      "source": [
        "## 3. Training and Evaluation [10 points]\n",
        "We will now instantiate a `train_loop`, and a `val_loop` to evaluate our model at each epoch.\n",
        "\n",
        "**Fill out the train and test loops below. Treat real headlines as `False`, and Onion headlines as `True`.**"
      ],
      "id": "bVLeTa8wFEIh"
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "vganx5fCFEIh"
      },
      "outputs": [],
      "source": [
        "def train_loop(model, criterion, optim, iterator):\n",
        "  \"\"\"\n",
        "  Returns the total loss calculated from criterion\n",
        "  \"\"\"\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  for x, y in tqdm(iterator):\n",
        "    ### BEGIN YOUR CODE (~6 lines) ###\n",
        "\n",
        "    # Zero out the parameter gradients\n",
        "    optim.zero_grad()\n",
        "\n",
        "\n",
        "    # Do forward pass with current batch of input\n",
        "    classification_score = model(x)\n",
        "    # pred_y = (classification_score > 0.5).float()\n",
        "\n",
        "    # Get loss with model predictions and true labels\n",
        "    loss = criterion(classification_score, y)\n",
        "    # print(loss.shape)\n",
        "    loss.backward()\n",
        "    total_loss += loss.item()\n",
        "    optim.step()\n",
        "\n",
        "    ### END YOUR CODE ###\n",
        "\n",
        "  return total_loss\n",
        "\n",
        "\n",
        "def val_loop(model, iterator):\n",
        "  \"\"\"\n",
        "  Returns:\n",
        "    true (List[bool]): All the ground truth values taken from the dataset iterator\n",
        "    pred (List[bool]): All model predictions.\n",
        "  \"\"\"\n",
        "  true, pred = [], []\n",
        "\n",
        "  ### BEGIN YOUR CODE (~8 lines) ###\n",
        "\n",
        "  #put model into evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  # iterate over dataset and add predictions\n",
        "  for x, y in tqdm(iterator):\n",
        "    # print(\"input\", x)\n",
        "    classification_score = model(x)\n",
        "    pred_y = (classification_score > 0.5).float()\n",
        "\n",
        "    true.extend(y.tolist())\n",
        "    pred.extend(pred_y.tolist())\n",
        "\n",
        "  ### END YOUR CODE ###\n",
        "\n",
        "  return true, pred"
      ],
      "id": "vganx5fCFEIh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNXJevTu-tDZ"
      },
      "source": [
        "### 3.1 Define the evaluation metrics"
      ],
      "id": "JNXJevTu-tDZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IsZQs3rFEIi"
      },
      "source": [
        "We will also need evaluation metrics to tell us how well our model is doing on the validation set at each epoch and later how well the model does on the held-out test set. You may find $\\S$4.4.1 of Eisenstein useful for these questions.\n",
        "\n",
        "**Complete the functions in the following cell.**"
      ],
      "id": "7IsZQs3rFEIi"
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "gMQDg9Vy-wY0"
      },
      "outputs": [],
      "source": [
        "# Note: You will not need to import anything to implement these functions.\n",
        "\n",
        "def accuracy(true, pred):\n",
        "  \"\"\"\n",
        "  Calculate the ratio of correct predictions.\n",
        "\n",
        "  Args:\n",
        "    true (List[bool]): ground truth\n",
        "    pred (List[bool]): model predictions\n",
        "\n",
        "  Returns:\n",
        "    acc (float): percent accuracy with range [0, 1]\n",
        "  \"\"\"\n",
        "  acc = None\n",
        "  ### BEGIN YOUR CODE (~2-5 lines) ###\n",
        "  correct = 0\n",
        "  for i in range(len(true)):\n",
        "    if true[i] == pred[i]:\n",
        "      correct += 1\n",
        "  acc = correct / len(true)\n",
        "\n",
        "\n",
        "  ### END YOUR CODE ###\n",
        "  return acc\n",
        "\n",
        "\n",
        "def binary_f1(true, pred, selected_class=True):\n",
        "  \"\"\"\n",
        "  Calculate F-1 scores for a binary classification task.\n",
        "\n",
        "  Args:\n",
        "    true (List[bool]): ground truth\n",
        "    pred (List[bool]): model predictions\n",
        "    selected_class (bool): the selected class the F-1 is being calculated for.\n",
        "\n",
        "  Returns:\n",
        "    f1 (float): F-1 score between [0, 1]\n",
        "  \"\"\"\n",
        "  f1 = None\n",
        "  ### BEGIN YOUR CODE (~10-15 lines) ###\n",
        "\n",
        "  # iterate over truths and predictions to calculate true positive, false positive, etc.\n",
        "  true_positive = 0\n",
        "  true_negative = 0\n",
        "  false_positive = 0\n",
        "  false_negative = 0\n",
        "\n",
        "  for i in range(len(true)):\n",
        "    if true[i] == selected_class:\n",
        "      if true[i] == pred[i]:\n",
        "        true_positive += 1\n",
        "      else:\n",
        "        false_negative += 1\n",
        "    else:\n",
        "      if true[i] == pred[i]:\n",
        "        true_negative += 1\n",
        "      else:\n",
        "        false_positive += 1\n",
        "  # Calculate precision and recall\n",
        "  precision = true_positive / (true_positive + false_positive)\n",
        "  recall = true_positive / (true_positive + false_negative)\n",
        "\n",
        "  # Calculate F1 score\n",
        "  f1 = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "  ### END YOUR CODE ###\n",
        "  return f1\n",
        "\n",
        "\n",
        "def binary_macro_f1(true, pred):\n",
        "  \"\"\"\n",
        "  Calculate averaged F-1 for all selected (true/false) classes.\n",
        "\n",
        "  Args:\n",
        "    true (List[bool]): ground truth\n",
        "    pred (List[bool]): model predictions\n",
        "  \"\"\"\n",
        "  averaged_macro_f1 = None\n",
        "  ### BEGIN YOUR CODE (~1 line) ###\n",
        "\n",
        "  # Simply call the binary f1 method with each class then average their f1 scores\n",
        "  averaged_macro_f1 = (binary_f1(true, pred, True) + binary_f1(true, pred, False)) / 2\n",
        "\n",
        "  ### END YOUR CODE ###\n",
        "  return averaged_macro_f1"
      ],
      "id": "gMQDg9Vy-wY0"
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "Yw79JFieFEIi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "220eb96f-d4d2-421d-c115-fb4a54a315a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 150/150 [00:00<00:00, 339.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'float'> <class 'float'>\n",
            "Binary Macro F1: 0.4642504530751841\n",
            "Accuracy: 0.6029166666666667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ===========================================================================\n",
        "# To test your eval implementation, we will evaluate the untrained model on our\n",
        "# dev dataset. It will do pretty poorly (it's untrained), but the exact performance\n",
        "# will be random since the initialization of the model parameters is random.\n",
        "# ===========================================================================\n",
        "\n",
        "true, pred = val_loop(model, val_iterator)\n",
        "print(type(true[0]), type(pred[0]))\n",
        "print(f'Binary Macro F1: {binary_macro_f1(true, pred)}')\n",
        "print(f'Accuracy: {accuracy(true, pred)}')"
      ],
      "id": "Yw79JFieFEIi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2to0kWVFEIi"
      },
      "source": [
        "## 4. Full Training Run [5 points]\n",
        "Now we can perform a full run and see the model fit our loss. If everything goes correctly, you should be able to achieve a validation F1 score of at least `0.80`.\n",
        "\n",
        "**Feel free to adjust the number of epochs to prevent overfitting or underfitting and to play with your model hyperparameters/optimizer & loss function.**"
      ],
      "id": "Q2to0kWVFEIi"
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "N-iuqkKCFEIj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9128a7ec-cbed-433e-b636-774877d835c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:06<00:00, 194.96it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 351.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 0\n",
            "TRAIN LOSS: 764.3652588129044\n",
            "VAL F-1: 0.5776319258245519\n",
            "VAL ACC: 0.68875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:06<00:00, 183.97it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 524.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 1\n",
            "TRAIN LOSS: 657.1452968716621\n",
            "VAL F-1: 0.7110799833531767\n",
            "VAL ACC: 0.7575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:04<00:00, 267.15it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 571.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 2\n",
            "TRAIN LOSS: 537.9921697676182\n",
            "VAL F-1: 0.78597714129948\n",
            "VAL ACC: 0.8091666666666667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:05<00:00, 214.94it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 366.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 3\n",
            "TRAIN LOSS: 460.58085346221924\n",
            "VAL F-1: 0.8118685667535532\n",
            "VAL ACC: 0.8283333333333334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:04<00:00, 248.56it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 524.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 4\n",
            "TRAIN LOSS: 405.77545634657145\n",
            "VAL F-1: 0.8246749559984103\n",
            "VAL ACC: 0.8391666666666666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:05<00:00, 214.27it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 359.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 5\n",
            "TRAIN LOSS: 370.59475677087903\n",
            "VAL F-1: 0.8436372630920657\n",
            "VAL ACC: 0.8545833333333334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:05<00:00, 219.21it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 552.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 6\n",
            "TRAIN LOSS: 338.5805185250938\n",
            "VAL F-1: 0.847221509210299\n",
            "VAL ACC: 0.8579166666666667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:04<00:00, 254.27it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 541.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 7\n",
            "TRAIN LOSS: 315.9561192803085\n",
            "VAL F-1: 0.8476061750966837\n",
            "VAL ACC: 0.85875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:05<00:00, 209.35it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 554.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 8\n",
            "TRAIN LOSS: 292.03443878144026\n",
            "VAL F-1: 0.8501604494130247\n",
            "VAL ACC: 0.8620833333333333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:04<00:00, 262.76it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 518.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 9\n",
            "TRAIN LOSS: 274.5253622122109\n",
            "VAL F-1: 0.8567318138946263\n",
            "VAL ACC: 0.8666666666666667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "TOTAL_EPOCHS = 10\n",
        "for epoch in range(TOTAL_EPOCHS):\n",
        "    train_loss = train_loop(model, criterion, optimizer, train_iterator)\n",
        "    true, pred = val_loop(model, val_iterator)\n",
        "    print(f\"EPOCH: {epoch}\")\n",
        "    print(f\"TRAIN LOSS: {train_loss}\")\n",
        "    print(f\"VAL F-1: {binary_macro_f1(true, pred)}\")\n",
        "    print(f\"VAL ACC: {accuracy(true, pred)}\")"
      ],
      "id": "N-iuqkKCFEIj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_l91F4ooFEIj"
      },
      "source": [
        "We can also look at the models performance on the held-out test set, using the same `val_loop` from earlier."
      ],
      "id": "_l91F4ooFEIj"
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "vs8Fy_ncFEIo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d9fbfca-02e6-43fb-de3a-abd2599df141"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 150/150 [00:00<00:00, 541.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEST F-1: 0.8637364488455198\n",
            "TEST ACC: 0.8741666666666666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "true, pred = val_loop(model, test_iterator)\n",
        "print(f\"TEST F-1: {binary_macro_f1(true, pred)}\")\n",
        "print(f\"TEST ACC: {accuracy(true, pred)}\")"
      ],
      "id": "vs8Fy_ncFEIo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMPWmorEFEIp"
      },
      "source": [
        "## 5. Analysis [5 points]\n",
        "While modeling and accuracy are a great signal that our model is working in our specific task setup, an inspection of what the model is classifying (particularly its errors), can allow us to hypothesize about what is going on, why it works, and how to improve.\n",
        "\n"
      ],
      "id": "rMPWmorEFEIp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnjKlKt352hQ"
      },
      "source": [
        "### 5.1 Impact of Vocab Size\n",
        "**Question:** *What happens to the vocab size as you change the cutoff in the cell below? Can you explain this in the context of [Zipf's Law](https://en.wikipedia.org/wiki/Zipf%27s_law)?*\n",
        "\n",
        "**Answer:** The vocab size decrease by a factor of ~40% when changing the cutoff from 1 to 2 and decresaes by a factor of ~20% when changing the cutoff frmo 2 to 3 as you can see below. This follows **Zipf's Law** which states that the frequency of a word is inversely proportional to its rank in the frequency table. In other words, a small number of words appear very frequently, while the majority occur rarely, so increasing the cutoff disproportionately removes less common words, leading to a sharp drop in vocabulary size as the cutoff incresease but less and less percent of words get cutoff as you incresae the cuttoff threshold."
      ],
      "id": "fnjKlKt352hQ"
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "pI0fM4oMFEIp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fba7c19-734d-4f60-ed35-679b27b7af35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cutoff=1: 13345\n",
            "cutoff=2: 9592\n",
            "cutoff=3: 7674\n"
          ]
        }
      ],
      "source": [
        "tmp_vocab, _ = generate_vocab_map(train_df, cutoff = 1)\n",
        "print(f\"cutoff=1: {len(tmp_vocab)}\")\n",
        "\n",
        "tmp_vocab, _ = generate_vocab_map(train_df, cutoff = 2)\n",
        "print(f\"cutoff=2: {len(tmp_vocab)}\")\n",
        "\n",
        "tmp_vocab, _ = generate_vocab_map(train_df, cutoff = 3)\n",
        "print(f\"cutoff=3: {len(tmp_vocab)}\")"
      ],
      "id": "pI0fM4oMFEIp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0x54B1lFEIp"
      },
      "source": [
        "### 5.2 Error Analysis\n",
        "\n",
        "*Can you describe what cases the model is getting wrong in the witheld test-set?*\n",
        "\n",
        "To do this, you will need to create a new `val_train_loop_incorrect` which returns incorrect sequences **and** you will need to decode these sequences back into words. You have already created a map that can convert encoded sequences back to regular English (`reverse_vocab`)."
      ],
      "id": "d0x54B1lFEIp"
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "TfohtPF8FEIp"
      },
      "outputs": [],
      "source": [
        "def val_train_loop_incorrect(model, iterator):\n",
        "  \"\"\"\n",
        "  Implement this however you like! It should look very similar to val_loop.\n",
        "  Pass the test_iterator through this function to look at errors in the test set.\n",
        "  \"\"\"\n",
        "\n",
        "  # Make a list to hold the sequences the model mis-classified\n",
        "  incorrect_seqs = []\n",
        "\n",
        "  # Put model into evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  # Iterate over test data\n",
        "  for x, y in tqdm(iterator):\n",
        "    # print(\"input\", x)\n",
        "    classification_score = model(x)\n",
        "    pred_y = (classification_score > 0.5).float()\n",
        "\n",
        "    y = y.tolist()\n",
        "    pred_y = pred_y.tolist()\n",
        "\n",
        "    # print(type(y[0]), type(pred_y[0]))\n",
        "\n",
        "    for i in range(len(pred_y)):\n",
        "      if pred_y[i] != y[i]:\n",
        "        incorrect_seqs.append(x[i].tolist())\n",
        "\n",
        "\n",
        "\n",
        "  # Determine incorrect sequences\n",
        "\n",
        "  # Decode data back into English words\n",
        "  # (HINT: you should use the dictionary, reverse_vocab, defined earlier)\n",
        "  # (SECOND HINT: make sure you stop decoding once you hit padding tokens!)\n",
        "\n",
        "  for i, seq in enumerate(incorrect_seqs):\n",
        "    for j, token in enumerate(seq):\n",
        "      if token == PADDING_VALUE:\n",
        "        incorrect_seqs[i] = incorrect_seqs[i][:j]\n",
        "        break\n",
        "\n",
        "      incorrect_seqs[i][j] = reverse_vocab[int(token)]\n",
        "\n",
        "    incorrect_seqs[i] = \" \".join(incorrect_seqs[i])\n",
        "\n",
        "\n",
        "  return incorrect_seqs"
      ],
      "id": "TfohtPF8FEIp"
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "6-azPje88iU0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32901808-2055-4313-801d-c84b97c8ef8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 150/150 [00:00<00:00, 466.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6283783783783784\n",
            "['UNK bills fan not even banned from stadium', 'man billed thousands for UNK he never received .', 'UNK jeff sessions tries to commit suicide by smoking joint', 'request : no UNK links .', 'school fires employee after post about UNK', 'UNK onion discovered in sweden', '1 in 4 americans apparently unaware the earth UNK the sun', \"furry orgy breaks out at UNK ' premiere .\", 'UNK UNK storms off stage after fans UNK his UNK solo', 'beauty pageant forces college girls to twerk for UNK internship', \"UNK 's sexual UNK impatient for gay marriage slippery slope to kick in\", 'god deploys 100,000 more mosquitoes to u.s .', 'UNK dolphins gm ask players to please use UNK email form when making trade requests', 'mom gets last new hairstyle', \"if you come into my pawn shop with a UNK machine and you tell me that you need to sell your UNK machine because you need cash , i 'm going to just look at you and not say anything until it UNK on you just what the trouble with\", 'at times like this , we need to pull ourselves up , hold our loved ones close , block any legislation that would prevent suspected terrorists from buying guns , and say a prayer for the victims', 'UNK UNK has to tackle issue of too many single men making women feel uncomfortable', 'high-tech mirror for cancer patients only works if you smile', 'gynecologist can ’ t believe she has to tell women not to put UNK in their vaginas', '1 in 4 americans unaware that earth UNK sun', \"friend 's excuses for why he ca n't hang out getting more UNK over time\", 'UNK s ..... x UNK', 'epa promotes UNK black sludge to deputy director', 'so-called professional gamer not even racist', \"liam UNK on why there wo n't be a taken 3 : `` she can ’ t get taken again . that ’ s just bad parenting ''\", 'UNK anthem played before UNK match , organizers apologize to UNK', 'baltimore pigeons shocked to find beloved shitting statues gone', 'black conservatives support candidate whose religion believes black people bear mark of cain', '10 cat UNK for the blind', 'arnold UNK says ‘ i ’ m back ’ after UNK from emergency heart surgery', \"the collapse of the animal crossing economy and the rise of villager trading : `` UNK reads like some kind of grotesque slave exchange ''\", 'let us never forget the legendary david “ kim ” UNK', 'new law requires sex offenders to inform residents before moving into their homes', 'donald trump just tweeted a campaign ad featuring what look like nazi soldiers', 'critics say will smith isn ’ t black enough to play serena and venus williams ’ father in king richard', 'man who UNK coworker with bottle for changing michael jackson song just beat the charges', 'UNK woman on train quickly doing plastic surgery on face before work', 'UNK chinese kid cuts off own finger with kitchen knife', \"bro , you 're a god among bros\", 'pope francis holds sex abuse summit', 'this 19-year-old is paying her way through college by naming over UNK chinese babies', 'UNK alerts viewers of dish dropping the channel , dish UNK by replacing UNK UNK with nickelback concert ( xpost )', 'obama : only native americans can UNK object to immigration', 'wealthy teen nearly experiences UNK', \"man says 'fuck it , ' eats lunch at UNK a.m. | the onion\", 'yankees sign UNK to 10-year , $ 420 million front office consultant contract', 'rick perry to donald trump : you , me , UNK contest', '13-year-old drinking prodigy accepted to ohio state', 'shock poll : latinos really UNK donald trump', 'new yorkers go 24 hours without shooting , stabbing , or UNK each other', 'feeling bad about feeling bad can make you feel worse', 'woman lets god drive car , god immediately runs down guy on motorcycle', 'going undercover : jeff sessions has disguised himself as a cartel member and strapped himself with a surveillance wire in order to read the wikipedia entry for marijuana', 'my neighbour murdered nearly all of my family , but now we are friends', 'an UNK , but a very UNK UNK', 'reason man turning to religion later in life must be horrifying', 'michael jackson estate releases new documentary UNK king of pop gets lifetime pass for ‘ thriller ’', 'UNK off the coast of seattle test positive for UNK', 'man shoots self in UNK while taking off pants', 'my ex - UNK , nickname : UNK . search can be registered', 'looking for empathy ? avoid alabama', 'silicon valley startup seeks to change the way women flee tech industry', 'denver ’ s flaming skull mayor announces plans to decriminalize magic mushrooms', 'usda admits weight loss not possible for people who don ’ t like salmon', 'trump UNK himself while claiming wall street journal UNK him about kim jong un', 'study finds controlled washington , d.c. wildfires UNK for restoring healthy political environment', 'redskins ’ UNK owner refuses to change team ’ s offensive name', 'nfl to curb excessive celebrations by removing areas of players ’ brains responsible for emotions', 'french president pledges to rebuild notre dame in 5 years', 'trump orders presidential motorcade to take detour through homeless encampment', 'that fighter who died in the cage last weekend wants you to know he ’ s feeling much better', 'new yorkers protest long shadows cast by new skyscrapers', 'no accessible UNK at department in charge of accessibility act', 'positive UNK about the future linked to increased symptoms of depression', 'beer summit UNK : president trump has invited the racist nyc lawyer and the woman he yelled at to the white house so the lawyer can finish yelling at her', 'zoo hosts contest to name baby of pregnant gift shop worker', 'manslaughter laws sure are confusing', 'UNK isis prisoners hail american UNK', 'patriots win super bowl', \"'can i put my hand there ? ' : new york law changes the rules of the college UNK\", '15 episodes of ‘ seinfeld ’ when jerry UNK a woman with man hands', 'republicans ask joe biden to stop embarrassing america', \"UNK woman angrily demanding price check on rice pudding was once carefree youth , onlookers speculate '\", 'martin shkreli faces UNK stay in prison system where inmates who funded hair theft are lowest UNK', '8 ikea shopping hacks everyone should know', 'fortnite addiction UNK to 5 % of all uk divorces this year', 'tim kaine found riding conveyor belt during factory campaign stop', 'chelsea manning , reality winner excitedly hoping nation ’ s UNK approval of whistleblowers will get them out of jail', 'cameron signs up with tinder to woo young single voters', 'hundreds of pet UNK abandoned after harry potter craze UNK', 'eric UNK : schools ‘ pushing the liberal agenda ’ by teaching UNK', 'facebook bans extremist figures after UNK them dangerous to its public reputation', 'chuck UNK to leave the neil gorsuch hearing early to make his bedtime', 'report : make it stop', 'new music festival just large empty field to do drugs in', 'kim jong-un takes UNK from fashion week , reveals UNK new haircut', 'new york monument honors victims of giant octopus attack that never occurred', 'rob ford ’ s treatment ‘ kind of like what they did to jesus , ’ says brother doug', 'pr exec tweets , ‘ going to africa . hope i don ’ t get aids . just UNK . i ’ m white ! ’', \"average americans think they 're smarter than the average american\", 'UNK going UNK to avoid humans', 'movie theater security reports suspicious behavior after patron buys ticket to ‘ UNK man ’', 'victim of mall shooting determined not to die in yankee candle', 'study : UNK asian women remain UNK in media', \"hitler 's toilet has been in new jersey all this time\", 'world of warcraft classic UNK testers are reporting vanilla wow features as bugs', 'opening staff UNK against incompetence of closing staff', 'UNK UNK UNK UNK past earth this week causing no harm or danger .', 'UNK from u.s. , china meet in UNK talks', 'father launches campaign to find son ’ s hit and run driver only to UNK he was responsible', 'UNK who identifies as UNK sets world cycling record', 'science guy bill nye killed in massive UNK / UNK explosion', 'pimp my ride : hamas proudly shows off a tank , turns out to just be a car', 'meryl streep UNK more often than god in oscar acceptance UNK', 'federal government adds 600,000 acres to national forbidden zone', 'mark zuckerberg can ’ t believe india isn ’ t grateful for facebook ’ s free internet', 'there ’ s no use worrying over things the parasitic alien UNK living inside us can ’ t control', 'plan to UNK california into 3 new states clears first UNK', 'trump declares : ‘ i ’ m not a fan of hitler ’', 'man suing almost everyone for all the money on earth', \"jupiter 's liberals worried about their UNK footprint\", \"victim recalls UNK 's breasts , little else\", 'wtf', 'shark falls from sky onto golf course ? - yahoo ! news', 'family chooses different dog than reincarnated grandfather', 'less popular friend proposes UNK birthdays into single party', 'news : sending a message : following the russian lawyer scandal , nike has pulled its $ UNK million endorsement deal with donald trump jr .', 'jeremy the snail still lonely after potential UNK only have UNK for each other', \"dying lion sure does n't feel as though he 's completing some great cosmic circle\", 'john boehner ’ s primary opponent just hit him for ‘ UNK UNK ’ yes , seriously .', 'mauled shop', 'north korea open to UNK nuclear arms', 'survey : americans watching better sex than ever', 'UNK scientists UNK present UNK UNK UNK in comic sans', 'mask UNK', 'sad news , brits : the royal wedding has been delayed three months to give the band more time to learn ‘ UNK ’ on a prayer ’', 'logan park ‘ witches ’ put UNK on area to stop UNK', 'north korea : who would dare to UNK on kim jong-un ?', 'pakistani woman opens emergency exit door thinking it to be the toilet !', 'budweiser unveils social anxiety bottle with 900 % more label to pick at', 'man in hong kong reportedly beaten up outside cinema for leaking avengers : endgame UNK', 'last living UNK dies in captivity', 'registered florida sex predator wins $ 3 million lottery jackpot', 'cracker jack ’ d : new version of cracker UNK to contain caffeine', 'police union decries UNK UNK of justice in amber UNK murder conviction', 'man doesn ’ t understand why people wasting time attacking him for running over their dog when trump the real enemy', 'biden criticized for appearing in UNK ads', 'amputee inspires others not to lose limbs', 'alabama bomb squad opens suspicious bags full of hot dogs', 'nhl warns hockey fans that banging on glass scares players', 'thought this belonged here . [ x-post from wtf ]', 'they can do better', 'woman set UNK by partner UNK him to death', 'donald trump declares his inauguration day national day of patriotic UNK', 'civilian UNK UNK to have been mistaken for hamas leader', 'frustrated dad at restaurant just wants a normal burger', 'u.s. warns of feral UNK UNK country from canada', 'mother renames son after UNK makes spelling error', 'male UNK of san francisco biotech UNK promise to make vaginas smell like UNK UNK', 'americans favor legalizing pot and UNK congress', 'youtube cuts popular live stream of giraffe about to give birth for ‘ nudity and sexual content ’', '“ the onion ” accidentally breaks true story about us sending israel weapons', 'man keeps memory of dead teen alive by making her center of elaborate political conspiracy theory', 'female friend group fails in one duty of providing good gynecologist recommendation', 'doom launched by bethesda at e3 2015 , UNK criticised for being too violent', 'now you can buy your occupy wall street poster from wal-mart', 'man at amusement park gets right back in line for another funnel cake', 'UNK missouri bill requires college students to learn about UNK', \"chinese factory worker ca n't believe the shit he makes for americans\", 'nasa launches david bowie concept mission', 'fast-food purchase UNK with UNK class conflict', 'resistance democrat racking brain for way to UNK anonymous whistleblower', 'big ben now has more wins in cleveland than every browns qb since UNK', 'taylor swift bought UNK', \"police called as playstation 4 tantrum leads to UNK ' son\", 'antonio UNK welcomes third child since UNK', '/ r / theonion hits UNK subscribers', 'doctor makes UNK alternative UNK before handing over drugs', 'taco hell', 'mean UNK dash hope for flying cars', 'supermarket introduces UNK wrapped bananas', 'UNK UNK proposed to replace UNK in colombia', 'UNK on cnn : transgender girls will walk around bathrooms with their genitals exposed', 'dentist pulls out all of her ex-boyfriend ’ s teeth after split', '‘ the case , mr. kerry , give me the case , ’ demands malaysian ambassador holding dangling john kerry from UNK towers UNK', 'pope francis : fake news is like getting sexually aroused by UNK', 'UNK v out just in time to be blamed for washington shootings', 'UNK sorry for playing UNK you like a hurricane ’ during miami game', 'UNK is set to release UNK new christmas movies this year', 'onion writer ’ s UNK UNK UNK headline hits close to home', 'putin ’ s birthday present is a UNK art show about how UNK and amazing he is', 'lady gaga panics after hearing name called for halftime show while waiting in line for bathroom', 'friend UNK UNK from navigation UNK after missing exit', 'scientists genetically UNK flies to UNK under red light', 'area UNK publication the onion sold to UNK ( seriously )', 'the neighbour walks her girlfriend on a leash . what do i tell my kid ?', 'samsung warns users to watch what they say in front of smart tvs', 'tgi fridays is a human right', 'cops : UNK UNK joe UNK mr. UNK in ice cream truck turf war', 'lobbyists call on barack obama to tone down UNK rhetoric', 'dc bars to open early for comey hearing watch parties', 'trump in possible trouble for tweets', 'north korea tests out new knife in smaller UNK of threats to u.s .', 'UNK liberals vow to back bernie whether he likes it or not', 'tsa : you can not pack your klingon bat ’ UNK in your UNK', 'time capsule from 50 years ago has nothing inside', 'trump vows to eat all UNK pork products china refuses to UNK', 'how have we UNK without biscuits ?', 'anti-vaccine course brings u of t one step closer to offering a masters of UNK', 'woody UNK wears pajamas to premiere , like he gives a shit , he was on cheers - starwipe', \"amazon completes new suspension tank to house psychic beings who UNK customers ' future orders\", 'katy perry drops hints that super bowl halftime show will be awful', 'report : clippers owner donald sterling told girlfriend not to bring black people to game', 'UNK attempts to increase UNK by no longer mailing out free $ 500 a month to subscribers', 'kid rock apparently divorced UNK anderson because of UNK', 'trump staffer grateful to work with so many people he could turn over to fbi in exchange for immunity', \"the new american healthcare plan is called `` world 's greatest healthcare plan of 2017 ''\", \"do obama 's small UNK explain his liberal politics ?\", 'thousands of students forced to attend iowa state after university sets acceptance rate to 140 %', 'smoking UNK UNK milk could UNK depression for up to 4 weeks', 'american is fighter : i made a bad decision', \"area man creeped out by request to UNK love '\", 'bill cosby attacks UNK behavior , skyrocketing crime rate among elderly black male UNK', '75 year old explanation for why UNK are bad locked up behind UNK', 'this ‘ smart condom ’ will give UNK into your sex life you probably didn ’ t want', 'meth actually not that bad for you , report doctors UNK stereo', \"mit researchers conclude that UNK UNK hats UNK the government 's ability to read your mind .\", 'man who downloaded $ 2.99 meditation app prepares to enter UNK plane of eternal UNK', '84 % support marijuana legalization', 'orgy a UNK nightmare', 'UNK website the onion accidentally breaks story about the us offering missiles to israel', 'nasa plans first all-female UNK', \"man thanks god he 's not sexually attracted to children\", '5 UNK test positive for meth , must UNK races they won at lone star park', 'african-american neighborhood UNK by ask murderer', 'pastor UNK self in coffin until easter morning', \"dunkin ' donuts new strategy to spice up sales : fewer donuts\", 'fox news settles harassment suit against bill o ’ reilly', 'nevada death row inmate placed on suicide watch', 'dog watches the UNK , tries to warn family of UNK doom', 'man ’ s eyes glaze over whenever politician starts threatening to UNK him into UNK UNK', 'man wearing cobra command shirt missed the whole point of ‘ UNK . joe ’', \"elderly americans are dying without getting to read mueller 's report - and they 're not happy about it\", 'after 200 UNK it ’ s a girl !', 'mark zuckerberg confirms that he is not actually a lizard', 'new urine UNK walls in san francisco shoot pee back', \"fan ca n't believe he left 11 seconds into ronda rousey fight\", '42 million dead in UNK black friday weekend on record', 'one way walmart is UNK workers : less UNK UNK on the pa system', \"congressman UNK in sexting scandal explains : ' i wanted that girl to see my penis '\", 'UNK used ‘ UNK UNK , simple UNK ’ to explain trade to trump during meeting', \"'help has to be on the way now , ' thinks syrian man currently being UNK\", 'literally a third of world bank policy reports have never , ever been read online , by anyone', 'mom UNK into school office and slaps wrong child', 'bear downs 36 UNK , passes out at UNK', 'my big fat UNK wedding ruined my life : an open letter to channel 4 | tv & & film | UNK times', 'butterfly UNK filled with junk mail', 'UNK UNK ’ s son UNK opponent in derby match', 'heartwarming : when chris evans heard a fan had terminal cancer , he killed himself so they could hang out in heaven', 'UNK , on reddit , dudes can ’ t stop talking about fucking UNK', 'report : iranian science teachers may be UNK students', 'antonio UNK is going to fashion school so he can bring UNK back to UNK', 'most infamous UNK in history', \"bush : UNK long national nightmare of peace and UNK is finally over '\", '7 hacks to get the most out of your chipotle order', 'kim kardashian tries to escape l.a. in rowboat after realizing past 12 years of life have been tv show', 'UNK man does not maintain erection during national anthem', 'military apologizes after drone strike intended for yemeni isis base accidentally hits west UNK beach wedding', 'UNK to britain : massive power surges caused by millions of people simultaneously making tea', 'teachers still more effective at UNK than ‘ assassin ’ s UNK ’', 'study : congress literally doesn ’ t care what you think', 'rio 2016 : officials forced to cut their way into olympic stadium after losing keys to gate', 'taliban using facebook pictures of beautiful women to lure soldiers into giving out intelligence .', 'friends for 60 years find out they ’ re biological brothers', 'legalizing gay marriage lowers teen suicide', 'real estate developers push to rebrand murder heights neighborhood of baltimore', 'for men looking for great single women , online dating offers a UNK solution to the otherwise UNK UNK of finding long-term love . UNK', \"copy of 'the UNK letter ' ca n't believe the notes high schooler writing in UNK\", 'professor : if you read to your kids , you ’ re ‘ UNK UNK ’ others', 'paris seeks eiffel tower renovations', 'perverted creep keeps asking women what they ’ re wearing', 'bill belichick UNK seven times this season', 'paul ryan UNK the pope', 'casual drink with acquaintance actually first move in elaborate chess game to get hired at UNK', 'researchers release oil into north sea to study immediate result of an oil spill', 'UNK for pleasure here ...', 'home depot releases new UNK UNK hose', \"`` UNK ' across america '' may be the onion 's UNK , most fucked up series ever .\", 'UNK UNK locker room', 'you can hold snake , owner reports', 'glenn beck thinks watch dogs teaches you how to hack the real world', 'sarah palin admits she has no idea what she ’ s talking about', 'washing machine loses man ’ s trust', 'male substitute teacher with UNK UNK in mystery', 'local water tower celebrates UNK year as UNK of information on who is a slut', 'nfl ending partnership with the national UNK of health on concussion study']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "incorrect_seqs = val_train_loop_incorrect(model, test_iterator)\n",
        "counter = 0\n",
        "for seq in incorrect_seqs:\n",
        "  if \"UNK\" in seq:\n",
        "    counter += 1\n",
        "print(counter / len(incorrect_seqs))\n",
        "print(incorrect_seqs)"
      ],
      "id": "6-azPje88iU0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUoLTGAScre7"
      },
      "source": [
        "Now that we have our incorrect sequences:   \n",
        "**Question:** *Can you describe what cases the model is getting wrong in the witheld test-set?*\n",
        "\n",
        "**Answer:** The model is having trouble recognizing unknown vocabulary since a common theme in all the sequences this model is getting wrong is the appearence of the \"UNK\" token. This means that the vocab cutoff removed many lower frequency words, which may have lead to a loss of important context and meaning in these test cases. This causes the data to have sentences that do not make coherent sense or are semintically incomplete because the cutoff threshold caused us to remove important aspects of the sentnce like proper nouns, domain specific language, or even slang.\n",
        "\n",
        "Some examples of this include, the sentence \"cops bust filthy, UNK mark zuckerberg for selling personal data on street corner,\" where it is not clear what \"UNK\" is replacing. Given the setence, it can be an important adjective or proper noun that can change the meaning of the sentence completely. Similarly, in the example \"'game of thrones' UNK demands trial by combat in new york supreme court case\" the missing word could refer to domain specific (in this case game of thrones specific) language or it could be another word that could provide context on the humor in the sentence. Those examples show that the model is struggling with unknown vocab especially with proper nouns and domain specific language."
      ],
      "id": "mUoLTGAScre7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mY8S9ZK9zuVs"
      },
      "source": [
        "## 6. Submit Your Homework\n",
        "This is the end of Project 1. Congratulations!  \n",
        "\n",
        "Now, follow the steps below to submit your homework in [Gradescope](https://www.gradescope.com/courses/939466):\n",
        "\n",
        "1. Rename this ipynb file to `CS4650_p1_GTusername.ipynb`. Make sure all cells have been run. We recommend ensuring you have removed any extraneous cells & print statements, clearing all outputs, and using the Runtime --> Run all tool to make sure all output is update to date.\n",
        "2. Click on the menu 'File' --> 'Download' --> 'Download .py'.\n",
        "3. Click on the menu 'File' --> 'Download' --> 'Download .ipynb'.\n",
        "4. Download the notebook as a .pdf document. Make sure the output from Part 4 are captured so we can see how the loss, F1, & accuracy changes while training.\n",
        "5. **Double check to make sure you answered the cell text questions (sections 5.1 and 5.2)!** Every year some students forget to answer them and get points taken off.\n",
        "5. Upload all 3 files to Gradescope. Double check the files start with `CS4650_p1_*`, capitalization matters."
      ],
      "id": "mY8S9ZK9zuVs"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}