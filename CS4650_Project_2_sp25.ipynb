{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daiwikpal/NLP/blob/main/CS4650_Project_2_sp25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yj21IyLWKoxk"
      },
      "outputs": [],
      "source": [
        "# Licensing Information:  You are free to use or extend this project for\n",
        "# educational purposes provided that (1) you do not distribute or publish\n",
        "# solutions, (2) you retain this notice, and (3) you provide clear\n",
        "# attribution to The Georgia Institute of Technology, including a link to https://aritter.github.io/CS-7650/\n",
        "# Attribution Information: This assignment was developed at The Georgia Institute of Technology\n",
        "# by Alan Ritter (alan.ritter@cc.gatech.edu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEuVjuVkkRQw"
      },
      "source": [
        "# 📘 Named Entity Recognition (NLP Project 2)\n",
        "**Course: CS 4650 \"Natural Language Processing\"**  \n",
        "**Institution: Georgia Institute of Technology**  \n",
        "**Semester: Spring 2025**  \n",
        "**Instructor: Dr. Wei Xu**  \n",
        "**Teaching Assistants: Tarek Naous, Jonathan Zheng, Xiaofeng Wu, Yao Dou **\n",
        "\n",
        "---\n",
        "\n",
        "## 🚀 Introduction\n",
        "In this assignment, you will implement a bidirectional LSTM-CNN-CRF for sequence labeling, following [this paper by Xuezhe Ma and Ed Hovy](https://www.aclweb.org/anthology/P16-1101.pdf), on the CoNLL named entity recognition dataset.  Before starting the assignment, we recommend reading the Ma and Hovy paper. It is quite helpful to understanding the big picture of what exactly you are going to be implementing.\n",
        "\n",
        "---\n",
        "\n",
        "## 💻 Utilizing GPUs\n",
        "For enhanced training performance, consider utilizing GPUs. To switch your instance to use a GPU, navigate to: Runtime -> Change runtime type -> Hardware accelerator.\n",
        "\n",
        "---\n",
        "\n",
        "## 🕯️ Getting Started with PyTorch\n",
        "This project is centered around PyTorch, a powerful library for building neural networks. If you're new to PyTorch or need a quick refresher, the following resources are highly recommended:\n",
        "- **Introduction to PyTorch**: Explore these informative [slides](https://cocoxu.github.io/CS4650_spring2025/slides/PyTorch_tutorial.pdf) for a comprehensive introduction.\n",
        "- **PyTorch Basics**: Gain hands-on experience with this interactive [notebook](http://bit.ly/pytorchbasics).\n",
        "- **NLP Task Example**: Understand PyTorch's application in NLP through this [text sentiment analysis notebook](http://bit.ly/pytorchexample).\n",
        "\n",
        "---\n",
        "\n",
        "**As a first step, ensure you have a personal copy of this notebook. You can do so by downloading it to your local drive (File -> Download -> Download .ipynb) or by saving a copy to your Google Drive (File -> Save a copy in Drive).**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2U5ZbrBGbFes"
      },
      "source": [
        "## Imports + GPU\n",
        "\n",
        "First, let's import some libraries and make sure the runtime has access to a GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJdlwzlqq0LI",
        "outputId": "4ede36da-c153-4e1f-946f-fbfbac084732"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sat Feb 22 18:14:05 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   49C    P8             12W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "GPU available: True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "print(f'GPU available: {torch.cuda.is_available()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6uw-MGrpEOb"
      },
      "source": [
        "## Download the Data\n",
        "\n",
        "Run the following code to download the English part of the CoNLL 2003 dataset, the evaluation script and pre-filtered GloVe embeddings we are providing for this data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIzJzPU0p7e4",
        "outputId": "cef0af58-f6bd-47b1-8371-dd9ddf3a0f64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-02-22 18:14:06--  https://raw.githubusercontent.com/patverga/torch-ner-nlp-from-scratch/master/data/conll2003/eng.train\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3283420 (3.1M) [text/plain]\n",
            "Saving to: ‘eng.train’\n",
            "\n",
            "eng.train           100%[===================>]   3.13M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-02-22 18:14:06 (51.0 MB/s) - ‘eng.train’ saved [3283420/3283420]\n",
            "\n",
            "--2025-02-22 18:14:06--  https://raw.githubusercontent.com/patverga/torch-ner-nlp-from-scratch/master/data/conll2003/eng.testa\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 827443 (808K) [text/plain]\n",
            "Saving to: ‘eng.testa’\n",
            "\n",
            "eng.testa           100%[===================>] 808.05K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-02-22 18:14:06 (19.9 MB/s) - ‘eng.testa’ saved [827443/827443]\n",
            "\n",
            "--2025-02-22 18:14:06--  https://raw.githubusercontent.com/patverga/torch-ner-nlp-from-scratch/master/data/conll2003/eng.testb\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 748095 (731K) [text/plain]\n",
            "Saving to: ‘eng.testb’\n",
            "\n",
            "eng.testb           100%[===================>] 730.56K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-02-22 18:14:07 (22.2 MB/s) - ‘eng.testb’ saved [748095/748095]\n",
            "\n",
            "--2025-02-22 18:14:07--  https://raw.githubusercontent.com/aritter/twitter_nlp/master/data/annotated/wnut16/conlleval.pl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12754 (12K) [text/plain]\n",
            "Saving to: ‘conlleval.pl’\n",
            "\n",
            "conlleval.pl        100%[===================>]  12.46K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-02-22 18:14:07 (63.3 MB/s) - ‘conlleval.pl’ saved [12754/12754]\n",
            "\n",
            "--2025-02-22 18:14:07--  https://raw.githubusercontent.com/aritter/aritter.github.io/master/files/glove.840B.300d.conll_filtered.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 69798443 (67M) [text/plain]\n",
            "Saving to: ‘glove.840B.300d.conll_filtered.txt’\n",
            "\n",
            "glove.840B.300d.con 100%[===================>]  66.56M  95.9MB/s    in 0.7s    \n",
            "\n",
            "2025-02-22 18:14:08 (95.9 MB/s) - ‘glove.840B.300d.conll_filtered.txt’ saved [69798443/69798443]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#CoNLL 2003 data\n",
        "!wget https://raw.githubusercontent.com/patverga/torch-ner-nlp-from-scratch/master/data/conll2003/eng.train\n",
        "!wget https://raw.githubusercontent.com/patverga/torch-ner-nlp-from-scratch/master/data/conll2003/eng.testa\n",
        "!wget https://raw.githubusercontent.com/patverga/torch-ner-nlp-from-scratch/master/data/conll2003/eng.testb\n",
        "!cat eng.train | awk '{print $1 \"\\t\" $4}' > train\n",
        "!cat eng.testa | awk '{print $1 \"\\t\" $4}' > dev\n",
        "!cat eng.testb | awk '{print $1 \"\\t\" $4}' > test\n",
        "\n",
        "#Evaluation Script\n",
        "!wget https://raw.githubusercontent.com/aritter/twitter_nlp/master/data/annotated/wnut16/conlleval.pl\n",
        "\n",
        "#Pre-filtered GloVe embeddings\n",
        "!wget https://raw.githubusercontent.com/aritter/aritter.github.io/master/files/glove.840B.300d.conll_filtered.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecaJwyFuIRY7",
        "outputId": "181b471c-33fa-4db1-cb6f-4293ad28d2a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv5CEGkGp3w1"
      },
      "source": [
        "## CoNLL Data Format\n",
        "\n",
        "Run the following cell to see a sample of the data in CoNLL format.  As you can see, each line in the file represents a word and its labeled named entity tag in BIO format.  A blank line is used to seperate sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbSvvi3Jtb4g",
        "outputId": "e1e12efa-d1f3-4916-d492-c26499df663e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-DOCSTART-\tO\n",
            "\t\n",
            "EU\tI-ORG\n",
            "rejects\tO\n",
            "German\tI-MISC\n",
            "call\tO\n",
            "to\tO\n",
            "boycott\tO\n",
            "British\tI-MISC\n",
            "lamb\tO\n",
            ".\tO\n",
            "\t\n",
            "Peter\tI-PER\n",
            "Blackburn\tI-PER\n",
            "\t\n",
            "BRUSSELS\tI-LOC\n",
            "1996-08-22\tO\n",
            "\t\n",
            "The\tO\n",
            "European\tI-ORG\n"
          ]
        }
      ],
      "source": [
        "!head -n 20 train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f48PyOPb2Bqo"
      },
      "source": [
        "## Data Preprocessing Diagram\n",
        "\n",
        "Here is a simplified diagram showing what is happening in the data processing sections coming up. Make sure to read through these sections, as you will need to use these functions in your code!\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=13yYcgvEyLC0m3RZegH8V6fErf-8DRtLd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XM9F617sqbvc"
      },
      "source": [
        "## Reading in the Data\n",
        "\n",
        "Below we proivide a bit of code to read in data in the CoNLL format.  This also reads in the filtered GloVe embeddings, to save you some effort - we will discuss this more later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQ4lnZuoqisy",
        "outputId": "98d612e1-1223-4d59-9327-5ab291fc8660"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The second sentence in train set: ['-START-', 'Peter', 'Blackburn', '-END-']\n",
            "The NER label of the sentence:    ['START', 'I-PER', 'I-PER', 'END']\n",
            "The char repersentation of the sentence: [['start', '-', 'S', 'T', 'A', 'R', 'T', '-', 'end'], ['start', 'P', 'e', 't', 'e', 'r', 'end'], ['start', 'B', 'l', 'a', 'c', 'k', 'b', 'u', 'r', 'n', 'end'], ['start', '-', 'E', 'N', 'D', '-', 'end']]\n"
          ]
        }
      ],
      "source": [
        "#Read in the training data\n",
        "def read_conll_format(filename):\n",
        "    (words, tags, currentSent, currentTags) = ([],[],['-START-'],['START'])\n",
        "    for line in open(filename).readlines():\n",
        "        line = line.strip()\n",
        "        #print(line)\n",
        "        if line == \"\":\n",
        "            currentSent.append('-END-')\n",
        "            currentTags.append('END')\n",
        "            words.append(currentSent)\n",
        "            tags.append(currentTags)\n",
        "            (currentSent, currentTags) = (['-START-'], ['START'])\n",
        "        else:\n",
        "            (word, tag) = line.split()\n",
        "            currentSent.append(word)\n",
        "            currentTags.append(tag)\n",
        "    return (words, tags)\n",
        "\n",
        "def sentences2char(sentences):\n",
        "    return [[['start'] + [c for c in w] + ['end'] for w in l] for l in sentences]\n",
        "\n",
        "\n",
        "(sentences_train, tags_train) = read_conll_format(\"train\")\n",
        "(sentences_dev, tags_dev)     = read_conll_format(\"dev\")\n",
        "\n",
        "print(\"The second sentence in train set:\", sentences_train[2])\n",
        "print(\"The NER label of the sentence:   \", tags_train[2])\n",
        "\n",
        "sentencesChar = sentences2char(sentences_train)\n",
        "\n",
        "print(\"The char repersentation of the sentence:\", sentencesChar[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmcwZUkPjNLi",
        "outputId": "a4b2debe-745d-4f47-e741-5583b11e291e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The GloVe word embedding of the word 'the': [0.27204, -0.06203, -0.1884, 0.023225, -0.018158, 0.0067192, -0.13877, 0.17708, 0.17709, 2.5882, -0.35179, -0.17312, 0.43285, -0.10708, 0.15006, -0.19982, -0.19093, 1.1871, -0.16207, -0.23538, 0.003664, -0.19156, -0.085662, 0.039199, -0.066449, -0.04209, -0.19122, 0.011679, -0.37138, 0.21886, 0.0011423, 0.4319, -0.14205, 0.38059, 0.30654, 0.020167, -0.18316, -0.0065186, -0.0080549, -0.12063, 0.027507, 0.29839, -0.22896, -0.22882, 0.14671, -0.076301, -0.1268, -0.0066651, -0.052795, 0.14258, 0.1561, 0.05551, -0.16149, 0.09629, -0.076533, -0.049971, -0.010195, -0.047641, -0.16679, -0.2394, 0.0050141, -0.049175, 0.013338, 0.41923, -0.10104, 0.015111, -0.077706, -0.13471, 0.119, 0.10802, 0.21061, -0.051904, 0.18527, 0.17856, 0.041293, -0.014385, -0.082567, -0.035483, -0.076173, -0.045367, 0.089281, 0.33672, -0.22099, -0.0067275, 0.23983, -0.23147, -0.88592, 0.091297, -0.012123, 0.013233, -0.25799, -0.02972, 0.016754, 0.01369, 0.32377, 0.039546, 0.042114, -0.088243, 0.30318, 0.087747, 0.16346, -0.40485, -0.043845, -0.040697, 0.20936, -0.77795, 0.2997, 0.2334, 0.14891, -0.39037, -0.053086, 0.062922, 0.065663, -0.13906, 0.094193, 0.10344, -0.2797, 0.28905, -0.32161, 0.020687, 0.063254, -0.23257, -0.4352, -0.017049, -0.32744, -0.047064, -0.075149, -0.18788, -0.015017, 0.029342, -0.3527, -0.044278, -0.13507, -0.11644, -0.1043, 0.1392, 0.0039199, 0.37603, 0.067217, -0.37992, -1.1241, -0.057357, -0.16826, 0.03941, 0.2604, -0.023866, 0.17963, 0.13553, 0.2139, 0.052633, -0.25033, -0.11307, 0.22234, 0.066597, -0.11161, 0.062438, -0.27972, 0.19878, -0.36262, -1.0006e-05, -0.17262, 0.29166, -0.15723, 0.054295, 0.06101, -0.39165, 0.2766, 0.057816, 0.39709, 0.025229, 0.24672, -0.08905, 0.15683, -0.2096, -0.22196, 0.052394, -0.01136, 0.050417, -0.14023, -0.042825, -0.031931, -0.21336, -0.20402, -0.23272, 0.07449, 0.088202, -0.11063, -0.33526, -0.014028, -0.29429, -0.086911, -0.1321, -0.43616, 0.20513, 0.0079362, 0.48505, 0.064237, 0.14261, -0.43711, 0.12783, -0.13111, 0.24673, -0.27496, 0.15896, 0.43314, 0.090286, 0.24662, 0.066463, -0.20099, 0.1101, 0.03644, 0.17359, -0.15689, -0.086328, -0.17316, 0.36975, -0.40317, -0.064814, -0.034166, -0.013773, 0.062854, -0.17183, -0.12366, -0.034663, -0.22793, -0.23172, 0.239, 0.27473, 0.15332, 0.10661, -0.060982, -0.024805, -0.13478, 0.17932, -0.37374, -0.02893, -0.11142, -0.08389, -0.055932, 0.068039, -0.10783, 0.1465, 0.094617, -0.084554, 0.067429, -0.3291, 0.034082, -0.16747, -0.25997, -0.22917, 0.020159, -0.02758, 0.16136, -0.18538, 0.037665, 0.57603, 0.20684, 0.27941, 0.16477, -0.018769, 0.12062, 0.069648, 0.059022, -0.23154, 0.24095, -0.3471, 0.04854, -0.056502, 0.41566, -0.43194, 0.4823, -0.051759, -0.27285, -0.25893, 0.16555, -0.1831, -0.06734, 0.42457, 0.010346, 0.14237, 0.25939, 0.17123, -0.13821, -0.066846, 0.015981, -0.30193, 0.043579, -0.043102, 0.35025, -0.19681, -0.4281, 0.16899, 0.22511, -0.28557, -0.1028, -0.018168, 0.11407, 0.13015, -0.18317, 0.1323]\n",
            "dimension of glove embedding: 300\n"
          ]
        }
      ],
      "source": [
        "#Read GloVe embeddings.\n",
        "def read_GloVe(filename):\n",
        "    embeddings = {}\n",
        "    for line in open(filename).readlines():\n",
        "        #print(line)\n",
        "        fields = line.strip().split(\" \")\n",
        "        word = fields[0]\n",
        "        embeddings[word] = [float(x) for x in fields[1:]]\n",
        "    return embeddings\n",
        "\n",
        "GloVe = read_GloVe(\"glove.840B.300d.conll_filtered.txt\")\n",
        "\n",
        "print(\"The GloVe word embedding of the word 'the':\", GloVe[\"the\"])\n",
        "print(\"dimension of glove embedding:\", len(GloVe[\"the\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zeg1gSiqyLD"
      },
      "source": [
        "## Mapping Tokens to Indices\n",
        "\n",
        "As in the last project, we will need to convert words in the dataset to numeric indices, so they can be presented as input to a neural network.  Code to handle this for you with sample usage is provided below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99p1PPX5qlPQ",
        "outputId": "d6393e3d-5f6d-413b-9d7e-14a664c2a7e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vocab size: 29148\n",
            "char vocab size: 88\n",
            "\n",
            "index of word 'the': 10705\n",
            "word of index 253: cantons\n",
            "\n",
            "-START- -DOCSTART- -END-\n",
            "-START- EU rejects German call to boycott British lamb . -END-\n",
            "-START- Peter Blackburn -END-\n",
            "-START- BRUSSELS 1996-08-22 -END-\n",
            "-START- The European Commission said on Thursday it disagreed with German advice to consumers to shun British lamb until scientists determine whether mad cow disease can be transmitted to sheep . -END-\n",
            "-START- Germany 's representative to the European Union 's veterinary committee Werner Zwingmann said on Wednesday consumers should buy sheepmeat from countries other than Britain until the scientific advice was clearer . -END-\n",
            "-START- \" We do n't support any such recommendation because we do n't see any grounds for it , \" the Commission 's chief spokesman Nikolaus van der Pas told a news briefing . -END-\n",
            "-START- He said further scientific study was required and if it was found that action was needed it should be taken by the European Union . -END-\n",
            "-START- He said a proposal last month by EU Farm Commissioner Franz Fischler to ban sheep brains , spleens and spinal cords from the human and animal food chains was a highly specific and precautionary move to protect human health . -END-\n",
            "-START- Fischler proposed EU-wide measures after reports from Britain and France that under laboratory conditions sheep could contract Bovine Spongiform Encephalopathy ( BSE ) -- mad cow disease . -END-\n"
          ]
        }
      ],
      "source": [
        "#Create mappings between tokens and indices.\n",
        "\n",
        "from collections import Counter\n",
        "import random\n",
        "\n",
        "#Will need this later to remove 50% of words that only appear once in the training data from the vocabulary (and don't have GloVe embeddings).\n",
        "wordCounts = Counter([w for l in sentences_train for w in l])\n",
        "charCounts = Counter([c for l in sentences_train for w in l for c in w])\n",
        "singletons = set([w for (w,c) in wordCounts.items() if c == 1 and not w in GloVe.keys()])\n",
        "charSingletons = set([w for (w,c) in charCounts.items() if c == 1])\n",
        "\n",
        "#Build dictionaries to map from words, characters to indices and vice versa.\n",
        "#Save first two words in the vocabulary for padding and \"UNK\" token.\n",
        "word2i = {w:i+2 for i,w in enumerate(set([w for l in sentences_train for w in l] + list(GloVe.keys())))}\n",
        "char2i = {w:i+2 for i,w in enumerate(set([c for l in sentencesChar for w in l for c in w]))}\n",
        "i2word = {i:w for w,i in word2i.items()}\n",
        "i2char = {i:w for w,i in char2i.items()}\n",
        "\n",
        "vocab_size = max(word2i.values()) + 1\n",
        "char_vocab_size = max(char2i.values()) + 1\n",
        "\n",
        "#Tag dictionaries.\n",
        "tag2i = {w:i for i,w in enumerate(set([t for l in tags_train for t in l]))}\n",
        "i2tag = {i:t for t,i in tag2i.items()}\n",
        "\n",
        "#When training, randomly replace singletons with UNK tokens sometimes to simulate situation at test time.\n",
        "def getDictionaryRandomUnk(w, dictionary, train=False):\n",
        "    if train and (w in singletons and random.random() > 0.5):\n",
        "        return 1\n",
        "    else:\n",
        "        return dictionary.get(w, 1)\n",
        "\n",
        "#Map a list of sentences from words to indices.\n",
        "def sentences2indices(words, dictionary, train=False):\n",
        "    #1.0 => UNK\n",
        "    return [[getDictionaryRandomUnk(w,dictionary, train=train) for w in l] for l in words]\n",
        "\n",
        "#Map a list of sentences containing to indices (character indices)\n",
        "def sentences2indicesChar(chars, dictionary):\n",
        "    #1.0 => UNK\n",
        "    return [[[dictionary.get(c,1) for c in w] for w in l] for l in chars]\n",
        "\n",
        "#Indices\n",
        "X       = sentences2indices(sentences_train, word2i, train=True)\n",
        "X_char  = sentences2indicesChar(sentencesChar, char2i)\n",
        "Y       = sentences2indices(tags_train, tag2i)\n",
        "\n",
        "print(\"vocab size:\", vocab_size)\n",
        "print(\"char vocab size:\", char_vocab_size)\n",
        "print()\n",
        "\n",
        "print(\"index of word 'the':\", word2i[\"the\"])\n",
        "print(\"word of index 253:\", i2word[253])\n",
        "print()\n",
        "\n",
        "#Print out some examples of what the dev inputs will look like\n",
        "for i in range(10):\n",
        "    print(\" \".join([i2word.get(w,'UNK') for w in X[i]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHGxN6QCr3GY"
      },
      "source": [
        "## Padding and Batching\n",
        "\n",
        "In this assignment, you should train your models using minibatched SGD.  When presenting multiple sentences to the network at the same time, we will need to pad them to be of the same length. We use [torch.nn.utils.rnn.pad_sequence](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_sequence.html) to do so.\n",
        "\n",
        "\n",
        "Below we provide some code to prepare batches of data to present to the network. We pad the sequence so that all the sequences have the same length.\n",
        "\n",
        "**Side Note:** PyTorch includes utilities in [`torch.utils.data`](https://pytorch.org/docs/stable/data.html) to help with padding, batching, shuffling and some other things, but for this assignment we will do everything from scratch to help you see exactly how this works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHuYuoSYmgiC",
        "outputId": "54ac6fda-a5c3-4252-ca82-1c30728344f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "max slen: 115\n",
            "X_padded: torch.Size([14987, 115])\n",
            "X_mask: torch.Size([14987, 115])\n",
            "X_padded_char: torch.Size([14987, 115, 32])\n",
            "Y_onehot: torch.Size([14987, 115, 10])\n"
          ]
        }
      ],
      "source": [
        "#Pad inputs to max sequence length (for batching)\n",
        "def prepare_input(X_list):\n",
        "    X_padded = torch.nn.utils.rnn.pad_sequence([torch.as_tensor(l) for l in X_list], batch_first=True).type(torch.LongTensor) # padding the sequences with 0\n",
        "    X_mask   = torch.nn.utils.rnn.pad_sequence([torch.as_tensor([1.0] * len(l)) for l in X_list], batch_first=True).type(torch.FloatTensor) # consisting of 0 and 1, 0 for padded positions, 1 for non-padded positions\n",
        "    return (X_padded, X_mask)\n",
        "\n",
        "#Maximum word length (for character representations)\n",
        "MAX_CLEN = 32\n",
        "\n",
        "def prepare_input_char(X_list):\n",
        "    MAX_SLEN = max([len(l) for l in X_list])\n",
        "    X_padded  = [l + [[]]*(MAX_SLEN-len(l))  for l in X_list]\n",
        "    X_padded  = [[w[0:MAX_CLEN] for w in l] for l in X_padded]\n",
        "    X_padded  = [[w + [1]*(MAX_CLEN-len(w)) for w in l] for l in X_padded]\n",
        "    return torch.as_tensor(X_padded).type(torch.LongTensor)\n",
        "\n",
        "#Pad outputs using one-hot encoding\n",
        "def prepare_output_onehot(Y_list, NUM_TAGS=max(tag2i.values())+1):\n",
        "    Y_onehot = [torch.zeros(len(l), NUM_TAGS) for l in Y_list]\n",
        "    for i in range(len(Y_list)):\n",
        "        for j in range(len(Y_list[i])):\n",
        "            # print(type(Y_list[i][j]))\n",
        "            Y_onehot[i][j,Y_list[i][j]] = 1.0\n",
        "    Y_padded = torch.nn.utils.rnn.pad_sequence(Y_onehot, batch_first=True).type(torch.FloatTensor)\n",
        "    return Y_padded\n",
        "\n",
        "print(\"max slen:\", max([len(x) for x in X_char]))\n",
        "\n",
        "\n",
        "(X_padded, X_mask) = prepare_input(X)\n",
        "X_padded_char      = prepare_input_char(X_char)\n",
        "Y_onehot           = prepare_output_onehot(Y)\n",
        "\n",
        "print(\"X_padded:\", X_padded.shape)\n",
        "print(\"X_mask:\", X_mask.shape)\n",
        "print(\"X_padded_char:\", X_padded_char.shape)\n",
        "print(\"Y_onehot:\", Y_onehot.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFHMaMDH78cE"
      },
      "source": [
        "## Model Diagram\n",
        "\n",
        "Below is a simplified diagram of the model you will be implementing. A lot is abstracted away here, but it should help you get the bigger picture of the model you are implementing. Red is part 1, green is part 2, and purple is the extra credit, part 3.\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1GEkDtTxu-x060pltpiHi5LtcaDpLJdw3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSynCsJzu717"
      },
      "source": [
        "## **Your code starts here:** Basic LSTM Tagger (10 points)\n",
        "\n",
        "OK, now you should have everything you need to get started.\n",
        "\n",
        "Recall that your goal is to to implement the BiLSTM-CNN-CRF, as described in [(Ma and Hovy, 2016)](https://www.aclweb.org/anthology/P16-1101.pdf).  This is a relatively complex network with various components.  Below we provide starter code to break down your implementation into increasingly complex versions of the final model, starting with a Basic LSTM tagger.  This way you can be confident that each part is working correctly before incrementally increasing the complexity of your implementation.  This is generally a good approach to take when implementing complex models, since buggy PyTorch code is often partially working, but produces worse results than a correct implementation, so it's hard to know whether added complexities are helping or hurting.  Also, if you aren't able to match published results it's hard to know which component of your model has the problem (or even whether or not it is a problem in the published result!)\n",
        "\n",
        "**Fill in the functions marked as `TODO` in the code block below. Please make your code changes only within the given commented block (#####).** If everything is working correctly, you should be able to achieve an **F1 score of 0.86 on the dev set and 0.82 on the test set (with GloVe embeddings)**. You are required to initialize word embeddings with GloVe later, but you can randomly initialize the word embeddings in the beginning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bl5EsH2WgxLh",
        "outputId": "0ea90b97-aca3-4b89-ec16-f5a2449f6c0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lstm output shape: torch.Size([5, 32, 10])\n",
            "Y onehot shape: torch.Size([5, 32, 10])\n"
          ]
        }
      ],
      "source": [
        "#####################################################################################\n",
        "#TODO: Add imports if needed:\n",
        "\n",
        "#####################################################################################\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "class BasicLSTMtagger(nn.Module):\n",
        "    def __init__(self, DIM_EMB=10, DIM_HID=10):\n",
        "        super(BasicLSTMtagger, self).__init__()\n",
        "        NUM_TAGS = max(tag2i.values())+1\n",
        "\n",
        "\n",
        "        (self.DIM_EMB, self.NUM_TAGS) = (DIM_EMB, NUM_TAGS)\n",
        "        #####################################################################################\n",
        "        #TODO: initialize parameters - embedding layer, nn.LSTM, nn.Linear and nn.LogSoftmax\n",
        "\n",
        "        self.vocab_size = max(word2i.values())+1\n",
        "\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, self.DIM_EMB, padding_idx = 0)\n",
        "        self.init_glove(GloVe)\n",
        "\n",
        "        self.lstm = nn.LSTM(self.DIM_EMB, DIM_HID, bidirectional=True, batch_first=True)\n",
        "\n",
        "        self.linear = nn.Linear(2 * DIM_HID, self.NUM_TAGS)\n",
        "\n",
        "        self.LogSoftmax = nn.LogSoftmax(dim = 2)\n",
        "\n",
        "        #####################################################################################\n",
        "\n",
        "    def forward(self, X, train=False):\n",
        "        #####################################################################################\n",
        "        #TODO: Implement the forward computation.\n",
        "\n",
        "        embed = self.word_embeddings(X)\n",
        "\n",
        "        lstm_out, _ = self.lstm(embed)\n",
        "\n",
        "        linear_out = self.linear(lstm_out)\n",
        "\n",
        "        output = self.LogSoftmax(linear_out)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "        #####################################################################################\n",
        "\n",
        "    def init_glove(self, GloVe):\n",
        "        #####################################################################################\n",
        "        #TODO: initialize word embeddings using GloVe (you can skip this part in your first version, if you want, see instructions below).\n",
        "\n",
        "\n",
        "        embedding_matrix = torch.FloatTensor(self.vocab_size, self.DIM_EMB).uniform_(-0.1, 0.1)\n",
        "\n",
        "        for word, idx in word2i.items():\n",
        "          if word in GloVe:\n",
        "              # print(type(torch.tensor(GloVe[word], dtype=torch.float32)))\n",
        "\n",
        "              embedding_matrix[idx] = torch.tensor(GloVe[word], dtype=torch.float32)\n",
        "\n",
        "        self.word_embeddings.weight.data.copy_(embedding_matrix)\n",
        "        self.word_embeddings.weight.requires_grad = False\n",
        "\n",
        "        #####################################################################################\n",
        "\n",
        "    def inference(self, sentences):\n",
        "        X, X_mask       = prepare_input(sentences2indices(sentences, word2i))\n",
        "        pred = self.forward(X.cuda()).argmax(dim=2)\n",
        "        return [[i2tag[pred[i,j].item()] for j in range(len(sentences[i]))] for i in range(len(sentences))]\n",
        "\n",
        "    def print_predictions(self, words, tags):\n",
        "        Y_pred = self.inference(words)\n",
        "        for i in range(len(words)):\n",
        "            print(\"----------------------------\")\n",
        "            print(\" \".join([f\"{words[i][j]}/{Y_pred[i][j]}/{tags[i][j]}\" for j in range(len(words[i]))]))\n",
        "            print(\"Predicted:\\t\", Y_pred[i])\n",
        "            print(\"Gold:\\t\\t\", tags[i])\n",
        "\n",
        "    def write_predictions(self, sentences, outFile):\n",
        "        fOut = open(outFile, 'w')\n",
        "        for s in sentences:\n",
        "            y = self.inference([s])[0]\n",
        "            #print(\"\\n\".join(y[1:len(y)-1]))\n",
        "            fOut.write(\"\\n\".join(y[1:len(y)-1]))  #Skip start and end tokens\n",
        "            fOut.write(\"\\n\\n\")\n",
        "\n",
        "#The following code will initialize a model and test that your forward computation runs without errors.\n",
        "lstm_test   = BasicLSTMtagger(DIM_HID=7, DIM_EMB=300)\n",
        "lstm_output = lstm_test.forward(prepare_input(X[0:5])[0])\n",
        "Y_onehot    = prepare_output_onehot(Y[0:5])\n",
        "\n",
        "#Check the shape of the lstm_output and one-hot label tensors.\n",
        "print(\"lstm output shape:\", lstm_output.shape)\n",
        "print(\"Y onehot shape:\", Y_onehot.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99ZVaS50gyGY"
      },
      "outputs": [],
      "source": [
        "#Read in the data\n",
        "\n",
        "(sentences_dev, tags_dev)     = read_conll_format('dev')\n",
        "(sentences_train, tags_train) = read_conll_format('train')\n",
        "(sentences_test, tags_test)   = read_conll_format('test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7Xbox3alX0b"
      },
      "source": [
        "## Train your Model (10 points)\n",
        "\n",
        "Next, implement the function below to train your basic BiLSTM tagger.  See [torch.nn.lstm](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html).  Make sure to save your predictions on the test set (`test_pred_lstm.txt`) for submission to GradeScope. Feel free to change number of epochs, optimizer, learning rate and batch size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvJyLIjR6HjT",
        "outputId": "bca9b144-c8ad-422a-c98c-d683535c004f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss on epoch 0 = 17.932065963745117\n",
            "conlleval:\n",
            "processed 51578 tokens with 5942 phrases; found: 5783 phrases; correct: 5055.\n",
            "accuracy:  97.63%; precision:  87.41%; recall:  85.07%; FB1:  86.23\n",
            "              LOC: precision:  93.77%; recall:  89.33%; FB1:  91.50  1750\n",
            "             MISC: precision:  84.33%; recall:  77.66%; FB1:  80.86  849\n",
            "              ORG: precision:  79.41%; recall:  71.89%; FB1:  75.46  1214\n",
            "              PER: precision:  88.02%; recall:  94.14%; FB1:  90.98  1970\n",
            "\n",
            "----------------------------\n",
            "-START-/START/START -DOCSTART-/O/O -END-/END/END\n",
            "Predicted:\t ['START', 'O', 'END']\n",
            "Gold:\t\t ['START', 'O', 'END']\n",
            "----------------------------\n",
            "-START-/START/START BASEBALL/O/O -/O/O MAJOR/I-MISC/I-MISC LEAGUE/I-MISC/I-MISC RESULTS/O/O THURSDAY/O/O ./O/O -END-/END/END\n",
            "Predicted:\t ['START', 'O', 'O', 'I-MISC', 'I-MISC', 'O', 'O', 'O', 'END']\n",
            "Gold:\t\t ['START', 'O', 'O', 'I-MISC', 'I-MISC', 'O', 'O', 'O', 'END']\n",
            "----------------------------\n",
            "-START-/START/START Russian/I-MISC/I-MISC judge/O/O stabbed/O/O to/O/O death/O/O over/O/O $/O/O 7/O/O fine/O/O ./O/O -END-/END/END\n",
            "Predicted:\t ['START', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'END']\n",
            "Gold:\t\t ['START', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'END']\n",
            "----------------------------\n",
            "-START-/START/START NORTHAMPTON/I-LOC/I-LOC ,/O/O England/I-LOC/I-LOC 1996-08-31/O/O -END-/END/END\n",
            "Predicted:\t ['START', 'I-LOC', 'O', 'I-LOC', 'O', 'END']\n",
            "Gold:\t\t ['START', 'I-LOC', 'O', 'I-LOC', 'O', 'END']\n",
            "----------------------------\n",
            "-START-/START/START \"/O/O I/O/O told/O/O Monica/I-PER/I-PER we/O/O need/O/O her/O/O if/O/O we/O/O want/O/O to/O/O win/O/O ,/O/O \"/O/O King/I-PER/I-PER said/O/O ./O/O -END-/END/END\n",
            "Predicted:\t ['START', 'O', 'O', 'O', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-PER', 'O', 'O', 'END']\n",
            "Gold:\t\t ['START', 'O', 'O', 'O', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-PER', 'O', 'O', 'END']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss on epoch 1 = 3.171403408050537\n",
            "conlleval:\n",
            "processed 51578 tokens with 5942 phrases; found: 6068 phrases; correct: 5357.\n",
            "accuracy:  98.33%; precision:  88.28%; recall:  90.15%; FB1:  89.21\n",
            "                 : precision:   0.00%; recall:   0.00%; FB1:   0.00  1\n",
            "              LOC: precision:  90.03%; recall:  96.30%; FB1:  93.06  1965\n",
            "             MISC: precision:  84.50%; recall:  82.75%; FB1:  83.62  903\n",
            "              ORG: precision:  83.07%; recall:  80.84%; FB1:  81.93  1305\n",
            "              PER: precision:  91.92%; recall:  94.52%; FB1:  93.20  1894\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss on epoch 2 = 1.727961540222168\n",
            "conlleval:\n",
            "processed 51578 tokens with 5942 phrases; found: 6130 phrases; correct: 5430.\n",
            "accuracy:  98.38%; precision:  88.58%; recall:  91.38%; FB1:  89.96\n",
            "                 : precision:   0.00%; recall:   0.00%; FB1:   0.00  1\n",
            "              LOC: precision:  92.30%; recall:  95.32%; FB1:  93.79  1897\n",
            "             MISC: precision:  79.29%; recall:  85.14%; FB1:  82.11  990\n",
            "              ORG: precision:  83.86%; recall:  85.61%; FB1:  84.72  1369\n",
            "              PER: precision:  93.22%; recall:  94.79%; FB1:  94.00  1873\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss on epoch 3 = 1.0144685506820679\n",
            "conlleval:\n",
            "processed 51578 tokens with 5942 phrases; found: 6049 phrases; correct: 5417.\n",
            "accuracy:  98.49%; precision:  89.55%; recall:  91.16%; FB1:  90.35\n",
            "              LOC: precision:  94.27%; recall:  94.88%; FB1:  94.57  1849\n",
            "             MISC: precision:  87.37%; recall:  83.30%; FB1:  85.29  879\n",
            "              ORG: precision:  81.46%; recall:  87.17%; FB1:  84.22  1435\n",
            "              PER: precision:  92.10%; recall:  94.30%; FB1:  93.19  1886\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss on epoch 4 = 0.5038003325462341\n",
            "conlleval:\n",
            "processed 51578 tokens with 5942 phrases; found: 6055 phrases; correct: 5435.\n",
            "accuracy:  98.52%; precision:  89.76%; recall:  91.47%; FB1:  90.61\n",
            "              LOC: precision:  94.02%; recall:  94.99%; FB1:  94.50  1856\n",
            "             MISC: precision:  85.67%; recall:  84.27%; FB1:  84.96  907\n",
            "              ORG: precision:  83.58%; recall:  85.76%; FB1:  84.65  1376\n",
            "              PER: precision:  92.01%; recall:  95.71%; FB1:  93.83  1916\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss on epoch 5 = 0.3063792288303375\n",
            "conlleval:\n",
            "processed 51578 tokens with 5942 phrases; found: 6043 phrases; correct: 5437.\n",
            "accuracy:  98.52%; precision:  89.97%; recall:  91.50%; FB1:  90.73\n",
            "              LOC: precision:  94.44%; recall:  93.47%; FB1:  93.95  1818\n",
            "             MISC: precision:  84.59%; recall:  85.14%; FB1:  84.86  928\n",
            "              ORG: precision:  82.68%; recall:  88.67%; FB1:  85.57  1438\n",
            "              PER: precision:  93.92%; recall:  94.79%; FB1:  94.35  1859\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss on epoch 6 = 0.19363655149936676\n",
            "conlleval:\n",
            "processed 51578 tokens with 5942 phrases; found: 6019 phrases; correct: 5448.\n",
            "accuracy:  98.55%; precision:  90.51%; recall:  91.69%; FB1:  91.10\n",
            "              LOC: precision:  95.37%; recall:  95.26%; FB1:  95.32  1835\n",
            "             MISC: precision:  84.81%; recall:  85.36%; FB1:  85.08  928\n",
            "              ORG: precision:  85.30%; recall:  86.13%; FB1:  85.71  1354\n",
            "              PER: precision:  92.32%; recall:  95.33%; FB1:  93.80  1902\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss on epoch 7 = 0.27926748991012573\n",
            "conlleval:\n",
            "processed 51578 tokens with 5942 phrases; found: 6032 phrases; correct: 5459.\n",
            "accuracy:  98.56%; precision:  90.50%; recall:  91.87%; FB1:  91.18\n",
            "              LOC: precision:  93.74%; recall:  95.43%; FB1:  94.58  1870\n",
            "             MISC: precision:  88.24%; recall:  84.60%; FB1:  86.38  884\n",
            "              ORG: precision:  84.54%; recall:  86.88%; FB1:  85.69  1378\n",
            "              PER: precision:  92.68%; recall:  95.60%; FB1:  94.12  1900\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss on epoch 8 = 0.9046532511711121\n",
            "conlleval:\n",
            "processed 51578 tokens with 5942 phrases; found: 6103 phrases; correct: 5341.\n",
            "accuracy:  98.26%; precision:  87.51%; recall:  89.89%; FB1:  88.68\n",
            "              LOC: precision:  92.43%; recall:  93.74%; FB1:  93.08  1863\n",
            "             MISC: precision:  82.50%; recall:  80.80%; FB1:  81.64  903\n",
            "              ORG: precision:  80.78%; recall:  83.07%; FB1:  81.91  1379\n",
            "              PER: precision:  89.89%; recall:  95.55%; FB1:  92.63  1958\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss on epoch 9 = 1.12472403049469\n",
            "conlleval:\n",
            "processed 51578 tokens with 5942 phrases; found: 6108 phrases; correct: 5417.\n",
            "accuracy:  98.40%; precision:  88.69%; recall:  91.16%; FB1:  89.91\n",
            "              LOC: precision:  92.30%; recall:  95.86%; FB1:  94.05  1908\n",
            "             MISC: precision:  83.39%; recall:  83.84%; FB1:  83.61  927\n",
            "              ORG: precision:  82.83%; recall:  86.73%; FB1:  84.74  1404\n",
            "              PER: precision:  92.03%; recall:  93.38%; FB1:  92.70  1869\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss on epoch 10 = 0.6710819005966187\n",
            "conlleval:\n",
            "processed 51578 tokens with 5942 phrases; found: 6064 phrases; correct: 5442.\n",
            "accuracy:  98.51%; precision:  89.74%; recall:  91.59%; FB1:  90.65\n",
            "              LOC: precision:  92.22%; recall:  96.79%; FB1:  94.45  1928\n",
            "             MISC: precision:  85.64%; recall:  85.36%; FB1:  85.50  919\n",
            "              ORG: precision:  85.09%; recall:  83.82%; FB1:  84.45  1321\n",
            "              PER: precision:  92.46%; recall:  95.17%; FB1:  93.79  1896\n",
            "\n",
            "----------------------------\n",
            "-START-/START/START Everything/O/O else/O/O is/O/O muddy/O/O ,/O/O the/O/O waters/O/O of/O/O the/O/O fjord/O/O leaden/O/O ./O/O -END-/END/END\n",
            "Predicted:\t ['START', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'END']\n",
            "Gold:\t\t ['START', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'END']\n",
            "----------------------------\n",
            "-START-/START/START Most/O/O of/O/O the/O/O Marines/I-ORG/I-MISC are/O/O on/O/O three/O/O ships/O/O in/O/O the/O/O Tarawa/I-ORG/I-ORG Amphibious/I-ORG/I-ORG Readiness/I-ORG/I-ORG Group/I-ORG/I-ORG ./O/O -END-/END/END\n",
            "Predicted:\t ['START', 'O', 'O', 'O', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'END']\n",
            "Gold:\t\t ['START', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'END']\n",
            "----------------------------\n",
            "-START-/START/START FRANCE/I-LOC/I-LOC -/O/O Bond/O/O and/O/O PIBOR/I-MISC/O futures/O/O ended/O/O the/O/O day/O/O higher/O/O despite/O/O much/O/O stronger/O/O than/O/O expected/O/O U.S./I-LOC/I-LOC data/O/O ./O/O -END-/END/END\n",
            "Predicted:\t ['START', 'I-LOC', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-LOC', 'O', 'O', 'END']\n",
            "Gold:\t\t ['START', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-LOC', 'O', 'O', 'END']\n",
            "----------------------------\n",
            "-START-/START/START Morris/I-PER/I-PER '/O/O departure/O/O raised/O/O fears/O/O that/O/O Clinton/I-PER/I-PER would/O/O veer/O/O more/O/O to/O/O the/O/O left/O/O in/O/O a/O/O second/O/O term/O/O ./O/O -END-/END/END\n",
            "Predicted:\t ['START', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'END']\n",
            "Gold:\t\t ['START', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'END']\n",
            "----------------------------\n",
            "-START-/START/START Palestinian/I-MISC/I-MISC President/O/O Arafat/I-PER/I-PER ,/O/O attacking/O/O Israel/I-LOC/I-LOC 's/O/O decision/O/O to/O/O expand/O/O Jewish/I-MISC/I-MISC settlements/O/O and/O/O its/O/O policy/O/O on/O/O Jerusalem/I-LOC/I-LOC ,/O/O went/O/O before/O/O the/O/O Palestinian/I-MISC/I-MISC legislature/O/O on/O/O Wednesday/O/O to/O/O urge/O/O the/O/O two/O/O million/O/O Arabs/I-MISC/I-MISC in/O/O the/O/O West/I-LOC/I-LOC Bank/I-LOC/I-LOC and/O/O Gaza/I-LOC/I-LOC to/O/O go/O/O to/O/O the/O/O holy/O/O city/O/O ./O/O -END-/END/END\n",
            "Predicted:\t ['START', 'I-MISC', 'O', 'I-PER', 'O', 'O', 'I-LOC', 'O', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'I-LOC', 'O', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'I-LOC', 'I-LOC', 'O', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'END']\n",
            "Gold:\t\t ['START', 'I-MISC', 'O', 'I-PER', 'O', 'O', 'I-LOC', 'O', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'I-LOC', 'O', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'I-LOC', 'I-LOC', 'O', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'END']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss on epoch 11 = 0.3149062991142273\n",
            "conlleval:\n",
            "processed 51578 tokens with 5942 phrases; found: 6045 phrases; correct: 5466.\n",
            "accuracy:  98.62%; precision:  90.42%; recall:  91.99%; FB1:  91.20\n",
            "              LOC: precision:  94.29%; recall:  96.24%; FB1:  95.26  1875\n",
            "             MISC: precision:  84.67%; recall:  84.49%; FB1:  84.58  920\n",
            "              ORG: precision:  84.57%; recall:  87.47%; FB1:  86.00  1387\n",
            "              PER: precision:  93.72%; recall:  94.79%; FB1:  94.25  1863\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss on epoch 12 = 0.16931620240211487\n",
            "conlleval:\n",
            "processed 51578 tokens with 5942 phrases; found: 6049 phrases; correct: 5452.\n",
            "accuracy:  98.60%; precision:  90.13%; recall:  91.75%; FB1:  90.93\n",
            "              LOC: precision:  94.63%; recall:  94.94%; FB1:  94.78  1843\n",
            "             MISC: precision:  83.80%; recall:  85.25%; FB1:  84.52  938\n",
            "              ORG: precision:  84.35%; recall:  87.25%; FB1:  85.78  1387\n",
            "              PER: precision:  93.14%; recall:  95.11%; FB1:  94.12  1881\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss on epoch 13 = 0.09059637784957886\n",
            "conlleval:\n",
            "processed 51578 tokens with 5942 phrases; found: 6019 phrases; correct: 5463.\n",
            "accuracy:  98.65%; precision:  90.76%; recall:  91.94%; FB1:  91.35\n",
            "              LOC: precision:  94.39%; recall:  96.14%; FB1:  95.25  1871\n",
            "             MISC: precision:  86.86%; recall:  84.60%; FB1:  85.71  898\n",
            "              ORG: precision:  85.65%; recall:  86.80%; FB1:  86.22  1359\n",
            "              PER: precision:  92.70%; recall:  95.17%; FB1:  93.92  1891\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss on epoch 14 = 0.05419852212071419\n",
            "conlleval:\n",
            "processed 51578 tokens with 5942 phrases; found: 6042 phrases; correct: 5481.\n",
            "accuracy:  98.66%; precision:  90.71%; recall:  92.24%; FB1:  91.47\n",
            "              LOC: precision:  94.83%; recall:  95.92%; FB1:  95.37  1858\n",
            "             MISC: precision:  87.49%; recall:  84.92%; FB1:  86.19  895\n",
            "              ORG: precision:  85.47%; recall:  87.32%; FB1:  86.39  1370\n",
            "              PER: precision:  91.97%; recall:  95.82%; FB1:  93.86  1919\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Training\n",
        "\n",
        "#####################################################################################\n",
        "#TODO: Add imports if needed:\n",
        "\n",
        "from torch.optim import Adam, AdamW, SGD\n",
        "\n",
        "\n",
        "#####################################################################################\n",
        "\n",
        "from random import sample\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import subprocess\n",
        "import random\n",
        "\n",
        "def shuffle_sentences(sentences, tags):\n",
        "    shuffled_sentences = []\n",
        "    shuffled_tags      = []\n",
        "    indices = list(range(len(sentences)))\n",
        "    random.shuffle(indices)\n",
        "    for i in indices:\n",
        "        shuffled_sentences.append(sentences[i])\n",
        "        shuffled_tags.append(tags[i])\n",
        "    return (shuffled_sentences, shuffled_tags)\n",
        "\n",
        "\n",
        "def train_basic_lstm(sentences, tags, lstm):\n",
        "    #####################################################################################\n",
        "    #TODO: initialize optimizer and other parameters\n",
        "\n",
        "    optim = AdamW(lstm.parameters(), lr=0.01)\n",
        "    nEpochs = 15\n",
        "    batchSize = 64\n",
        "    loss_function = nn.NLLLoss()\n",
        "\n",
        "    lstm.train()\n",
        "    lstm.cuda()\n",
        "\n",
        "\n",
        "    #####################################################################################\n",
        "\n",
        "    for epoch in range(nEpochs):\n",
        "        totalLoss = 0.0\n",
        "\n",
        "        (sentences_shuffled, tags_shuffled) = shuffle_sentences(sentences, tags)\n",
        "        for batch in tqdm(range(0, len(sentences), batchSize), leave=False):\n",
        "            #####################################################################################\n",
        "            #TODO: Implement gradient update on a batch of data.\n",
        "\n",
        "            optim.zero_grad()\n",
        "\n",
        "            input_data = prepare_input(sentences2indices(sentences_shuffled[batch:batch+batchSize], word2i, train=True))[0].cuda()\n",
        "\n",
        "            lstm_output = lstm.forward(input_data)\n",
        "\n",
        "            Y_onehot = prepare_output_onehot(sentences2indices(tags_shuffled[batch:batch+batchSize], tag2i)).cuda()\n",
        "\n",
        "            loss = loss_function(lstm_output.view(-1, lstm.NUM_TAGS), Y_onehot.view(-1, lstm.NUM_TAGS).argmax(dim=1))\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            optim.step()\n",
        "\n",
        "            totalLoss += loss\n",
        "\n",
        "            #####################################################################################\n",
        "\n",
        "        print(f\"loss on epoch {epoch} = {totalLoss}\")\n",
        "        lstm.write_predictions(sentences_dev, 'dev_pred')   #Performance on dev set\n",
        "        print('conlleval:')\n",
        "        print(subprocess.Popen('paste dev dev_pred | perl conlleval.pl -d \"\\t\"', shell=True, stdout=subprocess.PIPE,stderr=subprocess.STDOUT).communicate()[0].decode('UTF-8'))\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            s = sample(range(len(sentences_dev)), 5)\n",
        "            lstm.print_predictions([sentences_dev[i] for i in s], [tags_dev[i] for i in s])\n",
        "\n",
        "\n",
        "lstm = BasicLSTMtagger(DIM_HID=500, DIM_EMB=300).cuda()\n",
        "train_basic_lstm(sentences_train, tags_train, lstm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Om69KEe_KKxs",
        "outputId": "5d6a2eae-8d1b-46c0-8441-0c7883195e06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-02-22 18:17:51--  https://raw.githubusercontent.com/aritter/twitter_nlp/master/data/annotated/wnut16/conlleval.pl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12754 (12K) [text/plain]\n",
            "Saving to: ‘conlleval.pl.1’\n",
            "\n",
            "\rconlleval.pl.1        0%[                    ]       0  --.-KB/s               \rconlleval.pl.1      100%[===================>]  12.46K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2025-02-22 18:17:51 (16.8 MB/s) - ‘conlleval.pl.1’ saved [12754/12754]\n",
            "\n",
            "processed 46666 tokens with 5648 phrases; found: 5769 phrases; correct: 5022.\n",
            "accuracy:  97.79%; precision:  87.05%; recall:  88.92%; FB1:  87.97\n",
            "                 : precision:   0.00%; recall:   0.00%; FB1:   0.00  2\n",
            "              LOC: precision:  90.56%; recall:  92.03%; FB1:  91.29  1695\n",
            "             MISC: precision:  75.42%; recall:  76.50%; FB1:  75.95  712\n",
            "              ORG: precision:  83.81%; recall:  86.94%; FB1:  85.34  1723\n",
            "              PER: precision:  92.00%; recall:  93.14%; FB1:  92.56  1637\n"
          ]
        }
      ],
      "source": [
        "#Evaluation on test data\n",
        "lstm.write_predictions(sentences_test, 'test_pred_lstm.txt')\n",
        "!wget https://raw.githubusercontent.com/aritter/twitter_nlp/master/data/annotated/wnut16/conlleval.pl\n",
        "!paste test test_pred_lstm.txt | perl conlleval.pl -d \"\\t\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvyY4qzvuBzK"
      },
      "source": [
        "## Initialization with GloVe Embeddings (5 points)\n",
        "\n",
        "If you haven't already, implement the `init_glove()` method in `BasicLSTMtagger` above.\n",
        "\n",
        "Rather than initializing word embeddings randomly, it is common to use learned word embeddings (GloVe or Word2Vec), as discussed in lecture.  To make this simpler, we have already pre-filtered [GloVe](https://nlp.stanford.edu/projects/glove/) embeddings to only contain words in the vocabulary of the CoNLL NER dataset, and loaded them into a dictionary (`GloVe`) at the beginning of this notebook.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdJSNuMKuN8J"
      },
      "source": [
        "## Character Embeddings (10 points)\n",
        "\n",
        "Now that you have your basic LSTM tagger working, the next step is to add a convolutional network that computes word embeddings from character representations of words.  See Figure 2 and Figure 3 in the [Ma and Hovy](https://www.aclweb.org/anthology/P16-1101.pdf) paper.  We have provided code in `sentences2input_tensors` to convert sentences into lists of word and character indices.  See also [nn.Conv1d](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html) and [MaxPool1d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool1d.html).\n",
        "\n",
        "Hint: The nn.Conv1d accepts input size $(N, C_{in}, L_{in})$, but we have input size $(N, \\text{SLEN}, \\text{CLEN}, \\text{EMB_DIM})$. We can reshape and [permute](https://pytorch.org/docs/stable/generated/torch.permute.html) our input to satisfy the nn.Conv1d, and recover the dimensions later.\n",
        "\n",
        "Make sure to save your predictions on the test set, for submission to GradeScope. You should be able to achieve **90 F1 / 85 F1 on the dev/test sets**.\n",
        "\n",
        "**Fill in the functions marked as `TODO` in the code block below. Please make your code changes only within the given commented block (#####).**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwiKOvSs7Xgx",
        "outputId": "bea560be-cee5-4fee-e97c-403eeb1169d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lstm output shape: torch.Size([5, 32, 10])\n",
            "Y onehot shape: torch.Size([5, 32, 10])\n"
          ]
        }
      ],
      "source": [
        "#####################################################################################\n",
        "#TODO: Add imports if needed:\n",
        "import torch.nn.functional as F\n",
        "#####################################################################################\n",
        "\n",
        "\n",
        "class CharLSTMtagger(BasicLSTMtagger):\n",
        "    def __init__(self, DIM_EMB=10, DIM_CHAR_EMB=30, DIM_HID=10):\n",
        "        super(CharLSTMtagger, self).__init__(DIM_EMB=DIM_EMB, DIM_HID=DIM_HID)\n",
        "        NUM_TAGS = max(tag2i.values())+1\n",
        "\n",
        "        (self.DIM_EMB, self.NUM_TAGS) = (DIM_EMB, NUM_TAGS)\n",
        "        #####################################################################################\n",
        "        #TODO: Initialize parameters.\n",
        "\n",
        "\n",
        "        self.char_embeddings = nn.Embedding(char_vocab_size, DIM_CHAR_EMB)\n",
        "        self.char_cnn = nn.Conv1d(in_channels = DIM_CHAR_EMB, out_channels = 30, kernel_size = 3, padding = 1)\n",
        "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=1, padding=1)\n",
        "        self.char_dropout = nn.Dropout(p=0.5)\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, DIM_EMB)\n",
        "        self.dropout_in = nn.Dropout(p=0.5)\n",
        "        self.dropout_out = nn.Dropout(p=0.5)\n",
        "        self.log_softmax = nn.LogSoftmax(dim = 2)\n",
        "        self.linear = nn.Linear(2 * DIM_HID, NUM_TAGS)\n",
        "        self.lstm = nn.LSTM(DIM_EMB + 30, DIM_HID, batch_first = True, bidirectional = True)\n",
        "        self.init_glove(GloVe)\n",
        "\n",
        "        #####################################################################################\n",
        "\n",
        "    def forward(self, X, X_char, train=False):\n",
        "        #####################################################################################\n",
        "        #TODO: Implement the forward computation.\n",
        "\n",
        "        word_embeddings = self.word_embeddings(X)\n",
        "\n",
        "\n",
        "        char_embeddings = self.char_embeddings(X_char)\n",
        "        if train:\n",
        "            char_embeddings = self.char_dropout(char_embeddings)\n",
        "\n",
        "\n",
        "        batch_size, seq_len, word_len, char_dim = char_embeddings.shape\n",
        "        char_embeddings = char_embeddings.view(-1, char_dim, word_len)\n",
        "\n",
        "\n",
        "        char_cnn_out = F.relu(self.char_cnn(char_embeddings))\n",
        "        char_pooled = self.maxpool(char_cnn_out)\n",
        "\n",
        "\n",
        "        char_rep = torch.max(char_pooled, dim=2)[0]\n",
        "        char_rep = char_rep.view(batch_size, seq_len, -1)\n",
        "\n",
        "\n",
        "        combined_embeddings = torch.cat((word_embeddings, char_rep), dim=2)\n",
        "\n",
        "        if train:\n",
        "            combined_embeddings = self.dropout_in(combined_embeddings)\n",
        "\n",
        "\n",
        "        lstm_out, _ = self.lstm(combined_embeddings)\n",
        "\n",
        "        if train:\n",
        "            lstm_out = self.dropout_out(lstm_out)\n",
        "\n",
        "\n",
        "        linear_out = self.linear(lstm_out)\n",
        "        output = self.log_softmax(linear_out)\n",
        "\n",
        "        return output\n",
        "\n",
        "        #####################################################################################\n",
        "\n",
        "    def sentences2input_tensors(self, sentences):\n",
        "        (X, X_mask)   = prepare_input(sentences2indices(sentences, word2i))\n",
        "        X_char        = prepare_input_char(sentences2indicesChar(sentences, char2i))\n",
        "        return (X, X_mask, X_char)\n",
        "\n",
        "    def inference(self, sentences):\n",
        "        (X, X_mask, X_char) = self.sentences2input_tensors(sentences)\n",
        "        pred = self.forward(X.cuda(), X_char.cuda()).argmax(dim=2)\n",
        "        return [[i2tag[pred[i,j].item()] for j in range(len(sentences[i]))] for i in range(len(sentences))]\n",
        "\n",
        "    def print_predictions(self, words, tags):\n",
        "        Y_pred = self.inference(words)\n",
        "        for i in range(len(words)):\n",
        "            print(\"----------------------------\")\n",
        "            print(\" \".join([f\"{words[i][j]}/{Y_pred[i][j]}/{tags[i][j]}\" for j in range(len(words[i]))]))\n",
        "            print(\"Predicted:\\t\", Y_pred[i])\n",
        "            print(\"Gold:\\t\\t\", tags[i])\n",
        "\n",
        "char_lstm_test = CharLSTMtagger(DIM_HID=7, DIM_EMB=300)\n",
        "lstm_output    = char_lstm_test.forward(prepare_input(X[0:5])[0], prepare_input_char(X_char[0:5]))\n",
        "Y_onehot       = prepare_output_onehot(Y[0:5])\n",
        "\n",
        "print(\"lstm output shape:\", lstm_output.shape)\n",
        "print(\"Y onehot shape:\", Y_onehot.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FmQqNC_CB7Z",
        "outputId": "75450505-4239-4c60-85b1-ce59cb988985"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss on epoch 0 = 35.71760559082031\n",
            "conlleval:\n",
            "processed 51578 tokens with 5942 phrases; found: 6011 phrases; correct: 4542.\n",
            "accuracy:  95.90%; precision:  75.56%; recall:  76.44%; FB1:  76.00\n",
            "              LOC: precision:  80.18%; recall:  84.32%; FB1:  82.20  1932\n",
            "             MISC: precision:  74.54%; recall:  65.40%; FB1:  69.67  809\n",
            "              ORG: precision:  60.05%; recall:  59.51%; FB1:  59.78  1329\n",
            "              PER: precision:  82.02%; recall:  86.43%; FB1:  84.17  1941\n",
            "\n",
            "----------------------------\n",
            "-START-/START/START \"/O/O KDP/O/I-ORG (/O/O Kurdistan/I-ORG/I-ORG Democratic/I-ORG/I-ORG Party/I-ORG/I-ORG )/O/O is/O/O trying/O/O to/O/O overtake/O/O the/O/O city/O/O ./O/O -END-/END/END\n",
            "Predicted:\t ['START', 'O', 'O', 'O', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'END']\n",
            "Gold:\t\t ['START', 'O', 'I-ORG', 'O', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'END']\n",
            "----------------------------\n",
            "-START-/START/START Sanders/I-PER/I-PER (/O/O 8-4/O/O )/O/O struck/O/O out/O/O 10/O/O and/O/O walked/O/O three/O/O to/O/O win/O/O his/O/O fourth/O/O straight/O/O ./O/O -END-/END/END\n",
            "Predicted:\t ['START', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'END']\n",
            "Gold:\t\t ['START', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'END']\n",
            "----------------------------\n",
            "-START-/START/START OTTAWA/I-LOC/I-LOC 1996-08-30/O/O -END-/END/END\n",
            "Predicted:\t ['START', 'I-LOC', 'O', 'END']\n",
            "Gold:\t\t ['START', 'I-LOC', 'O', 'END']\n",
            "----------------------------\n",
            "-START-/START/START Tarango/I-PER/I-PER ,/O/O whose/O/O Wimbledon/O/I-MISC tantrum/O/O two/O/O years/O/O ago/O/O brought/O/O him/O/O a/O/O $/O/O 28,000/O/O fine/O/O and/O/O suspension/O/O from/O/O this/O/O year/O/O 's/O/O tournament/O/O at/O/O the/O/O All-England/I-MISC/I-ORG Club/I-ORG/I-ORG ,/O/O argued/O/O calls/O/O and/O/O taunted/O/O fans/O/O in/O/O his/O/O lively/O/O two/O/O hour/O/O ,/O/O 24/O/O minute/O/O tango/O/O with/O/O Rios/I-PER/I-PER on/O/O the/O/O grandstand/O/O ./O/O -END-/END/END\n",
            "Predicted:\t ['START', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-PER', 'O', 'O', 'O', 'O', 'END']\n",
            "Gold:\t\t ['START', 'I-PER', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-PER', 'O', 'O', 'O', 'O', 'END']\n",
            "----------------------------\n",
            "-START-/START/START More/O/O than/O/O 960/O/O people/O/O have/O/O been/O/O killed/O/O in/O/O the/O/O armed/O/O struggle/O/O the/O/O Gama'a/I-ORG/I-MISC launched/O/O in/O/O 1992/O/O to/O/O topple/O/O President/O/O Hosni/I-PER/I-PER Mubarak/I-PER/I-PER 's/O/O government/O/O and/O/O establish/O/O a/O/O purist/I-MISC/O Islamic/I-MISC/I-MISC state/O/O in/O/O its/O/O place/O/O -END-/END/END\n",
            "Predicted:\t ['START', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'END']\n",
            "Gold:\t\t ['START', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'O', 'END']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss on epoch 1 = 11.586722373962402\n",
            "conlleval:\n",
            "processed 51578 tokens with 5942 phrases; found: 5744 phrases; correct: 4804.\n",
            "accuracy:  96.97%; precision:  83.64%; recall:  80.85%; FB1:  82.22\n",
            "              LOC: precision:  90.18%; recall:  86.99%; FB1:  88.56  1772\n",
            "             MISC: precision:  72.57%; recall:  74.62%; FB1:  73.58  948\n",
            "              ORG: precision:  75.18%; recall:  63.68%; FB1:  68.95  1136\n",
            "              PER: precision:  88.14%; recall:  90.34%; FB1:  89.22  1888\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss on epoch 2 = 8.849044799804688\n",
            "conlleval:\n",
            "processed 51578 tokens with 5942 phrases; found: 5937 phrases; correct: 5080.\n",
            "accuracy:  97.58%; precision:  85.57%; recall:  85.49%; FB1:  85.53\n",
            "              LOC: precision:  89.79%; recall:  93.85%; FB1:  91.78  1920\n",
            "             MISC: precision:  77.46%; recall:  76.03%; FB1:  76.74  905\n",
            "              ORG: precision:  78.52%; recall:  70.62%; FB1:  74.36  1206\n",
            "              PER: precision:  89.61%; recall:  92.73%; FB1:  91.14  1906\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss on epoch 3 = 7.123241901397705\n",
            "conlleval:\n",
            "processed 51578 tokens with 5942 phrases; found: 5955 phrases; correct: 5281.\n",
            "accuracy:  98.10%; precision:  88.68%; recall:  88.88%; FB1:  88.78\n",
            "              LOC: precision:  91.92%; recall:  95.43%; FB1:  93.64  1907\n",
            "             MISC: precision:  81.03%; recall:  80.15%; FB1:  80.59  912\n",
            "              ORG: precision:  83.32%; recall:  78.60%; FB1:  80.89  1265\n",
            "              PER: precision:  92.73%; recall:  94.19%; FB1:  93.46  1871\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss on epoch 4 = 6.0636186599731445\n",
            "conlleval:\n",
            "processed 51578 tokens with 5942 phrases; found: 5937 phrases; correct: 5315.\n",
            "accuracy:  98.25%; precision:  89.52%; recall:  89.45%; FB1:  89.49\n",
            "              LOC: precision:  92.03%; recall:  95.59%; FB1:  93.78  1908\n",
            "             MISC: precision:  85.32%; recall:  80.04%; FB1:  82.60  865\n",
            "              ORG: precision:  85.63%; recall:  79.57%; FB1:  82.49  1246\n",
            "              PER: precision:  91.45%; recall:  95.22%; FB1:  93.30  1918\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss on epoch 5 = 5.178715229034424\n",
            "conlleval:\n",
            "processed 51578 tokens with 5942 phrases; found: 5925 phrases; correct: 5366.\n",
            "accuracy:  98.36%; precision:  90.57%; recall:  90.31%; FB1:  90.44\n",
            "              LOC: precision:  93.64%; recall:  95.37%; FB1:  94.50  1871\n",
            "             MISC: precision:  87.15%; recall:  80.15%; FB1:  83.50  848\n",
            "              ORG: precision:  85.11%; recall:  82.70%; FB1:  83.89  1303\n",
            "              PER: precision:  92.80%; recall:  95.87%; FB1:  94.31  1903\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss on epoch 6 = 4.571539878845215\n",
            "conlleval:\n",
            "processed 51578 tokens with 5942 phrases; found: 6080 phrases; correct: 5436.\n",
            "accuracy:  98.42%; precision:  89.41%; recall:  91.48%; FB1:  90.43\n",
            "              LOC: precision:  94.44%; recall:  95.21%; FB1:  94.82  1852\n",
            "             MISC: precision:  85.31%; recall:  81.89%; FB1:  83.56  885\n",
            "              ORG: precision:  80.48%; recall:  87.32%; FB1:  83.76  1455\n",
            "              PER: precision:  93.27%; recall:  95.60%; FB1:  94.42  1888\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss on epoch 7 = 3.7956318855285645\n",
            "conlleval:\n",
            "processed 51578 tokens with 5942 phrases; found: 6009 phrases; correct: 5456.\n",
            "accuracy:  98.62%; precision:  90.80%; recall:  91.82%; FB1:  91.31\n",
            "              LOC: precision:  93.78%; recall:  96.90%; FB1:  95.31  1898\n",
            "             MISC: precision:  86.01%; recall:  82.00%; FB1:  83.95  879\n",
            "              ORG: precision:  87.27%; recall:  84.86%; FB1:  86.05  1304\n",
            "              PER: precision:  92.43%; recall:  96.74%; FB1:  94.54  1928\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss on epoch 8 = 3.260950803756714\n",
            "conlleval:\n",
            "processed 51578 tokens with 5942 phrases; found: 6015 phrases; correct: 5475.\n",
            "accuracy:  98.71%; precision:  91.02%; recall:  92.14%; FB1:  91.58\n",
            "              LOC: precision:  94.90%; recall:  96.30%; FB1:  95.60  1864\n",
            "             MISC: precision:  84.61%; recall:  84.06%; FB1:  84.33  916\n",
            "              ORG: precision:  86.82%; recall:  85.46%; FB1:  86.13  1320\n",
            "              PER: precision:  93.21%; recall:  96.91%; FB1:  95.02  1915\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss on epoch 9 = 2.8344316482543945\n",
            "conlleval:\n",
            "processed 51578 tokens with 5942 phrases; found: 6016 phrases; correct: 5497.\n",
            "accuracy:  98.72%; precision:  91.37%; recall:  92.51%; FB1:  91.94\n",
            "              LOC: precision:  94.28%; recall:  96.90%; FB1:  95.57  1888\n",
            "             MISC: precision:  87.67%; recall:  83.30%; FB1:  85.43  876\n",
            "              ORG: precision:  87.46%; recall:  86.35%; FB1:  86.90  1324\n",
            "              PER: precision:  92.89%; recall:  97.23%; FB1:  95.01  1928\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss on epoch 10 = 2.506464719772339\n",
            "conlleval:\n",
            "processed 51578 tokens with 5942 phrases; found: 6022 phrases; correct: 5501.\n",
            "accuracy:  98.78%; precision:  91.35%; recall:  92.58%; FB1:  91.96\n",
            "              LOC: precision:  94.62%; recall:  96.68%; FB1:  95.64  1877\n",
            "             MISC: precision:  87.74%; recall:  83.84%; FB1:  85.75  881\n",
            "              ORG: precision:  87.09%; recall:  86.50%; FB1:  86.79  1332\n",
            "              PER: precision:  92.75%; recall:  97.29%; FB1:  94.97  1932\n",
            "\n",
            "----------------------------\n",
            "-START-/START/START \"/O/O I/O/O changed/O/O my/O/O strategy/O/O against/O/O him/O/O today/O/O and/O/O had/O/O him/O/O rattled/O/O ,/O/O \"/O/O he/O/O added/O/O ./O/O -END-/END/END\n",
            "Predicted:\t ['START', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'END']\n",
            "Gold:\t\t ['START', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'END']\n",
            "----------------------------\n",
            "-START-/START/START -DOCSTART-/O/O -END-/END/END\n",
            "Predicted:\t ['START', 'O', 'END']\n",
            "Gold:\t\t ['START', 'O', 'END']\n",
            "----------------------------\n",
            "-START-/START/START The/O/O authorities/O/O are/O/O apparently/O/O extremely/O/O afraid/O/O of/O/O any/O/O political/O/O and/O/O social/O/O discontent/O/O ,/O/O \"/O/O said/O/O Xiao/I-PER/I-PER ,/O/O in/O/O Manila/I-LOC/I-LOC to/O/O attend/O/O an/O/O Amnesty/I-ORG/I-ORG International/I-ORG/I-ORG conference/O/O on/O/O human/O/O rights/O/O in/O/O China/I-LOC/I-LOC ./O/O -END-/END/END\n",
            "Predicted:\t ['START', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-PER', 'O', 'O', 'I-LOC', 'O', 'O', 'O', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'I-LOC', 'O', 'END']\n",
            "Gold:\t\t ['START', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-PER', 'O', 'O', 'I-LOC', 'O', 'O', 'O', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'I-LOC', 'O', 'END']\n",
            "----------------------------\n",
            "-START-/START/START Ericsson/I-ORG/I-ORG said/O/O in/O/O a/O/O statement/O/O the/O/O order/O/O was/O/O from/O/O the/O/O Guangdong/I-ORG/I-ORG Post/I-ORG/I-ORG and/I-ORG/I-ORG Telecommunications/I-ORG/I-ORG Administration/I-ORG/I-ORG (/O/O GPTA/I-ORG/I-ORG )/O/O ./O/O -END-/END/END\n",
            "Predicted:\t ['START', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'I-ORG', 'O', 'O', 'END']\n",
            "Gold:\t\t ['START', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'I-ORG', 'O', 'O', 'END']\n",
            "----------------------------\n",
            "-START-/START/START It/O/O did/O/O not/O/O elaborate/O/O ./O/O -END-/END/END\n",
            "Predicted:\t ['START', 'O', 'O', 'O', 'O', 'O', 'END']\n",
            "Gold:\t\t ['START', 'O', 'O', 'O', 'O', 'O', 'END']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss on epoch 11 = 2.143665075302124\n",
            "conlleval:\n",
            "processed 51578 tokens with 5942 phrases; found: 6028 phrases; correct: 5507.\n",
            "accuracy:  98.78%; precision:  91.36%; recall:  92.68%; FB1:  92.01\n",
            "              LOC: precision:  93.92%; recall:  96.79%; FB1:  95.34  1893\n",
            "             MISC: precision:  88.84%; recall:  83.73%; FB1:  86.21  869\n",
            "              ORG: precision:  86.84%; recall:  86.58%; FB1:  86.71  1337\n",
            "              PER: precision:  93.11%; recall:  97.50%; FB1:  95.25  1929\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss on epoch 12 = 2.010096311569214\n",
            "conlleval:\n",
            "processed 51578 tokens with 5942 phrases; found: 6011 phrases; correct: 5517.\n",
            "accuracy:  98.84%; precision:  91.78%; recall:  92.85%; FB1:  92.31\n",
            "              LOC: precision:  94.46%; recall:  96.52%; FB1:  95.48  1877\n",
            "             MISC: precision:  86.66%; recall:  85.25%; FB1:  85.95  907\n",
            "              ORG: precision:  87.56%; recall:  87.10%; FB1:  87.33  1334\n",
            "              PER: precision:  94.56%; recall:  97.18%; FB1:  95.85  1893\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss on epoch 13 = 1.7580928802490234\n",
            "conlleval:\n",
            "processed 51578 tokens with 5942 phrases; found: 6018 phrases; correct: 5540.\n",
            "accuracy:  98.88%; precision:  92.06%; recall:  93.23%; FB1:  92.64\n",
            "              LOC: precision:  94.24%; recall:  97.06%; FB1:  95.63  1892\n",
            "             MISC: precision:  87.78%; recall:  86.44%; FB1:  87.10  908\n",
            "              ORG: precision:  88.83%; recall:  87.17%; FB1:  87.99  1316\n",
            "              PER: precision:  94.16%; recall:  97.23%; FB1:  95.67  1902\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss on epoch 14 = 1.5483956336975098\n",
            "conlleval:\n",
            "processed 51578 tokens with 5942 phrases; found: 5987 phrases; correct: 5531.\n",
            "accuracy:  98.86%; precision:  92.38%; recall:  93.08%; FB1:  92.73\n",
            "              LOC: precision:  94.58%; recall:  96.95%; FB1:  95.75  1883\n",
            "             MISC: precision:  88.14%; recall:  85.47%; FB1:  86.78  894\n",
            "              ORG: precision:  90.10%; recall:  86.88%; FB1:  88.46  1293\n",
            "              PER: precision:  93.74%; recall:  97.56%; FB1:  95.61  1917\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Training LSTM w/ character embeddings. Feel free to change number of epochs, optimizer, learning rate and batch size.\n",
        "\n",
        "#####################################################################################\n",
        "#TODO: Add imports if necessary.\n",
        "from torch.optim import Adam, AdamW, SGD\n",
        "\n",
        "\n",
        "#####################################################################################\n",
        "\n",
        "\n",
        "def train_char_lstm(sentences, tags, lstm):\n",
        "    #####################################################################################\n",
        "    #TODO: initialize optimizer and other hyperparameters.\n",
        "    optim = AdamW(lstm.parameters(), lr=0.001)\n",
        "    nEpochs = 15\n",
        "    batchSize = 64\n",
        "    loss_function = nn.NLLLoss()\n",
        "\n",
        "    # lstm.train()\n",
        "    lstm.cuda()\n",
        "\n",
        "    #####################################################################################\n",
        "\n",
        "    for epoch in range(nEpochs):\n",
        "        totalLoss = 0.0\n",
        "\n",
        "        (sentences_shuffled, tags_shuffled) = shuffle_sentences(sentences, tags)\n",
        "        for batch in tqdm(range(0, len(sentences), batchSize), leave=False):\n",
        "\n",
        "            #####################################################################################\n",
        "            #TODO: Implement gradient update on a batch of data.\n",
        "\n",
        "            optim.zero_grad()\n",
        "\n",
        "            input_data = prepare_input(sentences2indices(sentences_shuffled[batch:batch+batchSize], word2i, train=True))[0].cuda()\n",
        "\n",
        "            X_char = prepare_input_char(sentences2indicesChar(sentences_shuffled[batch:batch+batchSize], char2i))\n",
        "            X_char = X_char.cuda()\n",
        "            lstm_output = lstm.forward(input_data, X_char, train=True)\n",
        "\n",
        "            Y_onehot = prepare_output_onehot(sentences2indices(tags_shuffled[batch:batch+batchSize], tag2i)).cuda()\n",
        "\n",
        "            loss = loss_function(lstm_output.view(-1, lstm.NUM_TAGS), Y_onehot.view(-1, lstm.NUM_TAGS).argmax(dim=1))\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            optim.step()\n",
        "\n",
        "            totalLoss += loss\n",
        "\n",
        "\n",
        "            #####################################################################################\n",
        "\n",
        "        print(f\"loss on epoch {epoch} = {totalLoss}\")\n",
        "        lstm.write_predictions(sentences_dev, 'dev_pred')   #Performance on dev set\n",
        "        print('conlleval:')\n",
        "        print(subprocess.Popen('paste dev dev_pred | perl conlleval.pl -d \"\\t\"', shell=True, stdout=subprocess.PIPE,stderr=subprocess.STDOUT).communicate()[0].decode('UTF-8'))\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            s = sample(range(len(sentences_dev)), 5)\n",
        "            lstm.print_predictions([sentences_dev[i] for i in s], [tags_dev[i] for i in s])\n",
        "\n",
        "char_lstm = CharLSTMtagger(DIM_HID=500, DIM_EMB=300).cuda()\n",
        "train_char_lstm(sentences_train, tags_train, char_lstm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9NaMeuCnS3r",
        "outputId": "277b5e0e-ddb1-472a-eea1-18d032487fd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-02-22 18:22:47--  https://raw.githubusercontent.com/aritter/twitter_nlp/master/data/annotated/wnut16/conlleval.pl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12754 (12K) [text/plain]\n",
            "Saving to: ‘conlleval.pl.2’\n",
            "\n",
            "\rconlleval.pl.2        0%[                    ]       0  --.-KB/s               \rconlleval.pl.2      100%[===================>]  12.46K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2025-02-22 18:22:47 (15.2 MB/s) - ‘conlleval.pl.2’ saved [12754/12754]\n",
            "\n",
            "processed 46666 tokens with 5648 phrases; found: 5755 phrases; correct: 5043.\n",
            "accuracy:  97.83%; precision:  87.63%; recall:  89.29%; FB1:  88.45\n",
            "                 : precision:   0.00%; recall:   0.00%; FB1:   0.00  2\n",
            "              LOC: precision:  89.23%; recall:  93.41%; FB1:  91.27  1746\n",
            "             MISC: precision:  74.76%; recall:  76.78%; FB1:  75.76  721\n",
            "              ORG: precision:  85.71%; recall:  85.55%; FB1:  85.63  1658\n",
            "              PER: precision:  93.67%; recall:  94.31%; FB1:  93.99  1628\n"
          ]
        }
      ],
      "source": [
        "#Evaluation on test set\n",
        "char_lstm.write_predictions(sentences_test, 'test_pred_cnn_lstm.txt')\n",
        "!wget https://raw.githubusercontent.com/aritter/twitter_nlp/master/data/annotated/wnut16/conlleval.pl\n",
        "!paste test test_pred_cnn_lstm.txt | perl conlleval.pl -d \"\\t\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5C4P4Ycm1CT2"
      },
      "source": [
        "## Conditional Random Fields (+2% Course Grade - optional extra credit)\n",
        "\n",
        "Now we are ready to add a CRF layer to the `CharacterLSTMTagger`.  To train the model, implement `conditional_log_likelihood`, using the score (unnormalized log probability) of the gold sequence, in addition to the partition function, $Z(X)$, which is computed using the forward algorithm.  Then, you can simply use Pytorch's automatic differentiation to compute gradients by running backpropagation through the computation graph of the dynamic program (this should be very simple, so long as you are able to correctly implement the forward algorithm using a computation graph that is supported by PyTorch).  This approach to computing gradients for CRFs is discussed in Section 7.5.3 of the [Eisenstein Book](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)\n",
        "\n",
        "You will also need to implement the Viterbi algorithm for inference during decoding.\n",
        "\n",
        "After including CRF training and Viterbi decoding, you should be getting about **92 F1 / 88 F1 on the dev and test set**, respectively.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**IMPORTANT:** Note that training will be substantially slower this time - depending on the efficiency of your implementation, it could take about 5 minutes per epoch (e.g. 50 minutes for 10 iterations).  It is recommended to start out training on a single batch of data (and testing on this same batch), so that you can quickly debug, making sure your model can memorize the labels on a single batch, and then optimize your code.  Once you are fairly confident your code is working properly, then you can train using the full dataset.  We have provided a (commented out) line of code to switch between training on a single batch and the full dataset below.\n",
        "\n",
        "**Hint #1:** While debugging your implementation of the Forward algorithm it is helpful to look at the loss during training.  The loss should never be less than zero (the log-likelihood should always be negative).\n",
        "\n",
        "**Hint #2:** To sum log-probabilities in a numerically stable way at the end of the Forward algorithm, you will want to use [`torch.logsumexp`](https://pytorch.org/docs/stable/generated/torch.logsumexp.html).\n",
        "\n",
        "**Fill in the functions marked as `TODO` in the code block below. Please make your code changes only within the given commented block (#####).**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKBomV-L6SmZ"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "#####################################################################################\n",
        "#TODO: Add imports if needed.\n",
        "\n",
        "#####################################################################################\n",
        "\n",
        "class LSTM_CRFtagger(CharLSTMtagger):\n",
        "    def __init__(self, DIM_EMB=10, DIM_CHAR_EMB=30, DIM_HID=10):\n",
        "        super(LSTM_CRFtagger, self).__init__(DIM_EMB=DIM_EMB, DIM_HID=DIM_HID, DIM_CHAR_EMB=DIM_CHAR_EMB)\n",
        "        #####################################################################################\n",
        "        #TODO: Initialize parameters.\n",
        "\n",
        "        self.transitions = nn.Parameter(torch.randn(self.NUM_TAGS, self.NUM_TAGS))\n",
        "\n",
        "        #####################################################################################\n",
        "\n",
        "    def gold_score(self, lstm_scores, Y):\n",
        "        #####################################################################################\n",
        "        #TODO: compute score of gold sequence Y (unnormalized conditional log-probability)\n",
        "\n",
        "        score = lstm_scores[0, Y[0]]\n",
        "        seq_len = Y.size(0)\n",
        "        for t in range(1, seq_len):\n",
        "            score = score + self.transitions[Y[t-1], Y[t]] + lstm_scores[t, Y[t]]\n",
        "        return score\n",
        "\n",
        "\n",
        "        #####################################################################################\n",
        "\n",
        "\n",
        "    #Forward algorithm for a single sentence\n",
        "    #Efficiency will eventually be important here.  We recommend you start by\n",
        "    #training on a single batch and make sure your code can memorize the\n",
        "    #training data.  Then you can go back and re-write the inner loop using\n",
        "    #tensor operations to speed things up.\n",
        "    def forward_algorithm(self, lstm_scores, sLen):\n",
        "        #####################################################################################\n",
        "        #TODO: compute partition function Z\n",
        "\n",
        "        alpha = lstm_scores[0]  # shape: (NUM_TAGS,)\n",
        "        for t in range(1, sLen):\n",
        "          alpha = torch.logsumexp(alpha.unsqueeze(1) + self.transitions, dim=0) + lstm_scores[t]\n",
        "\n",
        "        return torch.logsumexp(alpha, dim=0)\n",
        "\n",
        "\n",
        "        #####################################################################################\n",
        "\n",
        "    def conditional_log_likelihood(self, sentences, tags, train=True):\n",
        "        #####################################################################################\n",
        "        #TODO: compute conditional log likelihood of Y (use forward_algorithm and gold_score)\n",
        "        (X, X_mask, X_char) = self.sentences2input_tensors(sentences)\n",
        "        lstm_scores = self.forward(X.cuda(), X_char.cuda(), train=train)\n",
        "        total_loss = 0.0\n",
        "        batch_size = lstm_scores.size(0)\n",
        "        for i in range(batch_size):\n",
        "            sLen = int(X_mask[i].sum().item())\n",
        "            scores = lstm_scores[i][:sLen]  # (sLen, NUM_TAGS)\n",
        "            gold_tags = torch.tensor(sentences2indices([tags[i]], tag2i)[0], device=scores.device)\n",
        "            gold = self.gold_score(scores, gold_tags)\n",
        "            partition = self.forward_algorithm(scores, sLen)\n",
        "            total_loss = total_loss + (partition - gold)\n",
        "        return total_loss / batch_size\n",
        "\n",
        "        #####################################################################################\n",
        "\n",
        "    def viterbi(self, lstm_scores, sLen):\n",
        "        #####################################################################################\n",
        "        #TODO: Implement the Viterbi algorithm\n",
        "        backpointers = []\n",
        "\n",
        "        best_score = lstm_scores[0]\n",
        "        for t in range(1, sLen):\n",
        "\n",
        "            scores_t = best_score.unsqueeze(1) + self.transitions  # shape: (NUM_TAGS, NUM_TAGS)\n",
        "            best_prev_scores, best_prev_tags = torch.max(scores_t, dim=0)\n",
        "            best_score = best_prev_scores + lstm_scores[t]\n",
        "            backpointers.append(best_prev_tags)\n",
        "\n",
        "        best_final_tag = torch.argmax(best_score).item()\n",
        "        best_path = [best_final_tag]\n",
        "        for bp in reversed(backpointers):\n",
        "            best_final_tag = bp[best_final_tag].item()\n",
        "            best_path.insert(0, best_final_tag)\n",
        "        return best_path, best_score[best_final_tag]\n",
        "\n",
        "        #####################################################################################\n",
        "\n",
        "    #Computes Viterbi sequences on a batch of data.\n",
        "    def viterbi_batch(self, sentences):\n",
        "        viterbiSeqs = []\n",
        "        (X, X_mask, X_char) = self.sentences2input_tensors(sentences)\n",
        "        lstm_scores = self.forward(X.cuda(), X_char.cuda(), train=True)\n",
        "        for s in range(len(sentences)):\n",
        "            (viterbiSeq, ll) = self.viterbi(lstm_scores[s], len(sentences[s]))\n",
        "            viterbiSeqs.append(viterbiSeq)\n",
        "\n",
        "        max_len = max(len(seq) for seq in viterbiSeqs)\n",
        "\n",
        "        padded_seqs = [seq + [0] * (max_len - len(seq)) for seq in viterbiSeqs]\n",
        "\n",
        "\n",
        "        return torch.tensor(padded_seqs, dtype=torch.long)\n",
        "\n",
        "\n",
        "    def forward(self, X, X_char, train=False):\n",
        "        #####################################################################################\n",
        "        #TODO: Implement the forward computation.\n",
        "        return super(LSTM_CRFtagger, self).forward(X, X_char, train=train)\n",
        "\n",
        "        #####################################################################################\n",
        "\n",
        "\n",
        "    def print_predictions(self, words, tags):\n",
        "        Y_pred = self.inference(words)\n",
        "        for i in range(len(words)):\n",
        "            print(\"----------------------------\")\n",
        "            print(\" \".join([f\"{words[i][j]}/{Y_pred[i][j]}/{tags[i][j]}\" for j in range(len(words[i]))]))\n",
        "            print(\"Predicted:\\t\", [Y_pred[i][j] for j in range(len(words[i]))])\n",
        "            print(\"Gold:\\t\\t\", tags[i])\n",
        "\n",
        "    #Need to use Viterbi this time.\n",
        "    def inference(self, sentences, viterbi=True):\n",
        "        pred = self.viterbi_batch(sentences)\n",
        "        return [[i2tag[pred[i][j].item()] for j in range(len(sentences[i]))] for i in range(len(sentences))]\n",
        "\n",
        "lstm_crf = LSTM_CRFtagger(DIM_EMB=300).cuda()\n",
        "# print(lstm_crf.conditional_log_likelihood(sentences_dev[0:5], tags_dev[0:5]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjcSzfi27uOM",
        "outputId": "667a0e9e-b740-489a-df37-68d46eea783b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(44.7500, device='cuda:0', grad_fn=<DivBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# This is a cell for debugging, feel free to change it as you like\n",
        "print(lstm_crf.conditional_log_likelihood(sentences_dev[0:5], tags_dev[0:5]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "in7f4nDWc6dd",
        "outputId": "b1d32001-1684-4c13-8623-7c7bb2db6109"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss on epoch 0 = 694.3001825809479\n",
            "conlleval:\n",
            "processed 51578 tokens with 5942 phrases; found: 5692 phrases; correct: 4684.\n",
            "accuracy:  96.42%; precision:  82.29%; recall:  78.83%; FB1:  80.52\n",
            "              LOC: precision:  91.30%; recall:  82.80%; FB1:  86.84  1666\n",
            "             MISC: precision:  73.99%; recall:  67.57%; FB1:  70.63  842\n",
            "              ORG: precision:  72.07%; recall:  67.34%; FB1:  69.62  1253\n",
            "              PER: precision:  84.77%; recall:  88.87%; FB1:  86.77  1931\n",
            "\n",
            "----------------------------\n",
            "-START-/START/START It/O/O brought/O/O in/O/O 4,275/O/O tonnes/O/O of/O/O British/I-MISC/I-MISC mutton/O/O ,/O/O some/O/O 10/O/O percent/O/O of/O/O overall/O/O imports/O/O ./O/O -END-/END/END\n",
            "Predicted:\t ['START', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'END']\n",
            "Gold:\t\t ['START', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'END']\n",
            "----------------------------\n",
            "-START-/START/START FRANKFURT/I-LOC/I-LOC 1996-08-22/O/O -END-/END/END\n",
            "Predicted:\t ['START', 'I-LOC', 'O', 'END']\n",
            "Gold:\t\t ['START', 'I-LOC', 'O', 'END']\n",
            "----------------------------\n",
            "-START-/START/START A/O/O Florida/I-LOC/I-LOC restaurant/O/O paid/O/O 10,925/O/O pounds/O/O (/O/O $/O/O 16,935/O/O )/O/O for/O/O the/O/O draft/O/O of/O/O \"/O/O Ai/O/I-MISC n't/O/I-MISC no/O/I-MISC telling/O/I-MISC \"/O/O ,/O/O which/O/O Hendrix/I-PER/I-PER penned/O/O on/O/O a/O/O piece/O/O of/O/O London/I-LOC/I-LOC hotel/O/O stationery/O/O in/O/O late/O/O 1966/O/O ./O/O -END-/END/END\n",
            "Predicted:\t ['START', 'O', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'END']\n",
            "Gold:\t\t ['START', 'O', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O', 'O', 'O', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'END']\n",
            "----------------------------\n",
            "-START-/START/START The/O/O guitarist/I-PER/O died/O/O of/O/O a/O/O drugs/O/O overdose/O/O in/O/O 1970/O/O aged/O/O 27/O/O ./O/O -END-/END/END\n",
            "Predicted:\t ['START', 'O', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'END']\n",
            "Gold:\t\t ['START', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'END']\n",
            "----------------------------\n",
            "-START-/START/START BEIJING/I-LOC/I-LOC 1996-08-22/O/O -END-/END/END\n",
            "Predicted:\t ['START', 'I-LOC', 'O', 'END']\n",
            "Gold:\t\t ['START', 'I-LOC', 'O', 'END']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss on epoch 1 = 143.81007074564695\n",
            "conlleval:\n",
            "processed 51578 tokens with 5942 phrases; found: 5710 phrases; correct: 4947.\n",
            "accuracy:  97.19%; precision:  86.64%; recall:  83.25%; FB1:  84.91\n",
            "              LOC: precision:  92.16%; recall:  88.95%; FB1:  90.53  1773\n",
            "             MISC: precision:  83.40%; recall:  71.91%; FB1:  77.23  795\n",
            "              ORG: precision:  78.85%; recall:  73.68%; FB1:  76.18  1253\n",
            "              PER: precision:  87.98%; recall:  90.23%; FB1:  89.09  1889\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss on epoch 2 = 103.66524684429169\n",
            "conlleval:\n",
            "processed 51578 tokens with 5942 phrases; found: 5925 phrases; correct: 5148.\n",
            "accuracy:  97.66%; precision:  86.89%; recall:  86.64%; FB1:  86.76\n",
            "              LOC: precision:  91.27%; recall:  90.53%; FB1:  90.90  1822\n",
            "             MISC: precision:  77.10%; recall:  78.52%; FB1:  77.81  939\n",
            "              ORG: precision:  82.95%; recall:  78.00%; FB1:  80.40  1261\n",
            "              PER: precision:  90.12%; recall:  93.11%; FB1:  91.59  1903\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss on epoch 3 = 81.7652291432023\n",
            "conlleval:\n",
            "processed 51578 tokens with 5942 phrases; found: 5845 phrases; correct: 5177.\n",
            "accuracy:  97.81%; precision:  88.57%; recall:  87.13%; FB1:  87.84\n",
            "              LOC: precision:  94.00%; recall:  91.24%; FB1:  92.60  1783\n",
            "             MISC: precision:  80.17%; recall:  80.69%; FB1:  80.43  928\n",
            "              ORG: precision:  85.81%; recall:  77.11%; FB1:  81.23  1205\n",
            "              PER: precision:  89.32%; recall:  93.54%; FB1:  91.38  1929\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss on epoch 4 = 71.03925773501396\n",
            "conlleval:\n",
            "processed 51578 tokens with 5942 phrases; found: 5941 phrases; correct: 5250.\n",
            "accuracy:  97.93%; precision:  88.37%; recall:  88.35%; FB1:  88.36\n",
            "              LOC: precision:  93.05%; recall:  92.54%; FB1:  92.79  1827\n",
            "             MISC: precision:  84.12%; recall:  79.28%; FB1:  81.63  869\n",
            "              ORG: precision:  79.12%; recall:  82.25%; FB1:  80.66  1394\n",
            "              PER: precision:  92.71%; recall:  93.16%; FB1:  92.93  1851\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss on epoch 5 = 68.21696072816849\n",
            "conlleval:\n",
            "processed 51578 tokens with 5942 phrases; found: 5938 phrases; correct: 5243.\n",
            "accuracy:  97.93%; precision:  88.30%; recall:  88.24%; FB1:  88.27\n",
            "              LOC: precision:  89.61%; recall:  93.41%; FB1:  91.47  1915\n",
            "             MISC: precision:  81.15%; recall:  81.24%; FB1:  81.19  923\n",
            "              ORG: precision:  85.18%; recall:  77.55%; FB1:  81.19  1221\n",
            "              PER: precision:  92.50%; recall:  94.35%; FB1:  93.42  1879\n",
            "\n",
            "----------------------------\n",
            "-START-/START/START BRUSSELS/I-LOC/I-LOC 1996-08-22/O/O -END-/END/END\n",
            "Predicted:\t ['START', 'I-LOC', 'O', 'END']\n",
            "Gold:\t\t ['START', 'I-LOC', 'O', 'END']\n",
            "----------------------------\n",
            "-START-/START/START State/O/O media/O/O quoted/O/O China/I-LOC/I-LOC 's/O/O top/O/O negotiator/O/O with/O/O Taipei/I-LOC/I-LOC ,/O/O Tang/I-PER/I-PER Shubei/I-PER/I-PER ,/O/O as/O/O telling/O/O a/O/O visiting/O/O group/O/O from/O/O Taiwan/I-LOC/I-LOC on/O/O Wednesday/O/O that/O/O it/O/O was/O/O time/O/O for/O/O the/O/O rivals/O/O to/O/O hold/O/O political/O/O talks/O/O ./O/O -END-/END/END\n",
            "Predicted:\t ['START', 'O', 'O', 'O', 'I-LOC', 'O', 'O', 'O', 'O', 'I-LOC', 'O', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'END']\n",
            "Gold:\t\t ['START', 'O', 'O', 'O', 'I-LOC', 'O', 'O', 'O', 'O', 'I-LOC', 'O', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'END']\n",
            "----------------------------\n",
            "-START-/START/START Germany/I-LOC/I-LOC 's/O/O representative/O/O to/O/O the/O/O European/I-ORG/I-ORG Union/I-ORG/I-ORG 's/O/O veterinary/O/O committee/O/O Werner/I-PER/I-PER Zwingmann/I-PER/I-PER said/O/O on/O/O Wednesday/O/O consumers/O/O should/O/O buy/O/O sheepmeat/O/O from/O/O countries/O/O other/O/O than/O/O Britain/I-LOC/I-LOC until/O/O the/O/O scientific/O/O advice/O/O was/O/O clearer/O/O ./O/O -END-/END/END\n",
            "Predicted:\t ['START', 'I-LOC', 'O', 'O', 'O', 'O', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'END']\n",
            "Gold:\t\t ['START', 'I-LOC', 'O', 'O', 'O', 'O', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'END']\n",
            "----------------------------\n",
            "-START-/START/START The/O/O office/O/O said/O/O 356,725/O/O new/O/O cars/O/O were/O/O registered/O/O in/O/O July/O/O 1996/O/O --/O/O 304,850/O/O passenger/O/O cars/O/O and/O/O 15,613/O/O trucks/O/O ./O/O -END-/END/END\n",
            "Predicted:\t ['START', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'END']\n",
            "Gold:\t\t ['START', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'END']\n",
            "----------------------------\n",
            "-START-/START/START The/O/O guitarist/O/O died/O/O of/O/O a/O/O drugs/O/O overdose/O/O in/O/O 1970/O/O aged/O/O 27/O/O ./O/O -END-/END/END\n",
            "Predicted:\t ['START', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'END']\n",
            "Gold:\t\t ['START', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'END']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss on epoch 6 = 59.262015253305435\n",
            "conlleval:\n",
            "processed 51578 tokens with 5942 phrases; found: 5936 phrases; correct: 5285.\n",
            "accuracy:  98.10%; precision:  89.03%; recall:  88.94%; FB1:  88.99\n",
            "              LOC: precision:  92.98%; recall:  93.69%; FB1:  93.33  1851\n",
            "             MISC: precision:  82.24%; recall:  81.34%; FB1:  81.79  912\n",
            "              ORG: precision:  85.35%; recall:  79.49%; FB1:  82.32  1249\n",
            "              PER: precision:  90.85%; recall:  94.90%; FB1:  92.83  1924\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss on epoch 7 = 56.06701733916998\n",
            "conlleval:\n",
            "processed 51578 tokens with 5942 phrases; found: 5916 phrases; correct: 5287.\n",
            "accuracy:  98.12%; precision:  89.37%; recall:  88.98%; FB1:  89.17\n",
            "                 : precision:   0.00%; recall:   0.00%; FB1:   0.00  1\n",
            "              LOC: precision:  91.81%; recall:  92.71%; FB1:  92.25  1855\n",
            "             MISC: precision:  83.09%; recall:  80.48%; FB1:  81.76  893\n",
            "              ORG: precision:  86.25%; recall:  80.46%; FB1:  83.26  1251\n",
            "              PER: precision:  92.01%; recall:  95.71%; FB1:  93.83  1916\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss on epoch 8 = 54.454126477241516\n",
            "conlleval:\n",
            "processed 51578 tokens with 5942 phrases; found: 5871 phrases; correct: 5215.\n",
            "accuracy:  98.00%; precision:  88.83%; recall:  87.77%; FB1:  88.29\n",
            "              LOC: precision:  91.86%; recall:  92.16%; FB1:  92.01  1843\n",
            "             MISC: precision:  87.22%; recall:  79.18%; FB1:  83.00  837\n",
            "              ORG: precision:  79.97%; recall:  82.48%; FB1:  81.20  1383\n",
            "              PER: precision:  93.25%; recall:  91.53%; FB1:  92.38  1808\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss on epoch 9 = 53.42488244920969\n",
            "conlleval:\n",
            "processed 51578 tokens with 5942 phrases; found: 5923 phrases; correct: 5279.\n",
            "accuracy:  98.07%; precision:  89.13%; recall:  88.84%; FB1:  88.98\n",
            "              LOC: precision:  92.90%; recall:  93.36%; FB1:  93.13  1846\n",
            "             MISC: precision:  82.31%; recall:  80.26%; FB1:  81.27  899\n",
            "              ORG: precision:  84.73%; recall:  82.33%; FB1:  83.51  1303\n",
            "              PER: precision:  91.73%; recall:  93.38%; FB1:  92.55  1875\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss on epoch 10 = 49.51237042248249\n",
            "conlleval:\n",
            "processed 51578 tokens with 5942 phrases; found: 5906 phrases; correct: 5269.\n",
            "accuracy:  98.11%; precision:  89.21%; recall:  88.67%; FB1:  88.94\n",
            "                 : precision:   0.00%; recall:   0.00%; FB1:   0.00  1\n",
            "              LOC: precision:  92.08%; recall:  92.98%; FB1:  92.52  1855\n",
            "             MISC: precision:  83.09%; recall:  80.48%; FB1:  81.76  893\n",
            "              ORG: precision:  84.34%; recall:  79.94%; FB1:  82.08  1271\n",
            "              PER: precision:  92.63%; recall:  94.84%; FB1:  93.72  1886\n",
            "\n",
            "----------------------------\n",
            "-START-/START/START BEIJING/I-LOC/I-LOC 1996-08-22/O/O -END-/END/END\n",
            "Predicted:\t ['START', 'I-LOC', 'O', 'END']\n",
            "Gold:\t\t ['START', 'I-LOC', 'O', 'END']\n",
            "----------------------------\n",
            "-START-/START/START -DOCSTART-/O/O -END-/END/END\n",
            "Predicted:\t ['START', 'O', 'END']\n",
            "Gold:\t\t ['START', 'O', 'END']\n",
            "----------------------------\n",
            "-START-/START/START China/I-LOC/I-LOC says/O/O Taiwan/I-LOC/I-LOC spoils/O/O atmosphere/O/O for/O/O talks/O/O ./O/O -END-/END/END\n",
            "Predicted:\t ['START', 'I-LOC', 'O', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'END']\n",
            "Gold:\t\t ['START', 'I-LOC', 'O', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'END']\n",
            "----------------------------\n",
            "-START-/START/START \"/O/O We/O/O do/O/O n't/O/O support/O/O any/O/O such/O/O recommendation/O/O because/O/O we/O/O do/O/O n't/O/O see/O/O any/O/O grounds/O/O for/O/O it/O/O ,/O/O \"/O/O the/O/O Commission/I-ORG/I-ORG 's/O/O chief/O/O spokesman/O/O Nikolaus/I-PER/I-PER van/I-PER/I-PER der/I-PER/I-PER Pas/I-PER/I-PER told/O/O a/O/O news/O/O briefing/O/O ./O/O -END-/END/END\n",
            "Predicted:\t ['START', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ORG', 'O', 'O', 'O', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'END']\n",
            "Gold:\t\t ['START', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ORG', 'O', 'O', 'O', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'END']\n",
            "----------------------------\n",
            "-START-/START/START \"/O/O What/O/O we/O/O have/O/O to/O/O be/O/O extremely/O/O careful/O/O of/O/O is/O/O how/O/O other/O/O countries/O/O are/O/O going/O/O to/O/O take/O/O Germany/I-LOC/I-LOC 's/O/O lead/O/O ,/O/O \"/O/O Welsh/I-ORG/I-ORG National/I-ORG/I-ORG Farmers/I-ORG/I-ORG '/I-ORG/I-ORG Union/I-ORG/I-ORG (/O/O NFU/I-ORG/I-ORG )/O/O chairman/O/O John/I-PER/I-PER Lloyd/I-PER/I-PER Jones/I-PER/I-PER said/O/O on/O/O BBC/I-ORG/I-ORG radio/I-ORG/I-ORG ./O/O -END-/END/END\n",
            "Predicted:\t ['START', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-LOC', 'O', 'O', 'O', 'O', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'I-ORG', 'O', 'O', 'I-PER', 'I-PER', 'I-PER', 'O', 'O', 'I-ORG', 'I-ORG', 'O', 'END']\n",
            "Gold:\t\t ['START', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-LOC', 'O', 'O', 'O', 'O', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'I-ORG', 'O', 'O', 'I-PER', 'I-PER', 'I-PER', 'O', 'O', 'I-ORG', 'I-ORG', 'O', 'END']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss on epoch 11 = 47.332240611314774\n",
            "conlleval:\n",
            "processed 51578 tokens with 5942 phrases; found: 5906 phrases; correct: 5303.\n",
            "accuracy:  98.19%; precision:  89.79%; recall:  89.25%; FB1:  89.52\n",
            "              LOC: precision:  91.16%; recall:  94.88%; FB1:  92.98  1912\n",
            "             MISC: precision:  86.55%; recall:  78.85%; FB1:  82.52  840\n",
            "              ORG: precision:  85.13%; recall:  81.95%; FB1:  83.51  1291\n",
            "              PER: precision:  93.08%; recall:  94.14%; FB1:  93.60  1863\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss on epoch 12 = 48.974113926291466\n",
            "conlleval:\n",
            "processed 51578 tokens with 5942 phrases; found: 5894 phrases; correct: 5272.\n",
            "accuracy:  98.09%; precision:  89.45%; recall:  88.72%; FB1:  89.08\n",
            "              LOC: precision:  94.46%; recall:  91.83%; FB1:  93.13  1786\n",
            "             MISC: precision:  83.63%; recall:  80.91%; FB1:  82.25  892\n",
            "              ORG: precision:  81.40%; recall:  83.89%; FB1:  82.63  1382\n",
            "              PER: precision:  93.46%; recall:  93.05%; FB1:  93.25  1834\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss on epoch 13 = 48.682118490338326\n",
            "conlleval:\n",
            "processed 51578 tokens with 5942 phrases; found: 5895 phrases; correct: 5253.\n",
            "accuracy:  98.05%; precision:  89.11%; recall:  88.40%; FB1:  88.76\n",
            "                 : precision:   0.00%; recall:   0.00%; FB1:   0.00  1\n",
            "              LOC: precision:  93.99%; recall:  92.00%; FB1:  92.98  1798\n",
            "             MISC: precision:  81.36%; recall:  80.48%; FB1:  80.92  912\n",
            "              ORG: precision:  83.38%; recall:  81.21%; FB1:  82.28  1306\n",
            "              PER: precision:  92.23%; recall:  94.03%; FB1:  93.12  1878\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss on epoch 14 = 48.52940855920315\n",
            "conlleval:\n",
            "processed 51578 tokens with 5942 phrases; found: 5884 phrases; correct: 5259.\n",
            "accuracy:  98.12%; precision:  89.38%; recall:  88.51%; FB1:  88.94\n",
            "              LOC: precision:  92.78%; recall:  92.27%; FB1:  92.52  1827\n",
            "             MISC: precision:  83.37%; recall:  81.56%; FB1:  82.46  902\n",
            "              ORG: precision:  83.64%; recall:  81.58%; FB1:  82.60  1308\n",
            "              PER: precision:  93.02%; recall:  93.27%; FB1:  93.14  1847\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss on epoch 15 = 48.969045743346214\n",
            "conlleval:\n",
            "processed 51578 tokens with 5942 phrases; found: 5957 phrases; correct: 5321.\n",
            "accuracy:  98.17%; precision:  89.32%; recall:  89.55%; FB1:  89.44\n",
            "              LOC: precision:  91.91%; recall:  94.01%; FB1:  92.95  1879\n",
            "             MISC: precision:  84.81%; recall:  81.78%; FB1:  83.27  889\n",
            "              ORG: precision:  84.66%; recall:  81.06%; FB1:  82.82  1284\n",
            "              PER: precision:  92.02%; recall:  95.17%; FB1:  93.57  1905\n",
            "\n",
            "----------------------------\n",
            "-START-/START/START China/I-LOC/I-LOC has/O/O said/O/O it/O/O was/O/O time/O/O for/O/O political/O/O talks/O/O with/O/O Taiwan/I-LOC/I-LOC and/O/O that/O/O the/O/O rival/O/O island/O/O should/O/O take/O/O practical/O/O steps/O/O towards/O/O that/O/O goal/O/O ./O/O -END-/END/END\n",
            "Predicted:\t ['START', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'END']\n",
            "Gold:\t\t ['START', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'END']\n",
            "----------------------------\n",
            "-START-/START/START that/O/O is/O/O to/O/O end/O/O the/O/O state/O/O of/O/O hostility/O/O ,/O/O \"/O/O Thursday/O/O 's/O/O overseas/O/O edition/O/O of/O/O the/I-ORG/O People/I-ORG/I-ORG 's/I-ORG/I-ORG Daily/I-ORG/I-ORG quoted/O/O Tang/I-PER/I-PER as/O/O saying/O/O ./O/O -END-/END/END\n",
            "Predicted:\t ['START', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'I-PER', 'O', 'O', 'O', 'END']\n",
            "Gold:\t\t ['START', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'I-PER', 'O', 'O', 'O', 'END']\n",
            "----------------------------\n",
            "-START-/START/START Buyers/O/O also/O/O snapped/O/O up/O/O 16/O/O other/O/O items/O/O that/O/O were/O/O put/O/O up/O/O for/O/O auction/O/O by/O/O Hendrix/I-PER/I-PER 's/O/O former/O/O girlfriend/O/O Kathy/I-PER/I-PER Etchingham/I-PER/I-PER ,/O/O who/O/O lived/O/O with/O/O him/O/O from/O/O 1966/O/O to/O/O 1969/O/O ./O/O -END-/END/END\n",
            "Predicted:\t ['START', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-PER', 'O', 'O', 'O', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'END']\n",
            "Gold:\t\t ['START', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-PER', 'O', 'O', 'O', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'END']\n",
            "----------------------------\n",
            "-START-/START/START Only/O/O France/I-LOC/I-LOC and/O/O Britain/I-LOC/I-LOC backed/O/O Fischler/I-PER/I-PER 's/O/O proposal/O/O ./O/O -END-/END/END\n",
            "Predicted:\t ['START', 'O', 'I-LOC', 'O', 'I-LOC', 'O', 'I-PER', 'O', 'O', 'O', 'END']\n",
            "Gold:\t\t ['START', 'O', 'I-LOC', 'O', 'I-LOC', 'O', 'I-PER', 'O', 'O', 'O', 'END']\n",
            "----------------------------\n",
            "-START-/START/START -DOCSTART-/O/O -END-/END/END\n",
            "Predicted:\t ['START', 'O', 'END']\n",
            "Gold:\t\t ['START', 'O', 'END']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss on epoch 16 = 48.595340356230736\n",
            "conlleval:\n",
            "processed 51578 tokens with 5942 phrases; found: 5941 phrases; correct: 5228.\n",
            "accuracy:  97.90%; precision:  88.00%; recall:  87.98%; FB1:  87.99\n",
            "              LOC: precision:  92.54%; recall:  91.78%; FB1:  92.16  1822\n",
            "             MISC: precision:  78.75%; recall:  82.00%; FB1:  80.34  960\n",
            "              ORG: precision:  80.87%; recall:  83.22%; FB1:  82.03  1380\n",
            "              PER: precision:  93.87%; recall:  90.66%; FB1:  92.24  1779\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss on epoch 17 = 50.954543486237526\n",
            "conlleval:\n",
            "processed 51578 tokens with 5942 phrases; found: 5953 phrases; correct: 5275.\n",
            "accuracy:  98.05%; precision:  88.61%; recall:  88.77%; FB1:  88.69\n",
            "                 : precision:   0.00%; recall:   0.00%; FB1:   0.00  2\n",
            "              LOC: precision:  91.98%; recall:  93.09%; FB1:  92.53  1859\n",
            "             MISC: precision:  84.94%; recall:  80.15%; FB1:  82.48  870\n",
            "              ORG: precision:  81.66%; recall:  81.36%; FB1:  81.51  1336\n",
            "              PER: precision:  91.99%; recall:  94.19%; FB1:  93.08  1886\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss on epoch 18 = 47.84092094004154\n",
            "conlleval:\n",
            "processed 51578 tokens with 5942 phrases; found: 5886 phrases; correct: 5295.\n",
            "accuracy:  98.17%; precision:  89.96%; recall:  89.11%; FB1:  89.53\n",
            "              LOC: precision:  93.07%; recall:  92.87%; FB1:  92.97  1833\n",
            "             MISC: precision:  86.87%; recall:  80.37%; FB1:  83.49  853\n",
            "              ORG: precision:  84.23%; recall:  81.28%; FB1:  82.73  1294\n",
            "              PER: precision:  92.24%; recall:  95.44%; FB1:  93.81  1906\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss on epoch 19 = 45.419065117836\n",
            "conlleval:\n",
            "processed 51578 tokens with 5942 phrases; found: 5885 phrases; correct: 5232.\n",
            "accuracy:  98.01%; precision:  88.90%; recall:  88.05%; FB1:  88.48\n",
            "                 : precision:   0.00%; recall:   0.00%; FB1:   0.00  2\n",
            "              LOC: precision:  91.98%; recall:  93.63%; FB1:  92.80  1870\n",
            "             MISC: precision:  79.78%; recall:  80.48%; FB1:  80.13  930\n",
            "              ORG: precision:  85.37%; recall:  78.75%; FB1:  81.92  1237\n",
            "              PER: precision:  92.85%; recall:  93.05%; FB1:  92.95  1846\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#CharLSTM-CRF Training\n",
        "\n",
        "#####################################################################################\n",
        "# TODO: Add imports if needed.\n",
        "import torch.optim as optim\n",
        "import torch.nn.utils as utils\n",
        "#####################################################################################\n",
        "\n",
        "#Get CoNLL evaluation script\n",
        "os.system('wget https://raw.githubusercontent.com/aritter/twitter_nlp/master/data/annotated/wnut16/conlleval.pl')\n",
        "\n",
        "def train_crf_lstm(sentences, tags, lstm):\n",
        "    #####################################################################################\n",
        "    #TODO: initialize optimizer and hyperparameters.\n",
        "\n",
        "    optimizer = AdamW(lstm.parameters(), lr=0.01)\n",
        "    # optimizer = SGD(lstm.parameters(), lr=0.01, momentum=0.9)\n",
        "    nEpochs = 20\n",
        "    batchSize = 128\n",
        "\n",
        "    lstm.train()\n",
        "    lstm.cuda()\n",
        "\n",
        "\n",
        "\n",
        "    #####################################################################################\n",
        "    for epoch in range(nEpochs):\n",
        "        totalLoss = 0.0\n",
        "        lstm.train()\n",
        "\n",
        "        #Shuffle the sentences\n",
        "        (sentences_shuffled, tags_shuffled) = shuffle_sentences(sentences, tags)\n",
        "        for batch in tqdm(range(0, len(sentences), batchSize), leave=False):\n",
        "            #####################################################################################\n",
        "            #TODO: Implement gradient update on a batch of data.\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            batch_sentences = sentences_shuffled[batch:batch+batchSize]\n",
        "            batch_tags = tags_shuffled[batch:batch+batchSize]\n",
        "\n",
        "\n",
        "            loss = lstm.conditional_log_likelihood(batch_sentences, batch_tags, train=True)\n",
        "\n",
        "\n",
        "            loss.backward()\n",
        "            utils.clip_grad_norm_(lstm.parameters(), max_norm=5.0)\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "            totalLoss += loss.item()\n",
        "\n",
        "            #####################################################################################\n",
        "\n",
        "\n",
        "\n",
        "        print(f\"loss on epoch {epoch} = {totalLoss}\")\n",
        "        lstm.write_predictions(sentences_dev, 'dev_pred')   #Performance on dev set\n",
        "        print('conlleval:')\n",
        "        print(subprocess.Popen('paste dev dev_pred | perl conlleval.pl -d \"\\t\"', shell=True, stdout=subprocess.PIPE,stderr=subprocess.STDOUT).communicate()[0].decode('UTF-8'))\n",
        "\n",
        "        if epoch % 5 == 0:\n",
        "            lstm.eval()\n",
        "            s = random.sample(range(50), 5)\n",
        "            lstm.print_predictions([sentences_train[i] for i in s], [tags_train[i] for i in s])   #Print predictions on train data (useful for debugging)\n",
        "\n",
        "crf_lstm = LSTM_CRFtagger(DIM_HID=500, DIM_EMB=300, DIM_CHAR_EMB=30).cuda()\n",
        "train_crf_lstm(sentences_train, tags_train, crf_lstm)             #Train on the full dataset\n",
        "# train_crf_lstm(sentences_train[0:50], tags_train[0:50], crf_lstm)   #Train only the first batch (use this during development/debugging)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4omMnnmjJPRD"
      },
      "outputs": [],
      "source": [
        "crf_lstm.eval()\n",
        "crf_lstm.write_predictions(sentences_test, 'test_pred_cnn_lstm_crf.txt')\n",
        "!wget https://raw.githubusercontent.com/aritter/twitter_nlp/master/data/annotated/wnut16/conlleval.pl\n",
        "!paste test test_pred_cnn_lstm_crf.txt | perl conlleval.pl -d \"\\t\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkO1dBtfHmWM"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkjfQ1kRyAKa"
      },
      "source": [
        "## Gradescope\n",
        "\n",
        "This is the end. Congratulations!  \n",
        "\n",
        "Now, follow the steps below to submit your homework in [Gradescope](https://www.gradescope.com/courses/939466):\n",
        "\n",
        "1. Rename this ipynb file to 'CS4650_p2_GTusername.ipynb'. We recommend ensuring you have removed any extraneous cells & print statements, clearing all outputs, and using the Runtime --> Run all tool to make sure all output is update to date. Additionally, leaving comments in your code to help us understand your operations will assist the teaching staff in grading. It is not a requirement, but is recommended.\n",
        "2. Click on the menu 'File' --> 'Download' --> 'Download .py'.\n",
        "3. Click on the menu 'File' --> 'Download' --> 'Download .ipynb'.\n",
        "4. Download the notebook as a .pdf document. Make sure the training and evaluation output are captured so we can see how the loss and accuracy changes while training.\n",
        "5. Download the predictions from Colab by clicking the folder icon on the left and finding them under Files, including 'test_pred_lstm.txt', 'test_pred_cnn_lstm.txt', and 'test_pred_cnn_lstm_crf.txt' (optional).\n",
        "5. Upload all 5 or 6 files to GradeScope:\n",
        "> CS4650_p2_GTusername.ipynb\n",
        ">\n",
        "> CS4650_p2_GTusername.py\n",
        ">\n",
        "> CS4650_p2_GTusername.pdf\n",
        ">\n",
        "> test_pred_lstm.txt\n",
        ">\n",
        "> test_pred_cnn_lstm.txt\n",
        ">\n",
        "> test_pred_cnn_lstm_crf.txt (optional)\n",
        "\n",
        "\n",
        "**Please make sure your implementation meets the accuracy requirements to get full credit.**\n",
        "\n",
        "**Please make sure that you name the files as specified above. You will be able to see the test set accuracy for your predictions on leaderboard. However, the final score will be assigned later based on accuracy in the notebook / PDF and implementation.**\n",
        "\n",
        "You can submit multiple times before the deadline and choose the submission which you want to be graded by going to `Submission History` on gradescope.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}