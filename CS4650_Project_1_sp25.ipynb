{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daiwikpal/NLP/blob/main/CS4650_Project_1_sp25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnnwupedFEIV"
      },
      "source": [
        "# Headline Classification with Neural Bag of Words\n",
        "**CS 4650 \"Natural Language Processing\" Project 1**  \n",
        "Georgia Tech, Spring 2025\n",
        "\n",
        "(Instructor: Prof. Wei Xu; TAs: Yao Dou,  Tarek Naous, Xiaofeng Wu, Jonathan Zheng)\n",
        "\n",
        "Welcome to the first full programming project for CS 4650! **To start, first make a copy of this notebook to your local drive, so you can edit it.**\n",
        "\n",
        "If you want GPUs (which will improve training times), you can always change your instance type by going to Runtime -> Change runtime type -> Hardware accelerator.\n",
        "\n",
        "**In this project, we will be using PyTorch.** If you are new to PyTorch, or simply want a refresher, we recommend you start by looking through these [Introduction to PyTorch](https://cocoxu.github.io/CS4650_spring2025/slides/PyTorch_tutorial.pdf) slides and this interactive [PyTorch basics notebook](http://bit.ly/pytorchbasics). Additionally, this [text sentiment](http://bit.ly/pytorchexample) notebook will provide some insight into working with PyTorch with a specific NLP task."
      ],
      "id": "JnnwupedFEIV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Honor Code  [1 points]\n",
        "\n",
        "**Honor Code:** I hereby agree to abide the Georgia Tech's Academic Honor Code, promise that the submitted assignment is my own work, and understand that my code is subject to plagiarism test.\n",
        "\n",
        "**Signature**: Daiwik Pal"
      ],
      "metadata": {
        "id": "BMgET_OMuuNZ"
      },
      "id": "BMgET_OMuuNZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRdPqeMMFEIY"
      },
      "source": [
        "## 1. Load and preprocess data [9 points]\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n",
        "\n",
        "This project will be modeling a *classification task* for headlines from [The Onion](https://www.theonion.com), a satirical news website. Our dataset contains headlines and whether they belong to The Onion or CNN. Given a headline, we want to predict whether it is Onion or not.\n",
        "\n",
        "The following cell loads, pre-processes and tokenizes our OnionOrNot dataset."
      ],
      "id": "ZRdPqeMMFEIY"
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "YmV_uknBJA-o"
      },
      "outputs": [],
      "source": [
        "!curl -so OnionOrNot.csv https://raw.githubusercontent.com/lukefeilberg/onion/master/OnionOrNot.csv"
      ],
      "id": "YmV_uknBJA-o"
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "L3DkMDu7FEIZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7549f65-b713-4666-8a00-17b9d8d8e8ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n"
          ]
        }
      ],
      "source": [
        "# ===========================================================================\n",
        "# Run some setup code for this notebook. Don't modify anything in this cell.\n",
        "# ===========================================================================\n",
        "\n",
        "import torch\n",
        "import random, sys\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "\n",
        "# ===========================================================================\n",
        "# A quick note on CUDA functionality (and `.to(model.device)`):\n",
        "# CUDA is a parallel GPU platform produced by NVIDIA and is used by most GPU\n",
        "# libraries in PyTorch. CUDA organizes GPUs into device IDs (i.e., \"cuda:X\" for GPU #X).\n",
        "# \"device\" will tell PyTorch which GPU (or CPU) to place an object in. Since\n",
        "# collab only uses one GPU, we will use 'cuda' as the device if a GPU is available\n",
        "# and the CPU if not. You will run into problems if your tensors are on different devices.\n",
        "# ===========================================================================\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Check what version of Python is running\n",
        "print(sys.version)"
      ],
      "id": "L3DkMDu7FEIZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Fulh0MZ8y8b"
      },
      "source": [
        "### 1.1 Dataset preprocessing functions\n",
        "The following cell define some methods to clean the dataset, but feel free to take a look to see some of the operations it's doing.\n"
      ],
      "id": "0Fulh0MZ8y8b"
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "ctNnE1Ui8oKw"
      },
      "outputs": [],
      "source": [
        "# ===========================================================================\n",
        "# Run some preprocessing code for our dataset. Don't modify anything in this cell.\n",
        "# This code was adapted from fast-bert.\n",
        "# ===========================================================================\n",
        "\n",
        "import re\n",
        "import html\n",
        "\n",
        "def spec_add_spaces(t: str) -> str:\n",
        "    \"Add spaces around / and # in `t`. \\n\"\n",
        "    return re.sub(r\"([/#\\n])\", r\" \\1 \", t)\n",
        "\n",
        "def rm_useless_spaces(t: str) -> str:\n",
        "    \"Remove multiple spaces in `t`.\"\n",
        "    return re.sub(\" {2,}\", \" \", t)\n",
        "\n",
        "def replace_multi_newline(t: str) -> str:\n",
        "    return re.sub(r\"(\\n(\\s)*){2,}\", \"\\n\", t)\n",
        "\n",
        "def fix_html(x: str) -> str:\n",
        "    \"List of replacements from html strings in `x`.\"\n",
        "    re1 = re.compile(r\"  +\")\n",
        "    x = (\n",
        "        x.replace(\"#39;\", \"'\")\n",
        "        .replace(\"amp;\", \"&\")\n",
        "        .replace(\"#146;\", \"'\")\n",
        "        .replace(\"nbsp;\", \" \")\n",
        "        .replace(\"#36;\", \"$\")\n",
        "        .replace(\"\\\\n\", \"\\n\")\n",
        "        .replace(\"quot;\", \"'\")\n",
        "        .replace(\"<br />\", \"\\n\")\n",
        "        .replace('\\\\\"', '\"')\n",
        "        .replace(\" @.@ \", \".\")\n",
        "        .replace(\" @-@ \", \"-\")\n",
        "        .replace(\" @,@ \", \",\")\n",
        "        .replace(\"\\\\\", \" \\\\ \")\n",
        "    )\n",
        "    return re1.sub(\" \", html.unescape(x))\n",
        "\n",
        "def clean_text(input_text):\n",
        "    text = fix_html(input_text)\n",
        "    text = replace_multi_newline(text)\n",
        "    text = spec_add_spaces(text)\n",
        "    text = rm_useless_spaces(text)\n",
        "    text = text.strip()\n",
        "    return text"
      ],
      "id": "ctNnE1Ui8oKw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiUlTSBB9Wx6"
      },
      "source": [
        "### 1.2 Tokenize using NLTK\n",
        "\n",
        "We will use our rule-based `clean_text` function to clean our raw text, then use the popular NLTK [punkt tokenizer](https://www.nltk.org/_modules/nltk/tokenize/punkt.html) to convert text to individual sub-words. This will take a while because you have to download the pre-trained punkt tokenizer.\n",
        "\n",
        "*If you are interested: There's a [long and diverse history of converting raw text to \"tokens\"](https://arxiv.org/abs/2112.10508), and many available methods/algorithms (you can experiment with some recently trained ones, trained on a dynamic programming-based method called BPE, [here](https://huggingface.co/spaces/Xenova/the-tokenizer-playground)).*"
      ],
      "id": "MiUlTSBB9Wx6"
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "vqtdrhF8FEIZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20cab424-e77c-4fbc-b8fa-588a030d8e36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# ===========================================================================\n",
        "# Tokenize using punkt. Don't modify anything in this cell.\n",
        "# ===========================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from tqdm import tqdm\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "df = pd.read_csv(\"OnionOrNot.csv\")\n",
        "df[\"tokenized\"] = df[\"text\"].apply(lambda x: nltk.word_tokenize(clean_text(x.lower())))"
      ],
      "id": "vqtdrhF8FEIZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBBdVOYxFEIa"
      },
      "source": [
        "We will use `pandas`, a popular library for data analysis and table manipulation, in this project to manage the dataset. For more information on usage, please refer to the [Pandas documentation](https://pandas.pydata.org/docs/).\n",
        "\n",
        "The primary data structure in Pandas is a `DataFrame`. The following cell will print out the basic information contained in our `DataFrame` structure, and the first few rows of our dataset."
      ],
      "id": "qBBdVOYxFEIa"
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "sJjScqV3FEIb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "061493d4-1b6e-43d6-daf7-86cf6f14524b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  label  \\\n",
              "0  Entire Facebook Staff Laughs As Man Tightens P...      1   \n",
              "1  Muslim Woman Denied Soda Can for Fear She Coul...      0   \n",
              "2  Bold Move: Hulu Has Announced That They’re Gon...      1   \n",
              "3  Despondent Jeff Bezos Realizes He’ll Have To W...      1   \n",
              "4  For men looking for great single women, online...      1   \n",
              "\n",
              "                                           tokenized  \n",
              "0  [entire, facebook, staff, laughs, as, man, tig...  \n",
              "1  [muslim, woman, denied, soda, can, for, fear, ...  \n",
              "2  [bold, move, :, hulu, has, announced, that, th...  \n",
              "3  [despondent, jeff, bezos, realizes, he, ’, ll,...  \n",
              "4  [for, men, looking, for, great, single, women,...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2a028e94-f4dc-4278-9944-0f7f09bcb691\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>tokenized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Entire Facebook Staff Laughs As Man Tightens P...</td>\n",
              "      <td>1</td>\n",
              "      <td>[entire, facebook, staff, laughs, as, man, tig...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Muslim Woman Denied Soda Can for Fear She Coul...</td>\n",
              "      <td>0</td>\n",
              "      <td>[muslim, woman, denied, soda, can, for, fear, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Bold Move: Hulu Has Announced That They’re Gon...</td>\n",
              "      <td>1</td>\n",
              "      <td>[bold, move, :, hulu, has, announced, that, th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Despondent Jeff Bezos Realizes He’ll Have To W...</td>\n",
              "      <td>1</td>\n",
              "      <td>[despondent, jeff, bezos, realizes, he, ’, ll,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>For men looking for great single women, online...</td>\n",
              "      <td>1</td>\n",
              "      <td>[for, men, looking, for, great, single, women,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2a028e94-f4dc-4278-9944-0f7f09bcb691')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2a028e94-f4dc-4278-9944-0f7f09bcb691 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2a028e94-f4dc-4278-9944-0f7f09bcb691');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e106825d-4aec-4780-90fb-cdf13b192f01\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e106825d-4aec-4780-90fb-cdf13b192f01')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e106825d-4aec-4780-90fb-cdf13b192f01 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 24000,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 23998,\n        \"samples\": [\n          \"Freeloading Refugee Children Taking Up Thousands Of Prison Cells Meant For Real Americans\",\n          \"Life: Finally Bought A Jackhammer? Don\\u2019t Make These 6 Rookie Mistakes\",\n          \"Apple 1 computer worth $200K left at recycling centre\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tokenized\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 141
        }
      ],
      "source": [
        "# View the first few entries of our dataset\n",
        "df.head()"
      ],
      "id": "sJjScqV3FEIb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9b4W9z1XhgS"
      },
      "source": [
        "Try to guess some examples! Is the task more difficult than you expected?\n",
        "\n",
        "DataFrames can be indexed using [`.iloc[]`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html). `iloc` uses interger based indexing and supports a single integer (`df.iloc[42]`), a list of integers (`df.iloc[[1, 5, 42]]`), or a slice (`df.iloc[7:42]`)."
      ],
      "id": "D9b4W9z1XhgS"
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "Ntm8laX6FEIb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "f37a4f16-a7c1-41e0-a662-564137311f7c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "text         Customers continued to wait at drive-thru even...\n",
              "label                                                        0\n",
              "tokenized    [customers, continued, to, wait, at, drive-thr...\n",
              "Name: 42, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>42</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>text</th>\n",
              "      <td>Customers continued to wait at drive-thru even...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>label</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tokenized</th>\n",
              "      <td>[customers, continued, to, wait, at, drive-thr...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ],
      "source": [
        "# E.g., get row 42 of our dataset\n",
        "df.iloc[42]"
      ],
      "id": "Ntm8laX6FEIb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQVT6HUA9htQ"
      },
      "source": [
        "### 1.3 Split the dataset into training, validation, and testing"
      ],
      "id": "TQVT6HUA9htQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDI72x8XFEIc"
      },
      "source": [
        "**Train/Test/Val Split** - Now that we've loaded this dataset, we need to split the data into train, validation, and test sets.\n",
        "\n",
        "A good explanation of why we need these different sets can be found in $\\S$2.2.5 of [Eisenstein](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf) but our high-level goal is to have a generalized model and have confidence in our results.\n",
        "\n",
        "\n",
        "The *training set* is used to fit our model's learned parameters (weights and biases) to the task. The *validation  set* (sometimes called development set) is used to verify our training jobs are minimizing loss on an unseen subset of the data and can also be used to help choose hyperparameters for our training setup. The *test set* is used to provide a final evaluation of our trained model (unbiased by development or training decisions), ideally providing some insight into how the model will perform in a scenario we cannot perfectly represent in our data (i.e., the real world). *Each of these sets should be disjoint from the others*, to prevent any leackage that could introduce bias in our evaluation metrics (in this case accuracy).\n",
        "\n",
        "**Model Vocabulary** - We cannot directly feed sub-word token strings into a model! We need to create a \"vocab map\", which contains an ID for each unique token in our Onion dataset. This will be used as a \"lookup\" in the next few sections, since your PyTorch implementation will require first converting your Onion token representations to a list of sub-word IDs.\n",
        "\n",
        "**In the following cell, please implement `split_train_val_test` and `generate_vocab_map`.**"
      ],
      "id": "GDI72x8XFEIc"
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "zeo9kX6i9pbH"
      },
      "outputs": [],
      "source": [
        "# ===========================================================================\n",
        "# Set constants for PAD and UNK. You will use these values, but DO NOT change\n",
        "# them, or import additional packages.\n",
        "\n",
        "from collections import Counter\n",
        "PADDING_VALUE = 0\n",
        "UNK_VALUE     = 1\n",
        "\n",
        "# ===========================================================================\n",
        "\n",
        "\n",
        "def split_train_val_test(df, props=[.8, .1, .1]):\n",
        "    \"\"\"\n",
        "    This method takes a dataframe and splits it into train/val/test splits.\n",
        "    It uses the props argument to split the dataset appropriately.\n",
        "\n",
        "    Args:\n",
        "      df (pd.DataFrame): A dataset as a Pandas DataFrame\n",
        "      props (list): Proportions for each split in the order of [train, validation, test].\n",
        "                    the last value of the props array is repetitive, but we've kept it for clarity.\n",
        "\n",
        "    Returns:\n",
        "      train_df (pd.DataFrame): Train DataFrame split.\n",
        "      val_df (pd.DataFrame): Validation DataFrame split.\n",
        "      test_df (pd.DataFrame): Test DataFramem split.\n",
        "    \"\"\"\n",
        "    assert round(sum(props), 2) == 1 and len(props) >= 2\n",
        "    train_df, test_df, val_df = None, None, None\n",
        "\n",
        "    ### BEGIN YOUR CODE (~3-5 lines) ###\n",
        "    ### Hint: You can use df.iloc to slice into specific indexes or ranges.\n",
        "\n",
        "    # Get length of dataframe\n",
        "    df_len = len(df)\n",
        "\n",
        "    # Get end location for training and validation (don't need test since that's the rest of the dataframe after val)\n",
        "    train_end = int(df_len * props[0])\n",
        "    val_end = int(df_len * (props[0] + props[1]))\n",
        "\n",
        "    # Split the dataframe based on the stored locations\n",
        "    train_df = df.iloc[:train_end]\n",
        "    val_df = df.iloc[train_end:val_end]\n",
        "    test_df = df.iloc[val_end:]\n",
        "\n",
        "    ### END YOUR CODE ###\n",
        "\n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "\n",
        "def generate_vocab_map(df, cutoff=2):\n",
        "    \"\"\"\n",
        "    This method takes a dataframe and builds a vocabulary to unique number map.\n",
        "    It uses the cutoff argument to remove rare words occuring <= cutoff times.\n",
        "    *NOTE*: \"\" and \"UNK\" are reserved tokens in our vocab that will be useful\n",
        "    later. You'll also find the Counter imported for you to be useful as well.\n",
        "\n",
        "    Args:\n",
        "      df (pd.DataFrame): The entire dataset this mapping is built from\n",
        "      cutoff (int): We exclude words from the vocab that appear less than or\n",
        "                    eq to cutoff\n",
        "\n",
        "    Returns:\n",
        "      vocab (dict[str, int]):\n",
        "        In vocab, each str is a unique token, and each dict[str] is a\n",
        "        unique integer ID. Only elements that appear > cutoff times appear\n",
        "        in vocab.\n",
        "\n",
        "      reversed_vocab (dict[int, str]):\n",
        "        A reversed version of vocab, which allows us to retrieve\n",
        "        words given their unique integer ID. This map will\n",
        "        allow us to \"decode\" integer sequences we'll encode using\n",
        "        vocab!\n",
        "    \"\"\"\n",
        "\n",
        "    vocab          = {\"\": PADDING_VALUE, \"UNK\": UNK_VALUE}\n",
        "    reversed_vocab = {}\n",
        "\n",
        "    ### BEGIN YOUR CODE (~5-15 lines) ###\n",
        "    ### Hint: Start by iterating over df[\"tokenized\"]\n",
        "\n",
        "    allTokens = []\n",
        "\n",
        "    # Make a dictionary of words and their counts\n",
        "    for tokens in df[\"tokenized\"]:\n",
        "      for token in tokens:\n",
        "        allTokens.append(token)\n",
        "\n",
        "    tokenCounter = Counter(allTokens)\n",
        "\n",
        "    # Start iterator for unique word IDs\n",
        "    idCounter = 2\n",
        "\n",
        "    # For each word in the dictionary, if it meets the minimum frequency add it and increase the word ID counter\n",
        "    for token in tokenCounter:\n",
        "      if tokenCounter[token] > cutoff:\n",
        "        vocab[token] = idCounter\n",
        "        idCounter += 1\n",
        "\n",
        "    # Create a new dictionary of the flipped values\n",
        "\n",
        "    for token, wordID in vocab.items():\n",
        "       reversed_vocab[wordID] = token\n",
        "\n",
        "    ### END YOUR CODE ###\n",
        "\n",
        "    return vocab, reversed_vocab"
      ],
      "id": "zeo9kX6i9pbH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9LEk83hRFgT"
      },
      "source": [
        "With the methods you have implemented above, we can now split the dataset into training, validation, and testing sets and generate our dictionaries mapping from word tokens to IDs (and vice versa).\n",
        "\n",
        "*Note: The props list currently being used splits the dataset so that 80% of samples are used to train, and the remaining 20% are evenly split between training and validation. How you split your dataset is itself a major choice and something you would need to consider in your own projects. Can you think of why?*"
      ],
      "id": "w9LEk83hRFgT"
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "rcmX931OFEId"
      },
      "outputs": [],
      "source": [
        "df                         = df.sample(frac=1)\n",
        "train_df, val_df, test_df  = split_train_val_test(df, props=[.8, .1, .1])\n",
        "train_vocab, reverse_vocab = generate_vocab_map(train_df)"
      ],
      "id": "rcmX931OFEId"
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "CAACzA8YFEId",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e36daeff-46a1-4b09-c322-4e8d905e914a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8, 0.1, 0.1)"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ],
      "source": [
        "# ===========================================================================\n",
        "# This line of code will help test your implementation, the expected output is\n",
        "# the same distribution used in 'props' in the above cell. Try out some\n",
        "# different values to ensure it works, but for submission ensure you use\n",
        "# [.8, .1, .1]\n",
        "# ===========================================================================\n",
        "\n",
        "(len(train_df) / len(df)), (len(val_df) / len(df)), (len(test_df) / len(df))"
      ],
      "id": "CAACzA8YFEId"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fCFfEHv1hnI"
      },
      "source": [
        "### 1.4 Building a Dataset Class"
      ],
      "id": "5fCFfEHv1hnI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-qTQQa2FEIe"
      },
      "source": [
        "PyTorch has custom Dataset Classes that have very useful extentions, we want to turn our current pandas DataFrame into a subclass of Dataset so that we can iterate and sample through it for minibatch updates. **In the following cell, fill out the `HeadlineDataset` class.** Refer to PyTorch documentation on [Dataset Classes](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n",
        "for help."
      ],
      "id": "8-qTQQa2FEIe"
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "tqt9q92J1QKK"
      },
      "outputs": [],
      "source": [
        "# ===========================================================================\n",
        "# Please do not change, or import additional packages.\n",
        "from torch.utils.data import Dataset\n",
        "# ===========================================================================\n",
        "\n",
        "class HeadlineDataset(Dataset):\n",
        "  \"\"\"\n",
        "  This class takes a Pandas DataFrame and wraps in a PyTorch Dataset.\n",
        "  Read more about Torch Datasets here:\n",
        "  https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, vocab, df, max_length=50):\n",
        "    \"\"\"\n",
        "    Initialize this class with appropriate instance variables\n",
        "\n",
        "    We would *strongly* recommend storing the dataframe itself as an instance\n",
        "    variable, and keeping this method very simple. Leave processing to\n",
        "    __getitem__.\n",
        "\n",
        "    Sometimes, however, it does make sense to preprocess in __init__. If you\n",
        "    are curious as to why, read the aside at the bottom of this cell.\n",
        "    \"\"\"\n",
        "\n",
        "    ### BEGIN YOUR CODE (~3 lines) ###\\\n",
        "    self.vocab = vocab\n",
        "    self.df  = df\n",
        "    self.MAX_LEN = max_length\n",
        "\n",
        "    ### END YOUR CODE ###\n",
        "\n",
        "  def __len__(self):\n",
        "    \"\"\"\n",
        "    Return the length of the dataframe instance variable\n",
        "    \"\"\"\n",
        "    df_len = self.df.shape[0]\n",
        "\n",
        "    ### BEGIN YOUR CODE (~1 line) ###\n",
        "\n",
        "    ### END YOUR CODE ###\n",
        "\n",
        "    return df_len\n",
        "\n",
        "  def __getitem__(self, index: int):\n",
        "    \"\"\"\n",
        "    Converts a dataframe row (row[\"tokenized\"]) to an encoded torch LongTensor,\n",
        "    using our vocab map created using generate_vocab_map. Restricts the encoded\n",
        "    headline length to max_length.\n",
        "\n",
        "    The purpose of this method is to convert the row - a list of words - into\n",
        "    a corresponding list of numbers.\n",
        "\n",
        "    i.e. using a map of {\"hi\": 2, \"hello\": 3, \"UNK\": 0}\n",
        "    this list [\"hi\", \"hello\", \"NOT_IN_DICT\"] will turn into [2, 3, 0]\n",
        "\n",
        "    Returns:\n",
        "      tokenized_word_tensor (torch.LongTensor):\n",
        "        A 1D tensor of type Long, that has each token in the dataframe mapped to\n",
        "        a number. These numbers are retrieved from the vocab_map we created in\n",
        "        generate_vocab_map.\n",
        "\n",
        "        **IMPORTANT**: if we filtered out the word because it's infrequent (and\n",
        "        it doesn't exist in the vocab) we need to replace it w/ the UNK token.\n",
        "\n",
        "      curr_label (int):\n",
        "        Binary 0/1 label retrieved from the DataFrame.\n",
        "\n",
        "    \"\"\"\n",
        "    tokenized_word_tensor = None\n",
        "    curr_label            = None\n",
        "\n",
        "    ### BEGIN YOUR CODE (~3-7 lines) ###\n",
        "\n",
        "    # Get the first MAX_LENGTH words in the headline, if it doesn't exist in the vocab replace it with UNK\n",
        "    row = self.df.iloc[index]\n",
        "    headline_array = row[\"tokenized\"][:self.MAX_LEN]\n",
        "\n",
        "    for i, word in enumerate(headline_array):\n",
        "      if word not in self.vocab:\n",
        "        headline_array[i] = \"UNK\"\n",
        "\n",
        "\n",
        "    # Get the ID of each word (or UNK) in the cleaned list for the headline\n",
        "    tokens_array = []\n",
        "    for word in headline_array:\n",
        "      tokens_array.append(self.vocab[word])\n",
        "\n",
        "    tokenized_word_tensor = torch.tensor(tokens_array, dtype=torch.long)\n",
        "\n",
        "    # Grab the label for the current index\n",
        "    curr_label = int(row[\"label\"])\n",
        "\n",
        "    ### END YOUR CODE ###\n",
        "\n",
        "    return tokenized_word_tensor, curr_label\n",
        "\n",
        "\n",
        "# ===========================================================================\n",
        "# Completely optional aside on preprocessing in __init__.\n",
        "#\n",
        "# Sometimes the compute bottleneck actually ends up being in __getitem__.\n",
        "# In this case, you'd loop over your dataset in __init__, passing data\n",
        "# to __getitem__ and storing it in another instance variable. Then,\n",
        "# you can simply return the preprocessed data in __getitem__ instead of\n",
        "# doing the preprocessing.\n",
        "#\n",
        "# There is a tradeoff though: can you think of one?\n",
        "# ==========================================================================="
      ],
      "id": "tqt9q92J1QKK"
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "KuLtIOAZFEIe"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import RandomSampler\n",
        "\n",
        "train_dataset = HeadlineDataset(train_vocab, train_df)\n",
        "val_dataset   = HeadlineDataset(train_vocab, val_df)\n",
        "test_dataset  = HeadlineDataset(train_vocab, test_df)\n",
        "\n",
        "\n",
        "# Now that we're wrapping our dataframes in PyTorch datsets, we can make use of\n",
        "# PyTorch Random Samplers, they'll define how our DataLoaders sample elements\n",
        "# from the HeadlineDatasets\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "val_sampler   = RandomSampler(val_dataset)\n",
        "test_sampler  = RandomSampler(test_dataset)"
      ],
      "id": "KuLtIOAZFEIe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9iBiSKF1yXA"
      },
      "source": [
        "### 1.5 Finalizing our DataLoader"
      ],
      "id": "n9iBiSKF1yXA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfXSbxoFFEIe"
      },
      "source": [
        "We can now use PyTorch `DataLoader` to batch our data for us. **In the following cell, please implement `collate_fn`.** Refer to PyTorch documentation on [`DataLoader`](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) for help."
      ],
      "id": "lfXSbxoFFEIe"
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "Zp1aQAvn1_mz"
      },
      "outputs": [],
      "source": [
        "# ===========================================================================\n",
        "# Please do not change, or import additional packages.\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "# ===========================================================================\n",
        "\n",
        "def collate_fn(batch, padding_value=PADDING_VALUE):\n",
        "  \"\"\"\n",
        "  This function is passed as a parameter to Torch DataSampler. collate_fn collects\n",
        "  batched rows, in the form of tuples, from a DataLoader and applies some final\n",
        "  pre-processing.\n",
        "\n",
        "  Objective:\n",
        "    In our case, we need to take the batched input array of 1D tokenized_word_tensors,\n",
        "    and create a 2D tensor that's padded to be the max length from all our tokenized_word_tensors\n",
        "    in a batch. We're moving from a Python array of tuples, to a padded 2D tensor.\n",
        "\n",
        "    *HINT*: you're allowed to use torch.nn.utils.rnn.pad_sequence (ALREADY IMPORTED)\n",
        "\n",
        "    Finally, you can read more about collate_fn here: https://pytorch.org/docs/stable/data.html\n",
        "\n",
        "  Args:\n",
        "    batch: PythonArray[tuple(tokenized_word_tensor: 1D Torch.LongTensor, curr_label: int)]\n",
        "           len(batch) == BATCH_SIZE\n",
        "\n",
        "  Returns:\n",
        "    padded_tokens: 2D LongTensor of shape (BATCH_SIZE, max len of all tokenized_word_tensor))\n",
        "    y_labels: 1D FloatTensor of shape (BATCH_SIZE)\n",
        "\n",
        "  \"\"\"\n",
        "  padded_tokens, y_labels = None, None\n",
        "\n",
        "  ### BEGIN YOUR CODE (~4-8 lines) ###\n",
        "\n",
        "  # Pad each sequence with the given value to the longest in the batch\n",
        "  sequences = [row[0] for row in batch]\n",
        "  padded_tokens = pad_sequence(sequences, batch_first=True, padding_value=padding_value)\n",
        "  padded_tokens.transpose(1, 0)\n",
        "  # Create a tensor of labels for the batch, ground truth\n",
        "  y_labels = [row[1] for row in batch]\n",
        "  y_labels = torch.tensor(y_labels, dtype=torch.float)\n",
        "\n",
        "  ### END YOUR CODE ###\n",
        "\n",
        "  return padded_tokens, y_labels"
      ],
      "id": "Zp1aQAvn1_mz"
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "OayoJRTeFEIf"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, collate_fn=collate_fn)\n",
        "val_iterator   = DataLoader(val_dataset, batch_size=BATCH_SIZE, sampler=val_sampler, collate_fn=collate_fn)\n",
        "test_iterator  = DataLoader(test_dataset, batch_size=BATCH_SIZE, sampler=test_sampler, collate_fn=collate_fn)"
      ],
      "id": "OayoJRTeFEIf"
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "pidbg12AFEIf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b53850c9-39a8-4818-dbb5-d6275b7ce2e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1932, 3154, 3211,   67, 4335,    1,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0],\n",
            "        [4788, 1952, 1245,   10,  174,  152,    1,    1,   51, 1569,   23, 4529,\n",
            "          152, 6863,    1, 3513,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0],\n",
            "        [ 364,    1, 3710, 3558, 4669,   29, 6114, 8025,    1, 8263, 5323,  204,\n",
            "         4762,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0],\n",
            "        [ 965, 7186, 2737, 2716,    1, 6643,   31,  110,    1,   10,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0],\n",
            "        [5892,    1, 2044,    1, 5363,   51, 1452,  196,    1, 7032,   23, 6406,\n",
            "          972,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0],\n",
            "        [3478, 5646,  716,  741, 1687,  101, 2134,    2,  148,   23,  896, 3778,\n",
            "          329,   51,   71,  286,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0],\n",
            "        [1882, 4430,  110,    1,   10, 3815, 8702,  713,  421,    1,   31, 7657,\n",
            "          568,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0],\n",
            "        [3564, 8240,  177,  110,   87, 7413,   10, 8045,   40, 1246, 1355, 2018,\n",
            "            8,  953,  101, 6140, 2787,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0],\n",
            "        [ 364, 5108,   37,  101,    1,   23, 5446,    1,   18, 1187,  353,   51,\n",
            "         1068,  973,    1,    1,    1,   23,  259, 4980,    1,   87,  483,  593,\n",
            "         1064,  204, 1710,  330],\n",
            "        [  66, 2716, 2275,  364, 6917,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0],\n",
            "        [   1, 1625, 4118, 3414,   23, 1478,   31, 2960,   87, 1549,  212,    1,\n",
            "          148,   51, 1211, 4310, 1200,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0],\n",
            "        [   1, 5452,  636,  212, 6289,    1,  285, 4027,   62,  915, 8082,  148,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0],\n",
            "        [6370,  954,  286,  652,   97, 7850,  403,   31, 2088, 2040, 1945,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0],\n",
            "        [  52,  113, 4573,  257, 3591,  276,   23,  666,  379, 3696,  764,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0],\n",
            "        [   7,    3,  152, 1150,   23, 1909,  272, 7888,  141,  331,   47,  122,\n",
            "         4433,   18,   91,    1,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0],\n",
            "        [1126, 2782, 1306,  136,    1,   29,  101,  373,   31,  101, 1113,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0]]) tensor([1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0.])\n",
            "x: torch.Size([16, 28])\n",
            "y: torch.Size([16])\n"
          ]
        }
      ],
      "source": [
        "# ===========================================================================\n",
        "# Use this to test your collate_fn implementation.\n",
        "#\n",
        "# You can look at the shapes of x and y or put print statements in collate_fn\n",
        "# while running this snippet\n",
        "# ===========================================================================\n",
        "\n",
        "for x, y in test_iterator:\n",
        "    print(x, y)\n",
        "    print(f'x: {x.shape}')\n",
        "    print(f'y: {y.shape}')\n",
        "    break\n",
        "test_iterator = DataLoader(test_dataset, batch_size=BATCH_SIZE, sampler=test_sampler, collate_fn=collate_fn)"
      ],
      "id": "pidbg12AFEIf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWLK7T1uFEIg"
      },
      "source": [
        "## 2. Modeling [10 points]\n",
        "Now that we have a clean dataset and a useful PyTorch `DataLoader` object, we can begin building a model for our task! In the following code block, you will build a feed-forward neural network implementing a neural bag-of-words baseline, `NBOW-RAND`, described in $\\S$2.1 of [this paper](https://www.aclweb.org/anthology/P15-1162.pdf). You may find [the PyTorch `torch.nn` docs](https://pytorch.org/docs/stable/nn.html) useful for understanding the different layers and [this PyTorch sequence models tutorial](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html) for how to put together `torch.nn` layers.\n",
        "\n",
        "The core intuition behind `NBOW-RAND` is that after we embed each word for our input, we average the embeddings to produce a single vector that hopefully averages the information across all embeddings. Formally, we first convert each document of length $n$ tokens into a matrix of $n\\times d$, where $d$ is the dimension of the token embedding. Then we average all embeddings to produce a vector of length $d$.\n",
        "\n",
        "If you are new to PyTorch, ensuring your matrix operations are correct is often the most common source of errors. Keep in mind how the dimensions change and what each axes represents. Your documents will be passed in as minibatches, so be careful when selecting which axes to apply certain operations. Feel free to experiment with the architecture of this network outside of the basic `NBOW-RAND` setup (such as adding in other layers) to see how this changes your results."
      ],
      "id": "BWLK7T1uFEIg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZDPs0Sf-H3V"
      },
      "source": [
        "### 2.1 Define the NBOW model class"
      ],
      "id": "pZDPs0Sf-H3V"
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "jzGx2q0jLqyU"
      },
      "outputs": [],
      "source": [
        "# ===========================================================================\n",
        "# Please do not change, or import additional packages.\n",
        "import torch.nn as nn\n",
        "# ===========================================================================\n",
        "\n",
        "class NBOW(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim):\n",
        "    \"\"\"\n",
        "    Instantiate layers for your model.\n",
        "    Your model architecture will be a feed-forward neural network.\n",
        "\n",
        "    You will need 3 nn.Modules at minimum\n",
        "     1. An embeddings layer (see nn.Embedding)\n",
        "     2. A linear layer (see nn.Linear)\n",
        "     3. A sigmoid output (see nn.Sigmoid)\n",
        "\n",
        "    HINT: In the forward step, the BATCH_SIZE is the first dimension.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    ### BEGIN YOUR CODE (~4 lines) ###\n",
        "\n",
        "    # Initialize embedding layer, turn word ids into representative vectors\n",
        "    self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    # Initilize linear layer, will receive a single vector of length embedding_dim\n",
        "    self.linear = nn.Linear(embedding_dim, 1)\n",
        "\n",
        "    # Intialize sigmoid layer, turn output from linear layer into a probabilistic(ish) score\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    ### END YOUR CODE ###\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Complete the forward pass of the model.\n",
        "\n",
        "    Use the output of the embedding layer to create the average vector,\n",
        "    which will be input into the linear layer.\n",
        "\n",
        "    Args:\n",
        "      x: 2D LongTensor of shape (BATCH_SIZE, max len of all tokenized_word_tensor))\n",
        "         This is the same output that comes out of the collate_fn function you completed\n",
        "    \"\"\"\n",
        "    ### BEGIN YOUR CODE (~4-5 lines) ###\n",
        "\n",
        "    # Embed each document, this produces a tensor of (minibatch, max doc len, embedding size)\n",
        "    embed = self.word_embeddings(x)\n",
        "\n",
        "    # Average over the second dimension, producing a tensor of size (minibatch, embedding size)\n",
        "    average_embeds = torch.mean(embed, dim=1)\n",
        "    # Pass average vector through linear layer\n",
        "    out = self.linear(average_embeds)\n",
        "    out = self.sigmoid(out)\n",
        "\n",
        "    # Get a probabilistic score from sigmoid layer and return scores\n",
        "    return out.squeeze(1)\n",
        "    ### END YOUR CODE ###\n"
      ],
      "id": "jzGx2q0jLqyU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xltosIzM-SP2"
      },
      "source": [
        "### 2.2 Initialize the NBOW classification model\n",
        "\n",
        "Since the NBOW model is rather basic, there is only one meaningful hyperparameter w.r.t. model architecture: the size of the embedding dimension (`embedding_dim`). (We also see a `vocab_size` parameter here, but this only a by-product on our cutoff for infrequent tokens, there also may more hyperparameters if you modified the architecture, such as adding a linear layer). Adjust the embedding dimension below when you start training your model. How big should your embedding dimension be? Recall that the embedding is a way to \"condense\" the sparsely populated tokenized vocabulary, so we suggest starting with values that are much lower than the vocab size.\n",
        "\n",
        "Remember the CUDA discussion in the first cell of this notebook? Here the `.to(device)` is where that discussion becomes relevant (if `device=='cuda'`, PyTorch will perform the matrix operations on GPU). If you recieve a mismatch error, your tensors may be on different devices."
      ],
      "id": "xltosIzM-SP2"
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "_HQWUu-ZFEIg"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_DIM = 10 # adjust here\n",
        "\n",
        "model = NBOW(\n",
        "  vocab_size    = len(train_vocab.keys()),\n",
        "  embedding_dim = EMBEDDING_DIM\n",
        ").to(device)"
      ],
      "id": "_HQWUu-ZFEIg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4CZnj1f-da-"
      },
      "source": [
        "### 2.3 Instantiate the loss function and optimizer"
      ],
      "id": "C4CZnj1f-da-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aXi8nA0FEIh"
      },
      "source": [
        "Please select and instantiate an appropriate loss function and optimizer.\n",
        "\n",
        "*Hint: What loss functions are availible for binary classification? Feel free to look at the [torch.nn docs on loss functions](https://pytorch.org/docs/stable/nn.html#loss-functions) for help!*"
      ],
      "id": "9aXi8nA0FEIh"
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "w98UvlXxFEIh"
      },
      "outputs": [],
      "source": [
        "# While we import Adam for you, you may try / import other optimizers as well\n",
        "from torch.optim import Adam\n",
        "\n",
        "\n",
        "criterion, optimizer = nn.BCELoss(), Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "### BEGIN YOUR CODE ###\n",
        "\n",
        "### END YOUR CODE ###"
      ],
      "id": "w98UvlXxFEIh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUXBtqPEjiRe"
      },
      "source": [
        "Now that we have a NBOW model, a loss function, optimizer and dataset, we can begin training!"
      ],
      "id": "hUXBtqPEjiRe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVLeTa8wFEIh"
      },
      "source": [
        "## 3. Training and Evaluation [10 points]\n",
        "We will now instantiate a `train_loop`, and a `val_loop` to evaluate our model at each epoch.\n",
        "\n",
        "**Fill out the train and test loops below. Treat real headlines as `False`, and Onion headlines as `True`.**"
      ],
      "id": "bVLeTa8wFEIh"
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "vganx5fCFEIh"
      },
      "outputs": [],
      "source": [
        "def train_loop(model, criterion, optim, iterator):\n",
        "  \"\"\"\n",
        "  Returns the total loss calculated from criterion\n",
        "  \"\"\"\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  for x, y in tqdm(iterator):\n",
        "    ### BEGIN YOUR CODE (~6 lines) ###\n",
        "\n",
        "    # Zero out the parameter gradients\n",
        "    optim.zero_grad()\n",
        "\n",
        "\n",
        "    # Do forward pass with current batch of input\n",
        "    classification_score = model(x)\n",
        "    # pred_y = (classification_score > 0.5).float()\n",
        "\n",
        "    # Get loss with model predictions and true labels\n",
        "    loss = criterion(classification_score, y)\n",
        "    # print(loss.shape)\n",
        "    loss.backward()\n",
        "    total_loss += loss.item()\n",
        "    optim.step()\n",
        "\n",
        "    ### END YOUR CODE ###\n",
        "\n",
        "  return total_loss\n",
        "\n",
        "\n",
        "def val_loop(model, iterator):\n",
        "  \"\"\"\n",
        "  Returns:\n",
        "    true (List[bool]): All the ground truth values taken from the dataset iterator\n",
        "    pred (List[bool]): All model predictions.\n",
        "  \"\"\"\n",
        "  true, pred = [], []\n",
        "\n",
        "  ### BEGIN YOUR CODE (~8 lines) ###\n",
        "\n",
        "  #put model into evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  # iterate over dataset and add predictions\n",
        "  for x, y in tqdm(iterator):\n",
        "    # print(\"input\", x)\n",
        "    classification_score = model(x)\n",
        "    pred_y = (classification_score > 0.5).float()\n",
        "\n",
        "    true.extend(y.tolist())\n",
        "    pred.extend(pred_y.tolist())\n",
        "\n",
        "  ### END YOUR CODE ###\n",
        "\n",
        "  return true, pred"
      ],
      "id": "vganx5fCFEIh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNXJevTu-tDZ"
      },
      "source": [
        "### 3.1 Define the evaluation metrics"
      ],
      "id": "JNXJevTu-tDZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IsZQs3rFEIi"
      },
      "source": [
        "We will also need evaluation metrics to tell us how well our model is doing on the validation set at each epoch and later how well the model does on the held-out test set. You may find $\\S$4.4.1 of Eisenstein useful for these questions.\n",
        "\n",
        "**Complete the functions in the following cell.**"
      ],
      "id": "7IsZQs3rFEIi"
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "gMQDg9Vy-wY0"
      },
      "outputs": [],
      "source": [
        "# Note: You will not need to import anything to implement these functions.\n",
        "\n",
        "def accuracy(true, pred):\n",
        "  \"\"\"\n",
        "  Calculate the ratio of correct predictions.\n",
        "\n",
        "  Args:\n",
        "    true (List[bool]): ground truth\n",
        "    pred (List[bool]): model predictions\n",
        "\n",
        "  Returns:\n",
        "    acc (float): percent accuracy with range [0, 1]\n",
        "  \"\"\"\n",
        "  acc = None\n",
        "  ### BEGIN YOUR CODE (~2-5 lines) ###\n",
        "  correct = 0\n",
        "  for i in range(len(true)):\n",
        "    if true[i] == pred[i]:\n",
        "      correct += 1\n",
        "  acc = correct / len(true)\n",
        "\n",
        "\n",
        "  ### END YOUR CODE ###\n",
        "  return acc\n",
        "\n",
        "\n",
        "def binary_f1(true, pred, selected_class=True):\n",
        "  \"\"\"\n",
        "  Calculate F-1 scores for a binary classification task.\n",
        "\n",
        "  Args:\n",
        "    true (List[bool]): ground truth\n",
        "    pred (List[bool]): model predictions\n",
        "    selected_class (bool): the selected class the F-1 is being calculated for.\n",
        "\n",
        "  Returns:\n",
        "    f1 (float): F-1 score between [0, 1]\n",
        "  \"\"\"\n",
        "  f1 = None\n",
        "  ### BEGIN YOUR CODE (~10-15 lines) ###\n",
        "\n",
        "  # iterate over truths and predictions to calculate true positive, false positive, etc.\n",
        "  true_positive = 0\n",
        "  true_negative = 0\n",
        "  false_positive = 0\n",
        "  false_negative = 0\n",
        "\n",
        "  for i in range(len(true)):\n",
        "    if true[i] == selected_class:\n",
        "      if true[i] == pred[i]:\n",
        "        true_positive += 1\n",
        "      else:\n",
        "        false_negative += 1\n",
        "    else:\n",
        "      if true[i] == pred[i]:\n",
        "        true_negative += 1\n",
        "      else:\n",
        "        false_positive += 1\n",
        "  # Calculate precision and recall\n",
        "  precision = true_positive / (true_positive + false_positive)\n",
        "  recall = true_positive / (true_positive + false_negative)\n",
        "\n",
        "  # Calculate F1 score\n",
        "  f1 = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "  ### END YOUR CODE ###\n",
        "  return f1\n",
        "\n",
        "\n",
        "def binary_macro_f1(true, pred):\n",
        "  \"\"\"\n",
        "  Calculate averaged F-1 for all selected (true/false) classes.\n",
        "\n",
        "  Args:\n",
        "    true (List[bool]): ground truth\n",
        "    pred (List[bool]): model predictions\n",
        "  \"\"\"\n",
        "  averaged_macro_f1 = None\n",
        "  ### BEGIN YOUR CODE (~1 line) ###\n",
        "\n",
        "  # Simply call the binary f1 method with each class then average their f1 scores\n",
        "  averaged_macro_f1 = (binary_f1(true, pred, True) + binary_f1(true, pred, False)) / 2\n",
        "\n",
        "  ### END YOUR CODE ###\n",
        "  return averaged_macro_f1"
      ],
      "id": "gMQDg9Vy-wY0"
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "Yw79JFieFEIi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea5c4453-f3ab-425b-c737-267004e2e764"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 150/150 [00:00<00:00, 228.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'float'> <class 'float'>\n",
            "Binary Macro F1: 0.4477378254694484\n",
            "Accuracy: 0.4554166666666667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ===========================================================================\n",
        "# To test your eval implementation, we will evaluate the untrained model on our\n",
        "# dev dataset. It will do pretty poorly (it's untrained), but the exact performance\n",
        "# will be random since the initialization of the model parameters is random.\n",
        "# ===========================================================================\n",
        "\n",
        "true, pred = val_loop(model, val_iterator)\n",
        "print(type(true[0]), type(pred[0]))\n",
        "print(f'Binary Macro F1: {binary_macro_f1(true, pred)}')\n",
        "print(f'Accuracy: {accuracy(true, pred)}')"
      ],
      "id": "Yw79JFieFEIi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2to0kWVFEIi"
      },
      "source": [
        "## 4. Full Training Run [5 points]\n",
        "Now we can perform a full run and see the model fit our loss. If everything goes correctly, you should be able to achieve a validation F1 score of at least `0.80`.\n",
        "\n",
        "**Feel free to adjust the number of epochs to prevent overfitting or underfitting and to play with your model hyperparameters/optimizer & loss function.**"
      ],
      "id": "Q2to0kWVFEIi"
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "N-iuqkKCFEIj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "781d5677-42b1-4c6a-ee12-20cb16316f58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:06<00:00, 195.27it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 346.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 0\n",
            "TRAIN LOSS: 767.8507190346718\n",
            "VAL F-1: 0.587673611111111\n",
            "VAL ACC: 0.6991666666666667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:03<00:00, 308.14it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 439.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 1\n",
            "TRAIN LOSS: 667.0417526066303\n",
            "VAL F-1: 0.7117369385007251\n",
            "VAL ACC: 0.7620833333333333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:04<00:00, 289.93it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 553.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 2\n",
            "TRAIN LOSS: 547.7262425273657\n",
            "VAL F-1: 0.79538555222088\n",
            "VAL ACC: 0.81875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:04<00:00, 243.51it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 475.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 3\n",
            "TRAIN LOSS: 468.44989389926195\n",
            "VAL F-1: 0.8161130550490365\n",
            "VAL ACC: 0.8341666666666666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:03<00:00, 312.08it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 567.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 4\n",
            "TRAIN LOSS: 415.070921190083\n",
            "VAL F-1: 0.8340184138203677\n",
            "VAL ACC: 0.84875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:03<00:00, 307.31it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 558.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 5\n",
            "TRAIN LOSS: 378.65580882132053\n",
            "VAL F-1: 0.8428343425236662\n",
            "VAL ACC: 0.8566666666666667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:04<00:00, 241.14it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 473.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 6\n",
            "TRAIN LOSS: 348.6848907135427\n",
            "VAL F-1: 0.8451548158574033\n",
            "VAL ACC: 0.8579166666666667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:04<00:00, 248.80it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 559.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 7\n",
            "TRAIN LOSS: 326.0540223605931\n",
            "VAL F-1: 0.8523431718844741\n",
            "VAL ACC: 0.8645833333333334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:03<00:00, 309.98it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 529.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 8\n",
            "TRAIN LOSS: 305.8578112833202\n",
            "VAL F-1: 0.8540276431396372\n",
            "VAL ACC: 0.865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [00:05<00:00, 237.77it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 523.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 9\n",
            "TRAIN LOSS: 288.4666445739567\n",
            "VAL F-1: 0.855666313184871\n",
            "VAL ACC: 0.8670833333333333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "TOTAL_EPOCHS = 10\n",
        "for epoch in range(TOTAL_EPOCHS):\n",
        "    train_loss = train_loop(model, criterion, optimizer, train_iterator)\n",
        "    true, pred = val_loop(model, val_iterator)\n",
        "    print(f\"EPOCH: {epoch}\")\n",
        "    print(f\"TRAIN LOSS: {train_loss}\")\n",
        "    print(f\"VAL F-1: {binary_macro_f1(true, pred)}\")\n",
        "    print(f\"VAL ACC: {accuracy(true, pred)}\")"
      ],
      "id": "N-iuqkKCFEIj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_l91F4ooFEIj"
      },
      "source": [
        "We can also look at the models performance on the held-out test set, using the same `val_loop` from earlier."
      ],
      "id": "_l91F4ooFEIj"
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "vs8Fy_ncFEIo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64aada11-b9aa-46b8-f686-93b9a7d73f64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 150/150 [00:00<00:00, 493.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEST F-1: 0.8473055797086082\n",
            "TEST ACC: 0.86\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "true, pred = val_loop(model, test_iterator)\n",
        "print(f\"TEST F-1: {binary_macro_f1(true, pred)}\")\n",
        "print(f\"TEST ACC: {accuracy(true, pred)}\")"
      ],
      "id": "vs8Fy_ncFEIo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMPWmorEFEIp"
      },
      "source": [
        "## 5. Analysis [5 points]\n",
        "While modeling and accuracy are a great signal that our model is working in our specific task setup, an inspection of what the model is classifying (particularly its errors), can allow us to hypothesize about what is going on, why it works, and how to improve.\n",
        "\n"
      ],
      "id": "rMPWmorEFEIp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnjKlKt352hQ"
      },
      "source": [
        "### 5.1 Impact of Vocab Size\n",
        "**Question:** *What happens to the vocab size as you change the cutoff in the cell below? Can you explain this in the context of [Zipf's Law](https://en.wikipedia.org/wiki/Zipf%27s_law)?*\n",
        "\n",
        "**Answer:** The vocab size decrease by a factor of ~40% when changing the cutoff from 1 to 2 and decresaes by a factor of ~20% when changing the cutoff frmo 2 to 3 as you can see below. This follows **Zipf's Law** which states that the frequency of a word is inversely proportional to its rank in the frequency table. In other words, a small number of words appear very frequently, while the majority occur rarely, so increasing the cutoff disproportionately removes less common words, leading to a sharp drop in vocabulary size as the cutoff incresease but less and less percent of words get cutoff as you incresae the cuttoff threshold."
      ],
      "id": "fnjKlKt352hQ"
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "pI0fM4oMFEIp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bae8601-347a-4614-cf0d-9be328b3a884"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cutoff=1: 13289\n",
            "cutoff=2: 9561\n",
            "cutoff=3: 7629\n"
          ]
        }
      ],
      "source": [
        "tmp_vocab, _ = generate_vocab_map(train_df, cutoff = 1)\n",
        "print(f\"cutoff=1: {len(tmp_vocab)}\")\n",
        "\n",
        "tmp_vocab, _ = generate_vocab_map(train_df, cutoff = 2)\n",
        "print(f\"cutoff=2: {len(tmp_vocab)}\")\n",
        "\n",
        "tmp_vocab, _ = generate_vocab_map(train_df, cutoff = 3)\n",
        "print(f\"cutoff=3: {len(tmp_vocab)}\")"
      ],
      "id": "pI0fM4oMFEIp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0x54B1lFEIp"
      },
      "source": [
        "### 5.2 Error Analysis\n",
        "\n",
        "*Can you describe what cases the model is getting wrong in the witheld test-set?*\n",
        "\n",
        "To do this, you will need to create a new `val_train_loop_incorrect` which returns incorrect sequences **and** you will need to decode these sequences back into words. You have already created a map that can convert encoded sequences back to regular English (`reverse_vocab`)."
      ],
      "id": "d0x54B1lFEIp"
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "TfohtPF8FEIp"
      },
      "outputs": [],
      "source": [
        "def val_train_loop_incorrect(model, iterator):\n",
        "  \"\"\"\n",
        "  Implement this however you like! It should look very similar to val_loop.\n",
        "  Pass the test_iterator through this function to look at errors in the test set.\n",
        "  \"\"\"\n",
        "\n",
        "  # Make a list to hold the sequences the model mis-classified\n",
        "  incorrect_seqs = []\n",
        "\n",
        "  # Put model into evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  # Iterate over test data\n",
        "  for x, y in tqdm(iterator):\n",
        "    # print(\"input\", x)\n",
        "    classification_score = model(x)\n",
        "    pred_y = (classification_score > 0.5).float()\n",
        "\n",
        "    for i in range(len(pred_y)):\n",
        "      if pred_y[i] != y[i]:\n",
        "        incorrect_seqs.append(x[i].tolist())\n",
        "\n",
        "  # Determine incorrect sequences\n",
        "\n",
        "  # Decode data back into English words\n",
        "  # (HINT: you should use the dictionary, reverse_vocab, defined earlier)\n",
        "  # (SECOND HINT: make sure you stop decoding once you hit padding tokens!)\n",
        "\n",
        "  for i, seq in enumerate(incorrect_seqs):\n",
        "    for j, token in enumerate(seq):\n",
        "      if token == 0:\n",
        "        incorrect_seqs[i] = incorrect_seqs[i][:j]\n",
        "        break\n",
        "\n",
        "      incorrect_seqs[i][j] = reverse_vocab[int(token)]\n",
        "\n",
        "\n",
        "  return incorrect_seqs"
      ],
      "id": "TfohtPF8FEIp"
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "6-azPje88iU0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b84a2071-0cc6-4c96-b761-de307864ebad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 150/150 [00:00<00:00, 450.36it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['white', 'house', 'announces', 'UNK', 'great', 'in', 'UNK', \"'\"],\n",
              " ['changing',\n",
              "  'digital',\n",
              "  'economy',\n",
              "  ':',\n",
              "  'the',\n",
              "  'UNK',\n",
              "  'has',\n",
              "  'laid',\n",
              "  'off',\n",
              "  '80',\n",
              "  'percent',\n",
              "  'of',\n",
              "  'its',\n",
              "  'firefighters',\n",
              "  'because',\n",
              "  'it',\n",
              "  'isn',\n",
              "  '’',\n",
              "  't',\n",
              "  'getting',\n",
              "  'enough',\n",
              "  'traffic',\n",
              "  'on',\n",
              "  'UNK'],\n",
              " ['north',\n",
              "  'carolina',\n",
              "  'residents',\n",
              "  'terrified',\n",
              "  'after',\n",
              "  'hearing',\n",
              "  'state',\n",
              "  'passed',\n",
              "  'new',\n",
              "  'law'],\n",
              " ['report',\n",
              "  ':',\n",
              "  'average',\n",
              "  'male',\n",
              "  '4,000',\n",
              "  '%',\n",
              "  'less',\n",
              "  'effective',\n",
              "  'in',\n",
              "  'fights',\n",
              "  'than',\n",
              "  'they',\n",
              "  'imagine'],\n",
              " ['UNK',\n",
              "  'sarah',\n",
              "  'palin',\n",
              "  'opens',\n",
              "  'up',\n",
              "  'about',\n",
              "  'UNK',\n",
              "  'baron',\n",
              "  'cohen',\n",
              "  'UNK',\n",
              "  'to',\n",
              "  'cardboard',\n",
              "  'cutout',\n",
              "  'of',\n",
              "  'UNK',\n",
              "  'UNK'],\n",
              " ['newt',\n",
              "  'gingrich',\n",
              "  'slams',\n",
              "  '‘',\n",
              "  'new',\n",
              "  'york',\n",
              "  'times',\n",
              "  '’',\n",
              "  'UNK',\n",
              "  'project',\n",
              "  'as',\n",
              "  'UNK',\n",
              "  'UNK',\n",
              "  'propaganda'],\n",
              " ['nasa',\n",
              "  'says',\n",
              "  'presence',\n",
              "  'of',\n",
              "  'diving',\n",
              "  'board',\n",
              "  'on',\n",
              "  'mars',\n",
              "  'confirms',\n",
              "  'planet',\n",
              "  'may',\n",
              "  'have',\n",
              "  'once',\n",
              "  'contained',\n",
              "  'water'],\n",
              " ['mitch',\n",
              "  'mcconnell',\n",
              "  'campaign',\n",
              "  'criticized',\n",
              "  'for',\n",
              "  'photos',\n",
              "  'of',\n",
              "  'UNK',\n",
              "  'with',\n",
              "  'UNK',\n",
              "  'names'],\n",
              " ['report',\n",
              "  ':',\n",
              "  'u.s.',\n",
              "  'death',\n",
              "  'rates',\n",
              "  'from',\n",
              "  'drugs',\n",
              "  ',',\n",
              "  'suicide',\n",
              "  ',',\n",
              "  'and',\n",
              "  'alcohol',\n",
              "  'have',\n",
              "  'UNK',\n",
              "  'increased',\n",
              "  ',',\n",
              "  'but',\n",
              "  'not',\n",
              "  'in',\n",
              "  'a',\n",
              "  'cool',\n",
              "  'rock',\n",
              "  'and',\n",
              "  'roll',\n",
              "  'way'],\n",
              " ['family',\n",
              "  'of',\n",
              "  'girl',\n",
              "  'killed',\n",
              "  'UNK',\n",
              "  'down',\n",
              "  'UNK',\n",
              "  'sues',\n",
              "  'palmer',\n",
              "  'house',\n",
              "  ',',\n",
              "  '[',\n",
              "  'also',\n",
              "  ']',\n",
              "  'event',\n",
              "  'hosts'],\n",
              " ['UNK',\n",
              "  'torch',\n",
              "  'company',\n",
              "  ':',\n",
              "  'we',\n",
              "  'have',\n",
              "  'nothing',\n",
              "  'to',\n",
              "  'do',\n",
              "  'with',\n",
              "  'white',\n",
              "  'UNK'],\n",
              " ['pr',\n",
              "  'exec',\n",
              "  'tweets',\n",
              "  ',',\n",
              "  '‘',\n",
              "  'going',\n",
              "  'to',\n",
              "  'africa',\n",
              "  '.',\n",
              "  'hope',\n",
              "  'i',\n",
              "  'don',\n",
              "  '’',\n",
              "  't',\n",
              "  'get',\n",
              "  'aids',\n",
              "  '.',\n",
              "  'just',\n",
              "  'UNK',\n",
              "  '.',\n",
              "  'i',\n",
              "  '’',\n",
              "  'm',\n",
              "  'white',\n",
              "  '!',\n",
              "  '’'],\n",
              " ['dog',\n",
              "  'dumped',\n",
              "  'by',\n",
              "  'roadside',\n",
              "  'with',\n",
              "  'her',\n",
              "  'dead',\n",
              "  'puppies',\n",
              "  'in',\n",
              "  'a',\n",
              "  'bag',\n",
              "  'UNK',\n",
              "  'tears',\n",
              "  'in',\n",
              "  'her',\n",
              "  'eyes',\n",
              "  \"'\"],\n",
              " ['my',\n",
              "  'UNK',\n",
              "  'came',\n",
              "  'to',\n",
              "  'this',\n",
              "  'country',\n",
              "  'with',\n",
              "  'nothing',\n",
              "  'but',\n",
              "  '$',\n",
              "  '10',\n",
              "  'in',\n",
              "  'his',\n",
              "  'pocket',\n",
              "  ',',\n",
              "  '$',\n",
              "  '300,000',\n",
              "  'in',\n",
              "  'his',\n",
              "  'bank',\n",
              "  'account',\n",
              "  ',',\n",
              "  'and',\n",
              "  'a',\n",
              "  'dream'],\n",
              " ['lyft', 'says', 'it', 'will', 'make', 'every', 'ride', 'carbon', 'UNK'],\n",
              " ['melting',\n",
              "  'UNK',\n",
              "  'to',\n",
              "  'have',\n",
              "  '$',\n",
              "  '70',\n",
              "  'trillion',\n",
              "  'impact',\n",
              "  ',',\n",
              "  'study',\n",
              "  'finds'],\n",
              " ['mom',\n",
              "  'of',\n",
              "  '5',\n",
              "  'says',\n",
              "  ',',\n",
              "  '‘',\n",
              "  'i',\n",
              "  '’',\n",
              "  'm',\n",
              "  'really',\n",
              "  'good',\n",
              "  'at',\n",
              "  'getting',\n",
              "  'pregnant',\n",
              "  'and',\n",
              "  'having',\n",
              "  'kids',\n",
              "  '.',\n",
              "  'i',\n",
              "  'just',\n",
              "  'don',\n",
              "  '’',\n",
              "  't',\n",
              "  'know',\n",
              "  'how',\n",
              "  'to',\n",
              "  'raise',\n",
              "  'them',\n",
              "  '’'],\n",
              " ['peta',\n",
              "  'to',\n",
              "  'hand',\n",
              "  'out',\n",
              "  'UNK',\n",
              "  'UNK',\n",
              "  'UNK',\n",
              "  'at',\n",
              "  'UNK',\n",
              "  '2',\n",
              "  ':',\n",
              "  'heart',\n",
              "  'of',\n",
              "  'the',\n",
              "  'UNK',\n",
              "  'launch'],\n",
              " ['hostages',\n",
              "  'trapped',\n",
              "  'inside',\n",
              "  'walmart',\n",
              "  'UNK',\n",
              "  'they',\n",
              "  'never',\n",
              "  'shop',\n",
              "  'at',\n",
              "  'walmart'],\n",
              " ['u.s.', 'protests', 'UNK', 'UNK'],\n",
              " ['gunman', 'kills', '15', 'potential', 'swing', 'voters'],\n",
              " ['UNK',\n",
              "  'UNK',\n",
              "  'reminds',\n",
              "  'herself',\n",
              "  'she',\n",
              "  'a',\n",
              "  'private',\n",
              "  'citizen',\n",
              "  'now',\n",
              "  'after',\n",
              "  'UNK',\n",
              "  'UNK',\n",
              "  'mexican',\n",
              "  'child',\n",
              "  'on',\n",
              "  'the',\n",
              "  'street'],\n",
              " ['michael', 'phelps', 'spots', 'UNK', 'father', 'UNK', 'in', 'stands'],\n",
              " ['now', 'UNK', 'can', 'give', 'you', 'malware'],\n",
              " ['UNK',\n",
              "  'rich',\n",
              "  'people',\n",
              "  'now',\n",
              "  'have',\n",
              "  'as',\n",
              "  'much',\n",
              "  'wealth',\n",
              "  'as',\n",
              "  '50',\n",
              "  '%',\n",
              "  'of',\n",
              "  'the',\n",
              "  'rest',\n",
              "  'of',\n",
              "  'humanity',\n",
              "  'UNK'],\n",
              " ['white',\n",
              "  'house',\n",
              "  'releases',\n",
              "  'UNK',\n",
              "  'trailer',\n",
              "  'for',\n",
              "  'state',\n",
              "  'of',\n",
              "  'the',\n",
              "  'union'],\n",
              " ['UNK',\n",
              "  'duck',\n",
              "  'doesn',\n",
              "  '’',\n",
              "  't',\n",
              "  'recall',\n",
              "  'asking',\n",
              "  'for',\n",
              "  'injured',\n",
              "  'baby',\n",
              "  'to',\n",
              "  'be',\n",
              "  'rescued',\n",
              "  'from',\n",
              "  'road'],\n",
              " ['gop',\n",
              "  'rep.',\n",
              "  'steve',\n",
              "  'king',\n",
              "  'questions',\n",
              "  'UNK',\n",
              "  'UNK',\n",
              "  'to',\n",
              "  'civilization'],\n",
              " ['visitors',\n",
              "  'to',\n",
              "  'chinese',\n",
              "  'zoo',\n",
              "  'feel',\n",
              "  'UNK',\n",
              "  'after',\n",
              "  'discovering',\n",
              "  'new',\n",
              "  'penguin',\n",
              "  'display',\n",
              "  'UNK',\n",
              "  'of',\n",
              "  'UNK',\n",
              "  'toys'],\n",
              " ['‘',\n",
              "  'cooking',\n",
              "  'together',\n",
              "  'is',\n",
              "  'so',\n",
              "  'fun',\n",
              "  ',',\n",
              "  '’',\n",
              "  'says',\n",
              "  'man',\n",
              "  'UNK',\n",
              "  'girlfriend',\n",
              "  '’',\n",
              "  's',\n",
              "  'every',\n",
              "  'knife',\n",
              "  'cut'],\n",
              " ['michael', 'UNK', 'trapped', 'in', 'dressing', 'room'],\n",
              " ['UNK',\n",
              "  'isis',\n",
              "  'supporters',\n",
              "  'can',\n",
              "  '’',\n",
              "  't',\n",
              "  'wait',\n",
              "  'for',\n",
              "  'another',\n",
              "  'beheading',\n",
              "  'video'],\n",
              " ['‘',\n",
              "  'UNK',\n",
              "  'hubris',\n",
              "  'led',\n",
              "  'me',\n",
              "  'here',\n",
              "  ',',\n",
              "  '’',\n",
              "  'thinks',\n",
              "  'naked',\n",
              "  'woman',\n",
              "  'sitting',\n",
              "  'on',\n",
              "  'public',\n",
              "  'toilet',\n",
              "  'with',\n",
              "  'UNK',\n",
              "  'around',\n",
              "  'her',\n",
              "  'ankles'],\n",
              " ['social', 'media', 'linked', 'to', 'increased', 'UNK'],\n",
              " ['tourist',\n",
              "  'in',\n",
              "  'white',\n",
              "  'house',\n",
              "  'gift',\n",
              "  'shop',\n",
              "  'UNK',\n",
              "  'UNK',\n",
              "  'of',\n",
              "  'security',\n",
              "  'UNK'],\n",
              " ['liberty',\n",
              "  'university',\n",
              "  'board',\n",
              "  'concerned',\n",
              "  'UNK',\n",
              "  '’',\n",
              "  's',\n",
              "  'corruption',\n",
              "  'UNK',\n",
              "  'UNK',\n",
              "  'college',\n",
              "  '’',\n",
              "  's',\n",
              "  'mission',\n",
              "  'of',\n",
              "  'UNK',\n",
              "  'women',\n",
              "  'and',\n",
              "  'gay',\n",
              "  'people'],\n",
              " ['new',\n",
              "  'yorkers',\n",
              "  'go',\n",
              "  '24',\n",
              "  'hours',\n",
              "  'without',\n",
              "  'shooting',\n",
              "  ',',\n",
              "  'stabbing',\n",
              "  ',',\n",
              "  'or',\n",
              "  'UNK',\n",
              "  'each',\n",
              "  'other'],\n",
              " ['UNK', 'dog', 'helped', 'bring', 'down', 'subway', 'star', 'jared', 'fogle'],\n",
              " ['proud',\n",
              "  'billionaire',\n",
              "  'helps',\n",
              "  'young',\n",
              "  'son',\n",
              "  'open',\n",
              "  'first',\n",
              "  'UNK',\n",
              "  'bank',\n",
              "  'account'],\n",
              " ['philadelphia',\n",
              "  'to',\n",
              "  'become',\n",
              "  'UNK',\n",
              "  ',',\n",
              "  'pa',\n",
              "  'after',\n",
              "  'cream',\n",
              "  'cheese',\n",
              "  'UNK',\n",
              "  'loses',\n",
              "  'naming',\n",
              "  'rights'],\n",
              " ['UNK',\n",
              "  'things',\n",
              "  'i',\n",
              "  'hate',\n",
              "  'about',\n",
              "  'you',\n",
              "  \"'\",\n",
              "  'star',\n",
              "  'andrew',\n",
              "  'UNK',\n",
              "  'starts',\n",
              "  'new',\n",
              "  'religion'],\n",
              " ['white',\n",
              "  'house',\n",
              "  ':',\n",
              "  'UNK',\n",
              "  'russia',\n",
              "  ',',\n",
              "  'the',\n",
              "  'real',\n",
              "  'UNK',\n",
              "  'is',\n",
              "  'knowing',\n",
              "  'that',\n",
              "  'they',\n",
              "  'let',\n",
              "  'us',\n",
              "  'down',\n",
              "  \"'\"],\n",
              " ['new',\n",
              "  'device',\n",
              "  'has',\n",
              "  'the',\n",
              "  'UNK',\n",
              "  'to',\n",
              "  'land',\n",
              "  'UNK',\n",
              "  'on',\n",
              "  'human',\n",
              "  'skin',\n",
              "  ',',\n",
              "  'use',\n",
              "  'its',\n",
              "  'UNK',\n",
              "  'UNK',\n",
              "  'UNK',\n",
              "  'to',\n",
              "  'take',\n",
              "  'dna',\n",
              "  'UNK',\n",
              "  'and',\n",
              "  'fly',\n",
              "  'off',\n",
              "  'again',\n",
              "  'at',\n",
              "  'speed',\n",
              "  '.'],\n",
              " ['UNK',\n",
              "  'attack',\n",
              "  'neil',\n",
              "  'degrasse',\n",
              "  'tyson',\n",
              "  'for',\n",
              "  'blind',\n",
              "  'faith',\n",
              "  'in',\n",
              "  'science'],\n",
              " ['UNK',\n",
              "  'UNK',\n",
              "  'academy',\n",
              "  'posts',\n",
              "  'UNK',\n",
              "  'video',\n",
              "  'for',\n",
              "  'defending',\n",
              "  'against',\n",
              "  'trump',\n",
              "  'UNK'],\n",
              " ['pope',\n",
              "  'promises',\n",
              "  'more',\n",
              "  'open',\n",
              "  ',',\n",
              "  'transparent',\n",
              "  'UNK',\n",
              "  'in',\n",
              "  'future'],\n",
              " ['north', 'korea', 'open', 'to', 'UNK', 'nuclear', 'arms'],\n",
              " ['mit',\n",
              "  'researchers',\n",
              "  'create',\n",
              "  '‘',\n",
              "  'psychopath',\n",
              "  '’',\n",
              "  'ai',\n",
              "  'by',\n",
              "  'feeding',\n",
              "  'it',\n",
              "  'reddit',\n",
              "  'data'],\n",
              " ['2018', 'winter', 'olympics', 'cancelled', 'due', 'to', 'UNK', 'weather'],\n",
              " ['cop', 'hired', 'for', 'posting', 'racist', 'rant', 'on', 'social', 'media'],\n",
              " ['giant',\n",
              "  'UNK',\n",
              "  'hitting',\n",
              "  'the',\n",
              "  'earth',\n",
              "  'polling',\n",
              "  'well',\n",
              "  'in',\n",
              "  'presidential',\n",
              "  'election'],\n",
              " ['just',\n",
              "  'in',\n",
              "  'time',\n",
              "  'for',\n",
              "  'prom',\n",
              "  ',',\n",
              "  'kfc',\n",
              "  'unveils',\n",
              "  'UNK',\n",
              "  'chicken',\n",
              "  'UNK'],\n",
              " ['climate', 'change', 'is', 'no', 'joke', 'jim'],\n",
              " ['the',\n",
              "  'backlash',\n",
              "  'continues',\n",
              "  ':',\n",
              "  'colin',\n",
              "  'kaepernick',\n",
              "  'is',\n",
              "  'joining',\n",
              "  'the',\n",
              "  'nike',\n",
              "  'boycott',\n",
              "  'after',\n",
              "  'learning',\n",
              "  'it',\n",
              "  'doesn',\n",
              "  '’',\n",
              "  't',\n",
              "  'make',\n",
              "  'top',\n",
              "  'hats'],\n",
              " ['seasons',\n",
              "  'turn',\n",
              "  'UNK',\n",
              "  'from',\n",
              "  'the',\n",
              "  'one',\n",
              "  'that',\n",
              "  'kills',\n",
              "  'old',\n",
              "  'people',\n",
              "  'to',\n",
              "  'the',\n",
              "  'one',\n",
              "  'that',\n",
              "  'kills',\n",
              "  'homeless',\n",
              "  'people'],\n",
              " ['sad', 'UNK', 'UNK', 'UNK', 'UNK', 'for', 'baby', 'animals', \"'\"],\n",
              " ['study',\n",
              "  ':',\n",
              "  'employees',\n",
              "  'UNK',\n",
              "  'when',\n",
              "  'pretending',\n",
              "  'to',\n",
              "  'work',\n",
              "  'from',\n",
              "  'home'],\n",
              " ['warren',\n",
              "  'buffett',\n",
              "  'eats',\n",
              "  'like',\n",
              "  'a',\n",
              "  'six-year-old',\n",
              "  'as',\n",
              "  'they',\n",
              "  'have',\n",
              "  'UNK',\n",
              "  'death',\n",
              "  'rate'],\n",
              " ['mike',\n",
              "  'pence',\n",
              "  'warns',\n",
              "  'u.s.',\n",
              "  'heading',\n",
              "  'for',\n",
              "  'UNK',\n",
              "  'health',\n",
              "  'care',\n",
              "  'if',\n",
              "  'UNK',\n",
              "  'bill',\n",
              "  'fails'],\n",
              " ['clinton',\n",
              "  'aide',\n",
              "  'told',\n",
              "  'to',\n",
              "  'leave',\n",
              "  'behind',\n",
              "  'weak',\n",
              "  'volunteer',\n",
              "  'who',\n",
              "  'UNK',\n",
              "  'during',\n",
              "  'march',\n",
              "  'to',\n",
              "  'south',\n",
              "  'carolina'],\n",
              " ['heart', 'attack', 'grill', 'spokesman', 'dies', 'from', 'heart', 'attack'],\n",
              " ['billionaire',\n",
              "  'reading',\n",
              "  'name',\n",
              "  'in',\n",
              "  'panama',\n",
              "  'papers',\n",
              "  'totally',\n",
              "  'forgot',\n",
              "  'he',\n",
              "  'even',\n",
              "  'had',\n",
              "  'funds',\n",
              "  'in',\n",
              "  'UNK'],\n",
              " ['obama', ':', '‘', 'we', 'tortured', 'some', 'folks', '’'],\n",
              " ['pornhub',\n",
              "  'forced',\n",
              "  'to',\n",
              "  'take',\n",
              "  'down',\n",
              "  'giant',\n",
              "  'times',\n",
              "  'square',\n",
              "  'erection',\n",
              "  'after',\n",
              "  'UNK',\n",
              "  'opposition'],\n",
              " ['mcdonald', '’', 's', 'launches', 'snapchat', 'job', 'UNK'],\n",
              " ['hurricane',\n",
              "  'concerned',\n",
              "  'it',\n",
              "  'caught',\n",
              "  'something',\n",
              "  'in',\n",
              "  'panama',\n",
              "  'city',\n",
              "  ',',\n",
              "  'florida'],\n",
              " ['vatican',\n",
              "  'on',\n",
              "  'sex',\n",
              "  'abuse',\n",
              "  'report',\n",
              "  ':',\n",
              "  '‘',\n",
              "  'listen',\n",
              "  ',',\n",
              "  'no',\n",
              "  'normal',\n",
              "  'person',\n",
              "  'is',\n",
              "  'going',\n",
              "  'to',\n",
              "  'sign',\n",
              "  'up',\n",
              "  'to',\n",
              "  'be',\n",
              "  'a',\n",
              "  'priest',\n",
              "  '’'],\n",
              " ['experience',\n",
              "  ':',\n",
              "  'i',\n",
              "  'was',\n",
              "  'swallowed',\n",
              "  'by',\n",
              "  'a',\n",
              "  'UNK',\n",
              "  '|',\n",
              "  'life',\n",
              "  'and',\n",
              "  'style'],\n",
              " ['kim', 'kardashian', 'is', 'giving', 'a', 'talk', 'on', 'female', 'UNK'],\n",
              " ['ready', 'meal', 'dinner', 'parties', '‘', 'no', 'longer', 'shameful', '’'],\n",
              " ['upcoming',\n",
              "  '‘',\n",
              "  'game',\n",
              "  'of',\n",
              "  'thrones',\n",
              "  '’',\n",
              "  'battle',\n",
              "  'reportedly',\n",
              "  'took',\n",
              "  '55',\n",
              "  'days',\n",
              "  'to',\n",
              "  'shoot'],\n",
              " ['you', 'can', 'hold', 'snake', ',', 'owner', 'reports'],\n",
              " ['so',\n",
              "  'long',\n",
              "  ',',\n",
              "  'UNK',\n",
              "  ':',\n",
              "  'man',\n",
              "  'UNK',\n",
              "  'out',\n",
              "  'part',\n",
              "  'of',\n",
              "  'toy',\n",
              "  'UNK',\n",
              "  'after',\n",
              "  'UNK',\n",
              "  'years'],\n",
              " ['google', 'news', 'today'],\n",
              " ['dad',\n",
              "  'of',\n",
              "  'famous',\n",
              "  'meme',\n",
              "  'star',\n",
              "  '‘',\n",
              "  'success',\n",
              "  'kid',\n",
              "  '’',\n",
              "  'severely',\n",
              "  'needs',\n",
              "  'new',\n",
              "  'kidney'],\n",
              " ['chinese',\n",
              "  'UNK',\n",
              "  'UNK',\n",
              "  'designs',\n",
              "  'phone',\n",
              "  'based',\n",
              "  'off',\n",
              "  'of',\n",
              "  'leaked',\n",
              "  'iphone',\n",
              "  '5',\n",
              "  'parts',\n",
              "  ',',\n",
              "  'then',\n",
              "  'patents',\n",
              "  'design',\n",
              "  '.'],\n",
              " ['samuel', 'adams', 'apologizes', 'for', 'UNK', 'sucks', \"'\", 'UNK'],\n",
              " ['obama',\n",
              "  'replaces',\n",
              "  'costly',\n",
              "  'high-speed',\n",
              "  'UNK',\n",
              "  'plan',\n",
              "  'with',\n",
              "  'high-speed',\n",
              "  'bus',\n",
              "  'plan'],\n",
              " ['UNK', 'sets', 'japan', 'back', 'to', 'UNK'],\n",
              " ['husband',\n",
              "  ',',\n",
              "  'wife',\n",
              "  'share',\n",
              "  'story',\n",
              "  'of',\n",
              "  'coming',\n",
              "  'out',\n",
              "  'together',\n",
              "  ',',\n",
              "  '20',\n",
              "  'years',\n",
              "  'into',\n",
              "  'their',\n",
              "  'marriage'],\n",
              " ['authorities',\n",
              "  'were',\n",
              "  'at',\n",
              "  'UNK',\n",
              "  'scene',\n",
              "  'for',\n",
              "  'an',\n",
              "  'hour',\n",
              "  'before',\n",
              "  'realizing',\n",
              "  'person',\n",
              "  'was',\n",
              "  'still',\n",
              "  'alive'],\n",
              " ['doctor',\n",
              "  'who',\n",
              "  'made',\n",
              "  'music',\n",
              "  'videos',\n",
              "  'in',\n",
              "  'UNK',\n",
              "  'room',\n",
              "  'facing',\n",
              "  'several',\n",
              "  'UNK',\n",
              "  'suits'],\n",
              " ['katy', 'perry', 'is', '2015', '’', 's', 'UNK', 'UNK', 'person'],\n",
              " ['morale',\n",
              "  'low',\n",
              "  'at',\n",
              "  'state',\n",
              "  'department',\n",
              "  'after',\n",
              "  'only',\n",
              "  'employee',\n",
              "  'fired'],\n",
              " ['the',\n",
              "  'creators',\n",
              "  'of',\n",
              "  'the',\n",
              "  'nintendo',\n",
              "  'UNK',\n",
              "  'UNK',\n",
              "  'have',\n",
              "  'officially',\n",
              "  'ruled',\n",
              "  ':',\n",
              "  'UNK',\n",
              "  'is',\n",
              "  'cheating'],\n",
              " ['‘',\n",
              "  'UNK',\n",
              "  'rule',\n",
              "  '’',\n",
              "  'for',\n",
              "  'food',\n",
              "  'dropped',\n",
              "  'on',\n",
              "  'the',\n",
              "  'floor',\n",
              "  'approved',\n",
              "  'by',\n",
              "  'UNK',\n",
              "  'scientists'],\n",
              " ['nate',\n",
              "  'silver',\n",
              "  'defends',\n",
              "  'torture',\n",
              "  'UNK',\n",
              "  'used',\n",
              "  'to',\n",
              "  'make',\n",
              "  'election',\n",
              "  'UNK'],\n",
              " ['legendary',\n",
              "  'bass',\n",
              "  'UNK',\n",
              "  'explains',\n",
              "  'how',\n",
              "  'easily',\n",
              "  'he',\n",
              "  \"'d\",\n",
              "  'catch',\n",
              "  'the',\n",
              "  'fish',\n",
              "  'monster',\n",
              "  'from',\n",
              "  \"'the\",\n",
              "  'shape',\n",
              "  'of',\n",
              "  'water',\n",
              "  \"'\"],\n",
              " ['trump',\n",
              "  ',',\n",
              "  'hitler',\n",
              "  'among',\n",
              "  'UNK',\n",
              "  'to',\n",
              "  'rename',\n",
              "  'robert',\n",
              "  'e.',\n",
              "  'lee',\n",
              "  'elementary'],\n",
              " ['biden',\n",
              "  'offers',\n",
              "  'government',\n",
              "  'post',\n",
              "  'to',\n",
              "  'UNK',\n",
              "  ',',\n",
              "  'UNK',\n",
              "  'of',\n",
              "  'the',\n",
              "  'dark'],\n",
              " ['UNK',\n",
              "  \"'s\",\n",
              "  'sexual',\n",
              "  'UNK',\n",
              "  'UNK',\n",
              "  'for',\n",
              "  'gay',\n",
              "  'marriage',\n",
              "  'slippery',\n",
              "  'slope',\n",
              "  'to',\n",
              "  'kick',\n",
              "  'in'],\n",
              " ['climate',\n",
              "  'change',\n",
              "  'researcher',\n",
              "  'describes',\n",
              "  'challenge',\n",
              "  'of',\n",
              "  'pulling',\n",
              "  'off',\n",
              "  'UNK',\n",
              "  'global',\n",
              "  'warming',\n",
              "  'conspiracy'],\n",
              " ['india',\n",
              "  \"'s\",\n",
              "  'UNK',\n",
              "  'plan',\n",
              "  'to',\n",
              "  'cut',\n",
              "  'red',\n",
              "  'tape',\n",
              "  'gets',\n",
              "  'UNK',\n",
              "  'in',\n",
              "  '...',\n",
              "  'red',\n",
              "  'tape',\n",
              "  '|',\n",
              "  'UNK'],\n",
              " ['time', 'capsule', 'from', '50', 'years', 'ago', 'has', 'nothing', 'inside'],\n",
              " ['warren',\n",
              "  'buffett',\n",
              "  'offers',\n",
              "  '$',\n",
              "  '1',\n",
              "  'billion',\n",
              "  'for',\n",
              "  'dick',\n",
              "  'UNK',\n",
              "  'to',\n",
              "  'shut',\n",
              "  'up'],\n",
              " ['ryan',\n",
              "  'UNK',\n",
              "  ':',\n",
              "  '‘',\n",
              "  'i',\n",
              "  '’',\n",
              "  'd',\n",
              "  'be',\n",
              "  'the',\n",
              "  'michael',\n",
              "  'phelps',\n",
              "  'of',\n",
              "  'swimming',\n",
              "  'if',\n",
              "  'he',\n",
              "  'wasn',\n",
              "  '’',\n",
              "  't',\n",
              "  'there',\n",
              "  '’'],\n",
              " ['eminem',\n",
              "  'horrified',\n",
              "  'upon',\n",
              "  'being',\n",
              "  'informed',\n",
              "  'that',\n",
              "  '‘',\n",
              "  'UNK',\n",
              "  '’',\n",
              "  'actually',\n",
              "  'a',\n",
              "  'harmful',\n",
              "  'gay',\n",
              "  'slur'],\n",
              " ['melania',\n",
              "  'trump',\n",
              "  'would',\n",
              "  'have',\n",
              "  'been',\n",
              "  'UNK',\n",
              "  'for',\n",
              "  'deportation',\n",
              "  'under',\n",
              "  'new',\n",
              "  'immigration',\n",
              "  'rules'],\n",
              " ['toddlers', 'have', 'shot', 'at', 'least', '23', 'people', 'this', 'year'],\n",
              " ['pillow',\n",
              "  'that',\n",
              "  'survived',\n",
              "  'man',\n",
              "  \"'s\",\n",
              "  'UNK',\n",
              "  'and',\n",
              "  'turning',\n",
              "  'stares',\n",
              "  'frozen',\n",
              "  'in',\n",
              "  'horror',\n",
              "  'at',\n",
              "  'fallen',\n",
              "  'UNK',\n",
              "  'lying',\n",
              "  'on',\n",
              "  'ground'],\n",
              " ['anti-vaccine',\n",
              "  'course',\n",
              "  'brings',\n",
              "  'u',\n",
              "  'of',\n",
              "  't',\n",
              "  'one',\n",
              "  'step',\n",
              "  'closer',\n",
              "  'to',\n",
              "  'offering',\n",
              "  'a',\n",
              "  'masters',\n",
              "  'of',\n",
              "  'UNK'],\n",
              " ['what', 'smoking', 'a', 'cigarette', 'does', 'to', 'the', 'body'],\n",
              " ['fast', 'food', 'customers', 'less', 'UNK', 'than', 'in', 'commercial'],\n",
              " ['UNK',\n",
              "  'man',\n",
              "  'does',\n",
              "  'not',\n",
              "  'maintain',\n",
              "  'erection',\n",
              "  'during',\n",
              "  'national',\n",
              "  'anthem'],\n",
              " ['gingrich',\n",
              "  ':',\n",
              "  'give',\n",
              "  'a',\n",
              "  'gun',\n",
              "  'to',\n",
              "  '‘',\n",
              "  'every',\n",
              "  'person',\n",
              "  'on',\n",
              "  'the',\n",
              "  'planet',\n",
              "  '’'],\n",
              " ['russians',\n",
              "  'purchased',\n",
              "  '500,000',\n",
              "  'baseball',\n",
              "  'bats',\n",
              "  'in',\n",
              "  'one',\n",
              "  'year',\n",
              "  '—',\n",
              "  'but',\n",
              "  'only',\n",
              "  'one',\n",
              "  'ball',\n",
              "  ':',\n",
              "  'report'],\n",
              " ['it',\n",
              "  '’',\n",
              "  's',\n",
              "  'official',\n",
              "  ':',\n",
              "  'UNK',\n",
              "  'are',\n",
              "  'just',\n",
              "  'cockroaches',\n",
              "  'with',\n",
              "  'a',\n",
              "  'fancy',\n",
              "  'social',\n",
              "  'life'],\n",
              " ['nervous',\n",
              "  'steve',\n",
              "  'bannon',\n",
              "  'UNK',\n",
              "  'entire',\n",
              "  'class',\n",
              "  'of',\n",
              "  'interns',\n",
              "  'amid',\n",
              "  'calls',\n",
              "  'for',\n",
              "  'removal'],\n",
              " ['american',\n",
              "  'UNK',\n",
              "  'founder',\n",
              "  'UNK',\n",
              "  'UNK',\n",
              "  ':',\n",
              "  '‘',\n",
              "  'sleeping',\n",
              "  'with',\n",
              "  'people',\n",
              "  'you',\n",
              "  'work',\n",
              "  'with',\n",
              "  'is',\n",
              "  'UNK',\n",
              "  '’'],\n",
              " ['leaving',\n",
              "  'this',\n",
              "  'UNK',\n",
              "  'UNK',\n",
              "  'with',\n",
              "  'a',\n",
              "  'bang',\n",
              "  '-',\n",
              "  'you',\n",
              "  'can',\n",
              "  'have',\n",
              "  'your',\n",
              "  'cremated',\n",
              "  'ashes',\n",
              "  'turned',\n",
              "  'into',\n",
              "  'UNK',\n",
              "  'fireworks',\n",
              "  'for',\n",
              "  'a',\n",
              "  'UNK',\n",
              "  'display',\n",
              "  '.'],\n",
              " ['top', 'prom', 'trends', 'for', '2015'],\n",
              " ['ice',\n",
              "  'argues',\n",
              "  'migrants',\n",
              "  'in',\n",
              "  'camps',\n",
              "  'are',\n",
              "  'free',\n",
              "  'to',\n",
              "  'die',\n",
              "  'at',\n",
              "  'any',\n",
              "  'time'],\n",
              " ['new',\n",
              "  'UNK',\n",
              "  'sorry',\n",
              "  'about',\n",
              "  'killing',\n",
              "  'journalist',\n",
              "  'UNK',\n",
              "  'UNK',\n",
              "  'and',\n",
              "  'promises',\n",
              "  'to',\n",
              "  'be',\n",
              "  'more',\n",
              "  'UNK'],\n",
              " ['man',\n",
              "  'UNK',\n",
              "  'hamster',\n",
              "  'in',\n",
              "  'front',\n",
              "  'of',\n",
              "  'children',\n",
              "  '‘',\n",
              "  'to',\n",
              "  'show',\n",
              "  'how',\n",
              "  'dear',\n",
              "  'life',\n",
              "  'is',\n",
              "  '’'],\n",
              " ['president',\n",
              "  'trump',\n",
              "  ':',\n",
              "  'i',\n",
              "  'would',\n",
              "  'have',\n",
              "  'run',\n",
              "  'into',\n",
              "  'school',\n",
              "  'during',\n",
              "  'shooting',\n",
              "  '‘',\n",
              "  'even',\n",
              "  'if',\n",
              "  'i',\n",
              "  'didn',\n",
              "  '’',\n",
              "  't',\n",
              "  'have',\n",
              "  'a',\n",
              "  'weapon',\n",
              "  '’'],\n",
              " ['fox',\n",
              "  'news',\n",
              "  'host',\n",
              "  'UNK',\n",
              "  'what',\n",
              "  'UNK',\n",
              "  'trans',\n",
              "  'UNK',\n",
              "  'in',\n",
              "  'UNK',\n",
              "  'doughnuts',\n",
              "  'means',\n",
              "  'for',\n",
              "  'freedom'],\n",
              " ['religion', 'triggers', 'brain', '’', 's', 'reward', 'centers'],\n",
              " ['u.s.', 'UNK', 'UNK', 'among', 'heavily', 'armed', 'nations'],\n",
              " ['pope',\n",
              "  'francis',\n",
              "  'trains',\n",
              "  'for',\n",
              "  'easter',\n",
              "  'mass',\n",
              "  'by',\n",
              "  'UNK',\n",
              "  'pew',\n",
              "  'loaded',\n",
              "  'with',\n",
              "  'rocks',\n",
              "  'across',\n",
              "  'snow'],\n",
              " ['sony',\n",
              "  'just',\n",
              "  'unveiled',\n",
              "  'the',\n",
              "  'UNK',\n",
              "  'product',\n",
              "  'in',\n",
              "  'the',\n",
              "  'history',\n",
              "  'of',\n",
              "  'the',\n",
              "  'universe'],\n",
              " ['shitty',\n",
              "  'region',\n",
              "  'of',\n",
              "  'country',\n",
              "  'figures',\n",
              "  'it',\n",
              "  'might',\n",
              "  'as',\n",
              "  'well',\n",
              "  'give',\n",
              "  'producing',\n",
              "  'wine',\n",
              "  'a',\n",
              "  'shot'],\n",
              " ['nasa',\n",
              "  'receives',\n",
              "  'info',\n",
              "  'on',\n",
              "  'UNK',\n",
              "  \"'s\",\n",
              "  'large',\n",
              "  'UNK',\n",
              "  'UNK',\n",
              "  'from',\n",
              "  'UNK',\n",
              "  'probe',\n",
              "  \"'s\",\n",
              "  'UNK',\n",
              "  ',',\n",
              "  'UNK',\n",
              "  'transmission'],\n",
              " ['life',\n",
              "  ':',\n",
              "  'rules',\n",
              "  'are',\n",
              "  'rules',\n",
              "  ':',\n",
              "  'pope',\n",
              "  'francis',\n",
              "  'has',\n",
              "  'been',\n",
              "  'UNK',\n",
              "  'from',\n",
              "  'the',\n",
              "  'vatican',\n",
              "  'for',\n",
              "  'owning',\n",
              "  'a',\n",
              "  'cat',\n",
              "  'in',\n",
              "  'violation',\n",
              "  'of',\n",
              "  'his',\n",
              "  'lease',\n",
              "  'agreement'],\n",
              " ['trump', ',', 'putin', 'hold', 'first', 'joint', 'press', 'crackdown'],\n",
              " ['potential', 'school', 'shooter', 'UNK', 'down', 'by', 'popular', 'UNK'],\n",
              " ['first',\n",
              "  'openly',\n",
              "  'gay',\n",
              "  'republican',\n",
              "  'on',\n",
              "  'gop',\n",
              "  'platform',\n",
              "  'committee',\n",
              "  ':',\n",
              "  '‘',\n",
              "  'why',\n",
              "  'am',\n",
              "  'i',\n",
              "  'even',\n",
              "  'here',\n",
              "  '?',\n",
              "  '’'],\n",
              " ['trump',\n",
              "  'UNK',\n",
              "  'out',\n",
              "  'at',\n",
              "  'UNK',\n",
              "  'photographer',\n",
              "  'who',\n",
              "  'UNK',\n",
              "  'empty',\n",
              "  'chairs'],\n",
              " ['dick',\n",
              "  'cheney',\n",
              "  'UNK',\n",
              "  'a',\n",
              "  'UNK',\n",
              "  'at',\n",
              "  'request',\n",
              "  'of',\n",
              "  'UNK',\n",
              "  'baron',\n",
              "  'cohen'],\n",
              " ['william',\n",
              "  'barr',\n",
              "  'shows',\n",
              "  'up',\n",
              "  'to',\n",
              "  'congress',\n",
              "  'to',\n",
              "  'testify',\n",
              "  'at',\n",
              "  '3',\n",
              "  'a.m.',\n",
              "  'after',\n",
              "  'reading',\n",
              "  'email',\n",
              "  'wrong'],\n",
              " ['bankrupt',\n",
              "  'toys',\n",
              "  \"'\",\n",
              "  'r',\n",
              "  \"'\",\n",
              "  'us',\n",
              "  'forced',\n",
              "  'to',\n",
              "  'UNK',\n",
              "  'thousands',\n",
              "  'of',\n",
              "  'UNK'],\n",
              " ['new',\n",
              "  'hampshire',\n",
              "  'man',\n",
              "  'loses',\n",
              "  'life',\n",
              "  'savings',\n",
              "  'on',\n",
              "  'carnival',\n",
              "  'game'],\n",
              " ['dad', 'suggests', 'arriving', 'at', 'airport', '14', 'hours', 'early'],\n",
              " ['cracker',\n",
              "  'jack',\n",
              "  '’',\n",
              "  'd',\n",
              "  ':',\n",
              "  'new',\n",
              "  'version',\n",
              "  'of',\n",
              "  'cracker',\n",
              "  'UNK',\n",
              "  'to',\n",
              "  'contain',\n",
              "  'caffeine'],\n",
              " ['is',\n",
              "  'pope',\n",
              "  'francis',\n",
              "  'secretly',\n",
              "  'sneaking',\n",
              "  'out',\n",
              "  'of',\n",
              "  'the',\n",
              "  'vatican',\n",
              "  'at',\n",
              "  'night',\n",
              "  'to',\n",
              "  'give',\n",
              "  'money',\n",
              "  'to',\n",
              "  'the',\n",
              "  'poor',\n",
              "  '?'],\n",
              " ['obama', \"'s\", 'embarrassing', 'UNK', 'album', 'UNK'],\n",
              " ['safe',\n",
              "  'sealed',\n",
              "  'for',\n",
              "  '40',\n",
              "  'years',\n",
              "  'until',\n",
              "  'museum',\n",
              "  'visitor',\n",
              "  'UNK',\n",
              "  'the',\n",
              "  'dial'],\n",
              " ['looking', 'for', 'empathy', '?', 'avoid', 'alabama'],\n",
              " ['obama',\n",
              "  '’',\n",
              "  's',\n",
              "  'between',\n",
              "  'two',\n",
              "  'UNK',\n",
              "  'video',\n",
              "  'gets',\n",
              "  'emmy',\n",
              "  'nomination'],\n",
              " ['‘',\n",
              "  'we',\n",
              "  'get',\n",
              "  'the',\n",
              "  'food',\n",
              "  'and',\n",
              "  'then',\n",
              "  'we',\n",
              "  'eat',\n",
              "  'the',\n",
              "  'food',\n",
              "  'until',\n",
              "  'all',\n",
              "  'the',\n",
              "  'food',\n",
              "  'is',\n",
              "  'gone',\n",
              "  ',',\n",
              "  '’',\n",
              "  'city',\n",
              "  'of',\n",
              "  'chicago',\n",
              "  'announces',\n",
              "  'UNK'],\n",
              " ['comments',\n",
              "  'UNK',\n",
              "  'disabled',\n",
              "  'on',\n",
              "  'youtube',\n",
              "  'video',\n",
              "  'of',\n",
              "  'UNK',\n",
              "  'in',\n",
              "  'yard'],\n",
              " ['‘',\n",
              "  'you',\n",
              "  'know',\n",
              "  ',',\n",
              "  'i',\n",
              "  'UNK',\n",
              "  'it',\n",
              "  'too',\n",
              "  ',',\n",
              "  '’',\n",
              "  'bradley',\n",
              "  'cooper',\n",
              "  'says',\n",
              "  'out',\n",
              "  'loud',\n",
              "  'again',\n",
              "  'to',\n",
              "  'no',\n",
              "  'one',\n",
              "  'in',\n",
              "  'UNK'],\n",
              " ['‘',\n",
              "  'these',\n",
              "  'kids',\n",
              "  'should',\n",
              "  'be',\n",
              "  'in',\n",
              "  'school',\n",
              "  'instead',\n",
              "  'of',\n",
              "  'protesting',\n",
              "  ',',\n",
              "  '’',\n",
              "  'say',\n",
              "  'people',\n",
              "  'so',\n",
              "  'UNK',\n",
              "  'close',\n",
              "  'to',\n",
              "  'getting',\n",
              "  'the',\n",
              "  'point'],\n",
              " ['god',\n",
              "  'pissed',\n",
              "  'after',\n",
              "  'learning',\n",
              "  'cost',\n",
              "  'to',\n",
              "  'replace',\n",
              "  'earth',\n",
              "  \"'s\",\n",
              "  'core'],\n",
              " ['due',\n",
              "  'to',\n",
              "  'over',\n",
              "  'UNK',\n",
              "  'and',\n",
              "  'traffic',\n",
              "  'issues',\n",
              "  'in',\n",
              "  'UNK',\n",
              "  ',',\n",
              "  'massive',\n",
              "  'construction',\n",
              "  'ensues',\n",
              "  '.'],\n",
              " ['expert',\n",
              "  'on',\n",
              "  'international',\n",
              "  'jewish',\n",
              "  'conspiracy',\n",
              "  'has',\n",
              "  'never',\n",
              "  'been',\n",
              "  'more',\n",
              "  'than',\n",
              "  '40',\n",
              "  'miles',\n",
              "  'outside',\n",
              "  'council',\n",
              "  'UNK',\n",
              "  ',',\n",
              "  'iowa'],\n",
              " ['red',\n",
              "  'lobster',\n",
              "  'is',\n",
              "  'giving',\n",
              "  'away',\n",
              "  'UNK',\n",
              "  'UNK',\n",
              "  'packs',\n",
              "  'that',\n",
              "  'will',\n",
              "  'keep',\n",
              "  'your',\n",
              "  'UNK',\n",
              "  'bay',\n",
              "  'biscuits',\n",
              "  'warm'],\n",
              " ['candidate', 'profile', ':', 'rick', 'santorum'],\n",
              " ['silicon',\n",
              "  'valley',\n",
              "  'startup',\n",
              "  'seeks',\n",
              "  'to',\n",
              "  'change',\n",
              "  'the',\n",
              "  'way',\n",
              "  'women',\n",
              "  'flee',\n",
              "  'tech',\n",
              "  'industry'],\n",
              " ['texans', 'UNK', 'for', 'president', \"'s\", 'response', 'to', 'hurricane'],\n",
              " ['abc',\n",
              "  'station',\n",
              "  'UNK',\n",
              "  'UNK',\n",
              "  'book',\n",
              "  'cover',\n",
              "  ':',\n",
              "  '‘',\n",
              "  'all',\n",
              "  'up',\n",
              "  'in',\n",
              "  'my',\n",
              "  'UNK',\n",
              "  '’'],\n",
              " ['kim',\n",
              "  'kardashian',\n",
              "  'tries',\n",
              "  'to',\n",
              "  'escape',\n",
              "  'l.a.',\n",
              "  'in',\n",
              "  'UNK',\n",
              "  'after',\n",
              "  'realizing',\n",
              "  'past',\n",
              "  '12',\n",
              "  'years',\n",
              "  'of',\n",
              "  'life',\n",
              "  'have',\n",
              "  'been',\n",
              "  'tv',\n",
              "  'show'],\n",
              " ['gym', 'teacher', 'secretly', 'hates', 'nerds'],\n",
              " ['UNK',\n",
              "  'UNK',\n",
              "  'human',\n",
              "  'skull',\n",
              "  'set',\n",
              "  'to',\n",
              "  'pass',\n",
              "  'earth',\n",
              "  'after',\n",
              "  'halloween'],\n",
              " ['UNK',\n",
              "  'UNK',\n",
              "  ',',\n",
              "  \"'\",\n",
              "  'says',\n",
              "  'man',\n",
              "  'who',\n",
              "  'spent',\n",
              "  'previous',\n",
              "  'day',\n",
              "  'masturbating',\n",
              "  'in',\n",
              "  'UNK',\n",
              "  'room'],\n",
              " ['42',\n",
              "  'million',\n",
              "  'dead',\n",
              "  'in',\n",
              "  'UNK',\n",
              "  'black',\n",
              "  'friday',\n",
              "  'weekend',\n",
              "  'on',\n",
              "  'record'],\n",
              " ['we',\n",
              "  'regret',\n",
              "  'to',\n",
              "  'inform',\n",
              "  'you',\n",
              "  'that',\n",
              "  'UNK',\n",
              "  'now',\n",
              "  'has',\n",
              "  'a',\n",
              "  'porn',\n",
              "  'parody'],\n",
              " ['anthony', 'UNK', 'UNK', 'due', 'to', 'UNK', 'from', 'UNK', 'flu'],\n",
              " ['life',\n",
              "  ':',\n",
              "  'genius',\n",
              "  'marketing',\n",
              "  ':',\n",
              "  'UNK',\n",
              "  'spring',\n",
              "  'is',\n",
              "  'putting',\n",
              "  'apple',\n",
              "  'juice',\n",
              "  'in',\n",
              "  'its',\n",
              "  'bottles',\n",
              "  'because',\n",
              "  'it',\n",
              "  'tastes',\n",
              "  'better'],\n",
              " ['the',\n",
              "  'incredible',\n",
              "  'story',\n",
              "  'of',\n",
              "  'how',\n",
              "  'an',\n",
              "  'insurance',\n",
              "  'company',\n",
              "  'thinks',\n",
              "  'a',\n",
              "  'man',\n",
              "  'burned',\n",
              "  'his',\n",
              "  'house',\n",
              "  'down',\n",
              "  'from',\n",
              "  'UNK',\n",
              "  'away'],\n",
              " ['kelly',\n",
              "  'UNK',\n",
              "  'makes',\n",
              "  'racial',\n",
              "  'remark',\n",
              "  'while',\n",
              "  'UNK',\n",
              "  'trump',\n",
              "  'for',\n",
              "  'racial',\n",
              "  'remark'],\n",
              " ['should', 'companies', 'UNK', 'unpaid', 'intern', 'fights', '?'],\n",
              " ['UNK',\n",
              "  'adds',\n",
              "  'lane',\n",
              "  'for',\n",
              "  'drivers',\n",
              "  'traveling',\n",
              "  'UNK',\n",
              "  'to',\n",
              "  'stop',\n",
              "  'woman',\n",
              "  'from',\n",
              "  'marrying',\n",
              "  'wrong',\n",
              "  'man'],\n",
              " ['neil', 'armstrong', \"'s\", 'legendary', 'phrase'],\n",
              " ['facebook',\n",
              "  'apologizes',\n",
              "  'for',\n",
              "  'giving',\n",
              "  'mark',\n",
              "  'zuckerberg',\n",
              "  'a',\n",
              "  'platform'],\n",
              " ['homeland',\n",
              "  'security',\n",
              "  'criticized',\n",
              "  'for',\n",
              "  'allowing',\n",
              "  'known',\n",
              "  'killer',\n",
              "  'to',\n",
              "  'stay',\n",
              "  'in',\n",
              "  'country'],\n",
              " ['UNK', 'robots', 'would', 'be', 'mistake', 'say', 'scientists'],\n",
              " ['UNK', 'body', 'causes', 'fire', 'at', 'cemetery'],\n",
              " ['UNK', 'UNK', 'blamed', 'in', 'massive', 'model', 'train', 'crash'],\n",
              " ['high',\n",
              "  'school',\n",
              "  'student',\n",
              "  'taking',\n",
              "  'rejection',\n",
              "  'from',\n",
              "  'UNK',\n",
              "  'college',\n",
              "  'in',\n",
              "  'UNK',\n",
              "  'as',\n",
              "  'if',\n",
              "  'future',\n",
              "  'not',\n",
              "  'over'],\n",
              " ['new',\n",
              "  'york',\n",
              "  'monument',\n",
              "  'honors',\n",
              "  'victims',\n",
              "  'of',\n",
              "  'giant',\n",
              "  'octopus',\n",
              "  'attack',\n",
              "  'that',\n",
              "  'never',\n",
              "  'occurred'],\n",
              " ['UNK', 'named', 'jackson', 'to', 'UNK', 'fire', 'you'],\n",
              " ['school', 'bully', 'not', 'so', 'tough', 'since', 'being', 'molested'],\n",
              " ['it',\n",
              "  'happened',\n",
              "  'to',\n",
              "  'me',\n",
              "  ':',\n",
              "  'my',\n",
              "  'UNK',\n",
              "  'locked',\n",
              "  'me',\n",
              "  'in',\n",
              "  'an',\n",
              "  'exam',\n",
              "  'room',\n",
              "  'and',\n",
              "  'forced',\n",
              "  'me',\n",
              "  'to',\n",
              "  'pray',\n",
              "  'to',\n",
              "  'jesus'],\n",
              " ['cdc',\n",
              "  ':',\n",
              "  'red',\n",
              "  'eyes',\n",
              "  'while',\n",
              "  'swimming',\n",
              "  'caused',\n",
              "  'by',\n",
              "  'urine',\n",
              "  ',',\n",
              "  'not',\n",
              "  'UNK'],\n",
              " ['sarah',\n",
              "  'palin',\n",
              "  'admits',\n",
              "  'she',\n",
              "  'has',\n",
              "  'no',\n",
              "  'idea',\n",
              "  'what',\n",
              "  'she',\n",
              "  '’',\n",
              "  's',\n",
              "  'talking',\n",
              "  'about'],\n",
              " ['temple',\n",
              "  'university',\n",
              "  'receives',\n",
              "  'anonymous',\n",
              "  'donation',\n",
              "  'to',\n",
              "  'build',\n",
              "  'center',\n",
              "  'for',\n",
              "  'UNK',\n",
              "  'rape',\n",
              "  'allegations'],\n",
              " ['steven',\n",
              "  'UNK',\n",
              "  'says',\n",
              "  'netflix',\n",
              "  'films',\n",
              "  'don',\n",
              "  '’',\n",
              "  't',\n",
              "  'deserve',\n",
              "  'oscars'],\n",
              " ['trump',\n",
              "  'raises',\n",
              "  'concerns',\n",
              "  'about',\n",
              "  'impact',\n",
              "  'of',\n",
              "  'violent',\n",
              "  'movies',\n",
              "  ':',\n",
              "  'UNK',\n",
              "  'they',\n",
              "  'have',\n",
              "  'to',\n",
              "  'put',\n",
              "  'a',\n",
              "  'rating',\n",
              "  'system',\n",
              "  'for',\n",
              "  'that',\n",
              "  \"'\"],\n",
              " ['fan',\n",
              "  'who',\n",
              "  'got',\n",
              "  'lions',\n",
              "  'super',\n",
              "  'bowl',\n",
              "  'tattoo',\n",
              "  'still',\n",
              "  'really',\n",
              "  'UNK',\n",
              "  'about',\n",
              "  'chances'],\n",
              " ['UNK',\n",
              "  'v',\n",
              "  'out',\n",
              "  'just',\n",
              "  'in',\n",
              "  'time',\n",
              "  'to',\n",
              "  'be',\n",
              "  'blamed',\n",
              "  'for',\n",
              "  'washington',\n",
              "  'shootings'],\n",
              " ['UNK',\n",
              "  'dutch',\n",
              "  'UNK',\n",
              "  'put',\n",
              "  'UNK',\n",
              "  'UNK',\n",
              "  'of',\n",
              "  'mentally',\n",
              "  'ill',\n",
              "  'suicide',\n",
              "  'victim',\n",
              "  'on',\n",
              "  'full',\n",
              "  'display',\n",
              "  'for',\n",
              "  'world',\n",
              "  'to',\n",
              "  'mock'],\n",
              " ['this', 'gop', 'house', 'candidate', 'proposed', 'UNK', 'the', 'weekend'],\n",
              " ['fat',\n",
              "  'kid',\n",
              "  'successfully',\n",
              "  'avoids',\n",
              "  'UNK',\n",
              "  'by',\n",
              "  'swimming',\n",
              "  'with',\n",
              "  'shirt',\n",
              "  'on'],\n",
              " ['ryan',\n",
              "  'UNK',\n",
              "  'to',\n",
              "  'produce',\n",
              "  'an',\n",
              "  'UNK',\n",
              "  ',',\n",
              "  'drug',\n",
              "  'UNK',\n",
              "  'version',\n",
              "  'of',\n",
              "  'the',\n",
              "  'hollywood',\n",
              "  'classic',\n",
              "  '‘',\n",
              "  'home',\n",
              "  'alone',\n",
              "  '’'],\n",
              " ['austin',\n",
              "  'makes',\n",
              "  'list',\n",
              "  'of',\n",
              "  'list',\n",
              "  'of',\n",
              "  'cities',\n",
              "  'that',\n",
              "  'make',\n",
              "  'every',\n",
              "  'list'],\n",
              " ['UNK', 'UNK', ':', 'UNK', 'contest', 'returns', 'to', 'UNK'],\n",
              " ['trader', 'joe', '’', 's', 'fires', 'employee', 'for', 'UNK', 'smile'],\n",
              " ['the',\n",
              "  'internet',\n",
              "  'finally',\n",
              "  'reaches',\n",
              "  'its',\n",
              "  'UNK',\n",
              "  'as',\n",
              "  'man',\n",
              "  'marrying',\n",
              "  'my',\n",
              "  'little',\n",
              "  'pony',\n",
              "  'character',\n",
              "  'writes',\n",
              "  'angry',\n",
              "  'email',\n",
              "  'to',\n",
              "  'erotic',\n",
              "  'pony',\n",
              "  'artist'],\n",
              " ['the',\n",
              "  'UNK',\n",
              "  ':',\n",
              "  'an',\n",
              "  'octopus',\n",
              "  'was',\n",
              "  'trained',\n",
              "  'to',\n",
              "  'take',\n",
              "  'UNK',\n",
              "  'of',\n",
              "  'aquarium',\n",
              "  'visitors',\n",
              "  'in',\n",
              "  'just',\n",
              "  'three',\n",
              "  'tries'],\n",
              " ['dea',\n",
              "  'UNK',\n",
              "  'lil',\n",
              "  'wayne',\n",
              "  'to',\n",
              "  'use',\n",
              "  'up',\n",
              "  'all',\n",
              "  'drugs',\n",
              "  'in',\n",
              "  'mexico'],\n",
              " ['black',\n",
              "  'man',\n",
              "  'does',\n",
              "  '8',\n",
              "  'years',\n",
              "  ':',\n",
              "  '``',\n",
              "  '‘',\n",
              "  'the',\n",
              "  'onion',\n",
              "  '’',\n",
              "  'looks',\n",
              "  'back',\n",
              "  'at',\n",
              "  'the',\n",
              "  'historic',\n",
              "  'legacy',\n",
              "  'of',\n",
              "  'barack',\n",
              "  'obama',\n",
              "  ',',\n",
              "  'a',\n",
              "  'UNK',\n",
              "  'president',\n",
              "  'for',\n",
              "  'a',\n",
              "  'UNK',\n",
              "  'america',\n",
              "  '.',\n",
              "  \"''\"],\n",
              " ['congress', 'actually', 'does', 'something'],\n",
              " ['states',\n",
              "  'now',\n",
              "  'offering',\n",
              "  'millions',\n",
              "  'in',\n",
              "  'tax',\n",
              "  'breaks',\n",
              "  'to',\n",
              "  'any',\n",
              "  'person',\n",
              "  'who',\n",
              "  'says',\n",
              "  '‘',\n",
              "  'high-tech',\n",
              "  'jobs',\n",
              "  '’'],\n",
              " ['UNK', 'battle', 'furiously', 'over', 'jennifer', 'UNK'],\n",
              " ['study',\n",
              "  'finds',\n",
              "  'health',\n",
              "  'benefits',\n",
              "  'associated',\n",
              "  'with',\n",
              "  'seriously',\n",
              "  'considering',\n",
              "  'going',\n",
              "  'vegetarian',\n",
              "  'for',\n",
              "  'a',\n",
              "  'while',\n",
              "  'now'],\n",
              " ['new',\n",
              "  'pumpkin',\n",
              "  'spice',\n",
              "  'channel',\n",
              "  'to',\n",
              "  'offer',\n",
              "  'UNK',\n",
              "  'hardcore',\n",
              "  'pornography',\n",
              "  '-',\n",
              "  'nsfw'],\n",
              " ['apple', 'releases', 'three', 'new', 'iphones'],\n",
              " ['panic',\n",
              "  'among',\n",
              "  'dozens',\n",
              "  'of',\n",
              "  'female',\n",
              "  'UNK',\n",
              "  'at',\n",
              "  'man',\n",
              "  '’',\n",
              "  's',\n",
              "  'funeral',\n",
              "  'when',\n",
              "  'they',\n",
              "  'discover',\n",
              "  'he',\n",
              "  'had',\n",
              "  'hiv'],\n",
              " ['anthony',\n",
              "  'weiner',\n",
              "  'sends',\n",
              "  'apology',\n",
              "  'UNK',\n",
              "  'to',\n",
              "  'entire',\n",
              "  'clinton',\n",
              "  'campaign'],\n",
              " ['the',\n",
              "  'case',\n",
              "  'for',\n",
              "  'and',\n",
              "  'against',\n",
              "  'getting',\n",
              "  'rid',\n",
              "  'of',\n",
              "  'the',\n",
              "  'UNK'],\n",
              " ['james',\n",
              "  'cameron',\n",
              "  'links',\n",
              "  'find',\n",
              "  'of',\n",
              "  'UNK',\n",
              "  'UNK',\n",
              "  'to',\n",
              "  'lost',\n",
              "  'city',\n",
              "  'of',\n",
              "  'UNK'],\n",
              " ['man', 'on', 'verge', 'of', 'UNK', 'instead', 'turns', 'to', 'god'],\n",
              " ['UNK', 'look', 'on', 'in', 'UNK', 'as', 'god', 'creates', 'world'],\n",
              " ['russian',\n",
              "  'official',\n",
              "  'to',\n",
              "  'hotel',\n",
              "  'critics',\n",
              "  ':',\n",
              "  'we',\n",
              "  'have',\n",
              "  'surveillance',\n",
              "  'videos',\n",
              "  'of',\n",
              "  'you',\n",
              "  'in',\n",
              "  'the',\n",
              "  'bathroom'],\n",
              " ['fcc',\n",
              "  'assures',\n",
              "  'nation',\n",
              "  'their',\n",
              "  'favorite',\n",
              "  'verizon',\n",
              "  'websites',\n",
              "  'wo',\n",
              "  \"n't\",\n",
              "  'be',\n",
              "  'UNK',\n",
              "  'by',\n",
              "  'net',\n",
              "  'neutrality',\n",
              "  'repeal'],\n",
              " ['newborn',\n",
              "  'loses',\n",
              "  'faith',\n",
              "  'in',\n",
              "  'humanity',\n",
              "  'after',\n",
              "  'record',\n",
              "  '6',\n",
              "  'days'],\n",
              " ['top',\n",
              "  'UNK',\n",
              "  'physicists',\n",
              "  ',',\n",
              "  'r',\n",
              "  '&',\n",
              "  '&',\n",
              "  'b',\n",
              "  'UNK',\n",
              "  'meet',\n",
              "  'to',\n",
              "  'debate',\n",
              "  'UNK',\n",
              "  'of',\n",
              "  'forever'],\n",
              " ['bored',\n",
              "  'journalists',\n",
              "  'camping',\n",
              "  'outside',\n",
              "  'hospital',\n",
              "  'where',\n",
              "  'kate',\n",
              "  'will',\n",
              "  'give',\n",
              "  'birth',\n",
              "  'resort',\n",
              "  'to',\n",
              "  'UNK',\n",
              "  'each',\n",
              "  'UNK'],\n",
              " ['john',\n",
              "  'kerry',\n",
              "  'scrambles',\n",
              "  'to',\n",
              "  'stop',\n",
              "  'UNK',\n",
              "  'UNK',\n",
              "  'UNK',\n",
              "  'as',\n",
              "  'russian',\n",
              "  'UNK',\n",
              "  'taunts',\n",
              "  'him',\n",
              "  'from',\n",
              "  'bank',\n",
              "  'of',\n",
              "  'monitors'],\n",
              " ['furry', 'orgy', 'breaks', 'out', 'at', 'UNK', \"'\", 'premiere', '.'],\n",
              " ['fema',\n",
              "  'UNK',\n",
              "  'emergency',\n",
              "  'UNK',\n",
              "  'pills',\n",
              "  'for',\n",
              "  'residents',\n",
              "  'stranded',\n",
              "  'by',\n",
              "  'hurricane',\n",
              "  'florence'],\n",
              " ['new',\n",
              "  'uber',\n",
              "  'update',\n",
              "  'allows',\n",
              "  'users',\n",
              "  'to',\n",
              "  'file',\n",
              "  'lawsuit',\n",
              "  'against',\n",
              "  'company',\n",
              "  'directly',\n",
              "  'in',\n",
              "  'app'],\n",
              " ['u.k.', 'passes', 'bill', 'making', '‘', 'UNK', '’', 'illegal'],\n",
              " ['class', 'is', 'UNK'],\n",
              " ['nyc', 'UNK', 'of', 'brains', 'UNK'],\n",
              " ['nintendo',\n",
              "  'never',\n",
              "  'should',\n",
              "  'have',\n",
              "  'UNK',\n",
              "  'to',\n",
              "  'women',\n",
              "  'and',\n",
              "  'created',\n",
              "  'a',\n",
              "  'female',\n",
              "  'mario'],\n",
              " ['madden',\n",
              "  'UNK',\n",
              "  'predicts',\n",
              "  'patriots',\n",
              "  'will',\n",
              "  'beat',\n",
              "  'the',\n",
              "  'seahawks',\n",
              "  'UNK',\n",
              "  'in',\n",
              "  'the',\n",
              "  'super',\n",
              "  'bowl',\n",
              "  'UNK',\n",
              "  'that',\n",
              "  \"'s\",\n",
              "  'exactly',\n",
              "  'what',\n",
              "  'happened'],\n",
              " ['onion',\n",
              "  'writer',\n",
              "  '’',\n",
              "  's',\n",
              "  'UNK',\n",
              "  'UNK',\n",
              "  'UNK',\n",
              "  'headline',\n",
              "  'hits',\n",
              "  'close',\n",
              "  'to',\n",
              "  'home'],\n",
              " ['high',\n",
              "  'school',\n",
              "  'teaches',\n",
              "  'parenting',\n",
              "  'skills',\n",
              "  'by',\n",
              "  'having',\n",
              "  'students',\n",
              "  'post',\n",
              "  'UNK',\n",
              "  'photos',\n",
              "  'of',\n",
              "  'egg',\n",
              "  'to',\n",
              "  'social',\n",
              "  'media'],\n",
              " ['scientific',\n",
              "  'community',\n",
              "  'baffled',\n",
              "  'by',\n",
              "  'man',\n",
              "  'whose',\n",
              "  'UNK',\n",
              "  '32',\n",
              "  'with',\n",
              "  'some',\n",
              "  'pants',\n",
              "  ',',\n",
              "  '33',\n",
              "  'with',\n",
              "  'others'],\n",
              " ['gay',\n",
              "  'couple',\n",
              "  'accepts',\n",
              "  'responsibility',\n",
              "  'for',\n",
              "  'every',\n",
              "  'major',\n",
              "  'disaster',\n",
              "  'of',\n",
              "  'the',\n",
              "  'last',\n",
              "  '50',\n",
              "  'years'],\n",
              " ['boulder',\n",
              "  ',',\n",
              "  'colorado',\n",
              "  ',',\n",
              "  'named',\n",
              "  'best',\n",
              "  'place',\n",
              "  'to',\n",
              "  'raise',\n",
              "  'abducted',\n",
              "  'children'],\n",
              " ['[',\n",
              "  'meta',\n",
              "  ']',\n",
              "  'with',\n",
              "  'our',\n",
              "  'recent',\n",
              "  'UNK',\n",
              "  'growth',\n",
              "  ',',\n",
              "  'i',\n",
              "  \"'ve\",\n",
              "  'decided',\n",
              "  'to',\n",
              "  'UNK',\n",
              "  'and',\n",
              "  'UNK',\n",
              "  'the',\n",
              "  'UNK',\n",
              "  '.',\n",
              "  'your',\n",
              "  'feedback',\n",
              "  'is',\n",
              "  'welcome',\n",
              "  '.'],\n",
              " ['gang',\n",
              "  'of',\n",
              "  'UNK',\n",
              "  'UNK',\n",
              "  'UNK',\n",
              "  'UNK',\n",
              "  'by',\n",
              "  'UNK',\n",
              "  'police',\n",
              "  'following',\n",
              "  'UNK',\n",
              "  'shop',\n",
              "  'assault'],\n",
              " ['trump', 'will', 'lose', ',', 'or', 'i', 'will', 'eat', 'this', 'column'],\n",
              " ['UNK', 'UNK', 'announces', 'las', 'vegas', 'UNK'],\n",
              " ['4',\n",
              "  'UNK',\n",
              "  'realized',\n",
              "  'hours',\n",
              "  'into',\n",
              "  'brain',\n",
              "  'surgery',\n",
              "  'that',\n",
              "  'they',\n",
              "  'were',\n",
              "  'UNK',\n",
              "  'on',\n",
              "  'the',\n",
              "  'wrong',\n",
              "  'patient'],\n",
              " ['man',\n",
              "  'in',\n",
              "  'rental',\n",
              "  'car',\n",
              "  'spends',\n",
              "  '20',\n",
              "  'minutes',\n",
              "  'trying',\n",
              "  'to',\n",
              "  'find',\n",
              "  'UNK',\n",
              "  'wheel'],\n",
              " ['proposed',\n",
              "  'new',\n",
              "  'military',\n",
              "  'branch',\n",
              "  'would',\n",
              "  'send',\n",
              "  'us',\n",
              "  'troops',\n",
              "  'to',\n",
              "  'guard',\n",
              "  'the',\n",
              "  'galaxy'],\n",
              " ['raccoon',\n",
              "  'crushed',\n",
              "  'to',\n",
              "  'death',\n",
              "  'by',\n",
              "  'garbage',\n",
              "  'truck',\n",
              "  'hits',\n",
              "  'jackpot',\n",
              "  'with',\n",
              "  'reincarnation'],\n",
              " ['2016', 'in', 'entertainment'],\n",
              " ['UNK', 'ben', 'UNK', 'claims'],\n",
              " ['UNK',\n",
              "  'isis',\n",
              "  'leader',\n",
              "  'credits',\n",
              "  'promotion',\n",
              "  'entirely',\n",
              "  'to',\n",
              "  'drone',\n",
              "  'strikes'],\n",
              " ['donald',\n",
              "  'trump',\n",
              "  'officially',\n",
              "  'names',\n",
              "  'obamacare',\n",
              "  'replacement',\n",
              "  \"'world\",\n",
              "  \"'s\",\n",
              "  'greatest',\n",
              "  'healthcare',\n",
              "  'plan',\n",
              "  'of',\n",
              "  '2017',\n",
              "  '’'],\n",
              " ['north', 'korean', 'UNK', 'says', 'kim', 'jong-un', 'won', '’', 't', 'last'],\n",
              " ['study',\n",
              "  ':',\n",
              "  'congress',\n",
              "  'literally',\n",
              "  'doesn',\n",
              "  '’',\n",
              "  't',\n",
              "  'care',\n",
              "  'what',\n",
              "  'you',\n",
              "  'think'],\n",
              " ['tough',\n",
              "  'farewell',\n",
              "  ':',\n",
              "  'man',\n",
              "  'UNK',\n",
              "  'cooks',\n",
              "  'breakfast',\n",
              "  'UNK',\n",
              "  'on',\n",
              "  'the',\n",
              "  'UNK',\n",
              "  'hot',\n",
              "  'asteroid',\n",
              "  'that',\n",
              "  'landed',\n",
              "  'on',\n",
              "  'his',\n",
              "  'elderly',\n",
              "  'mother'],\n",
              " ['man', 'pretty', 'UNK', 'since', 'beating', 'cancer'],\n",
              " ['football',\n",
              "  'program',\n",
              "  'in',\n",
              "  'jeopardy',\n",
              "  'after',\n",
              "  'high',\n",
              "  'school',\n",
              "  'UNK',\n",
              "  '$',\n",
              "  '500,000',\n",
              "  'to',\n",
              "  '‘',\n",
              "  'little',\n",
              "  'women',\n",
              "  '’',\n",
              "  'production'],\n",
              " ['fbi',\n",
              "  'warns',\n",
              "  'republican',\n",
              "  'UNK',\n",
              "  'could',\n",
              "  'UNK',\n",
              "  'faith',\n",
              "  'in',\n",
              "  'massive',\n",
              "  ',',\n",
              "  'UNK',\n",
              "  'government',\n",
              "  'secret',\n",
              "  'agencies'],\n",
              " ['4',\n",
              "  'times',\n",
              "  'mark',\n",
              "  'cuban',\n",
              "  'UNK',\n",
              "  'offered',\n",
              "  'a',\n",
              "  'contestant',\n",
              "  'a',\n",
              "  'deal',\n",
              "  'on',\n",
              "  '‘',\n",
              "  'shark',\n",
              "  'tank',\n",
              "  '’',\n",
              "  'because',\n",
              "  'the',\n",
              "  'contestant',\n",
              "  '’',\n",
              "  's',\n",
              "  'UNK',\n",
              "  'story',\n",
              "  'was',\n",
              "  'about',\n",
              "  'the',\n",
              "  'time',\n",
              "  'he',\n",
              "  'hit',\n",
              "  'them',\n",
              "  'with',\n",
              "  'his',\n",
              "  'car'],\n",
              " ['peru',\n",
              "  'haunted',\n",
              "  'by',\n",
              "  '7',\n",
              "  '%',\n",
              "  'UNK',\n",
              "  'UNK',\n",
              "  'UNK',\n",
              "  'UNK',\n",
              "  'UNK',\n",
              "  'discovery'],\n",
              " ['teacher',\n",
              "  'in',\n",
              "  'cash-strapped',\n",
              "  'ohio',\n",
              "  'school',\n",
              "  'district',\n",
              "  'forced',\n",
              "  'to',\n",
              "  'make',\n",
              "  'do',\n",
              "  'with',\n",
              "  'UNK',\n",
              "  'firearms'],\n",
              " ['UNK',\n",
              "  ',',\n",
              "  'UNK',\n",
              "  'grandpa',\n",
              "  'heads',\n",
              "  'into',\n",
              "  'town',\n",
              "  'to',\n",
              "  'UNK',\n",
              "  'in',\n",
              "  'UNK',\n",
              "  'UNK'],\n",
              " ['UNK', 'receives', 'honorary', 'degree', 'from', 'harvard'],\n",
              " ['we',\n",
              "  'already',\n",
              "  'knew',\n",
              "  'the',\n",
              "  'nsa',\n",
              "  'spies',\n",
              "  'on',\n",
              "  'us',\n",
              "  '.',\n",
              "  'we',\n",
              "  'already',\n",
              "  'know',\n",
              "  'everything',\n",
              "  '.',\n",
              "  'everything',\n",
              "  'is',\n",
              "  'boring',\n",
              "  '.'],\n",
              " ['do',\n",
              "  'obama',\n",
              "  \"'s\",\n",
              "  'small',\n",
              "  'UNK',\n",
              "  'explain',\n",
              "  'his',\n",
              "  'liberal',\n",
              "  'politics',\n",
              "  '?'],\n",
              " ['grand',\n",
              "  'UNK',\n",
              "  'museum',\n",
              "  'reportedly',\n",
              "  'had',\n",
              "  'UNK',\n",
              "  'of',\n",
              "  'uranium',\n",
              "  'sitting',\n",
              "  'around',\n",
              "  'for',\n",
              "  '18',\n",
              "  'years'],\n",
              " ['cnn',\n",
              "  'UNK',\n",
              "  'warn',\n",
              "  'north',\n",
              "  'korea',\n",
              "  'situation',\n",
              "  'way',\n",
              "  'too',\n",
              "  'complex',\n",
              "  'for',\n",
              "  'them',\n",
              "  'to',\n",
              "  'discuss',\n",
              "  'UNK'],\n",
              " ['cam', 'girl', 'has', 'UNK', 'on', 'forehead'],\n",
              " ['UNK', 'UNK', 'evolution'],\n",
              " ['UNK',\n",
              "  'UNK',\n",
              "  'says',\n",
              "  'hillary',\n",
              "  'UNK',\n",
              "  'clinton',\n",
              "  'high',\n",
              "  'palace',\n",
              "  'of',\n",
              "  'the',\n",
              "  'solar',\n",
              "  'order',\n",
              "  'was',\n",
              "  'almost',\n",
              "  'like',\n",
              "  'a',\n",
              "  'cult'],\n",
              " ['elderly',\n",
              "  'woman',\n",
              "  'spends',\n",
              "  'day',\n",
              "  'in',\n",
              "  'park',\n",
              "  'feeding',\n",
              "  'pigeons',\n",
              "  'UNK',\n",
              "  'husband'],\n",
              " ['UNK',\n",
              "  'bosses',\n",
              "  'are',\n",
              "  'cutting',\n",
              "  'down',\n",
              "  'and',\n",
              "  'burning',\n",
              "  'UNK',\n",
              "  'to',\n",
              "  'find',\n",
              "  'out',\n",
              "  'how',\n",
              "  'many',\n",
              "  'UNK',\n",
              "  'they',\n",
              "  'kill'],\n",
              " ['donald', 'trump', 'defends', 'size', 'of', 'his', 'penis'],\n",
              " ['bush',\n",
              "  ':',\n",
              "  'UNK',\n",
              "  'long',\n",
              "  'national',\n",
              "  'nightmare',\n",
              "  'of',\n",
              "  'peace',\n",
              "  'and',\n",
              "  'UNK',\n",
              "  'is',\n",
              "  'finally',\n",
              "  'over',\n",
              "  \"'\"],\n",
              " ['a',\n",
              "  'UNK',\n",
              "  'may',\n",
              "  'have',\n",
              "  'just',\n",
              "  'killed',\n",
              "  'someone',\n",
              "  'for',\n",
              "  'the',\n",
              "  'first',\n",
              "  'time',\n",
              "  'on',\n",
              "  'record'],\n",
              " ['study', 'finds', 'girls', 'UNK', 'future', 'employers', 'in', 'school'],\n",
              " ['scandal',\n",
              "  '!',\n",
              "  'player',\n",
              "  'caught',\n",
              "  'hiding',\n",
              "  'blank',\n",
              "  'UNK',\n",
              "  'at',\n",
              "  'UNK',\n",
              "  'national',\n",
              "  'championships'],\n",
              " ['dead',\n",
              "  'or',\n",
              "  'alive',\n",
              "  'UNK',\n",
              "  'pc',\n",
              "  'will',\n",
              "  'UNK',\n",
              "  'UNK',\n",
              "  'physics',\n",
              "  'on',\n",
              "  'UNK',\n",
              "  'UNK'],\n",
              " ['the',\n",
              "  'average',\n",
              "  'american',\n",
              "  'will',\n",
              "  'spend',\n",
              "  '43',\n",
              "  'days',\n",
              "  'of',\n",
              "  'his',\n",
              "  'life',\n",
              "  'on',\n",
              "  'hold'],\n",
              " ['stephen',\n",
              "  'hawking',\n",
              "  '’',\n",
              "  's',\n",
              "  'son',\n",
              "  ':',\n",
              "  'i',\n",
              "  'programmed',\n",
              "  'swear',\n",
              "  'words',\n",
              "  'into',\n",
              "  'my',\n",
              "  'dad',\n",
              "  '’',\n",
              "  's',\n",
              "  'voice',\n",
              "  'box'],\n",
              " ['a',\n",
              "  'reunion',\n",
              "  'in',\n",
              "  'the',\n",
              "  'works',\n",
              "  '?',\n",
              "  'robert',\n",
              "  'plant',\n",
              "  'and',\n",
              "  'jimmy',\n",
              "  'page',\n",
              "  'were',\n",
              "  'spotted',\n",
              "  'at',\n",
              "  'guitar',\n",
              "  'center',\n",
              "  'buying',\n",
              "  'a',\n",
              "  'led',\n",
              "  'UNK',\n",
              "  'UNK',\n",
              "  'book'],\n",
              " ['gay',\n",
              "  'UNK',\n",
              "  'has',\n",
              "  'improved',\n",
              "  'but',\n",
              "  'still',\n",
              "  'has',\n",
              "  'goat',\n",
              "  'panties',\n",
              "  '.'],\n",
              " ['must',\n",
              "  'see',\n",
              "  ':',\n",
              "  'reform',\n",
              "  'at',\n",
              "  'last',\n",
              "  ':',\n",
              "  'after',\n",
              "  'the',\n",
              "  'parkland',\n",
              "  'shooting',\n",
              "  'this',\n",
              "  'florida',\n",
              "  'gun',\n",
              "  'store',\n",
              "  'changed',\n",
              "  'its',\n",
              "  'sign',\n",
              "  'from',\n",
              "  '“',\n",
              "  'guns',\n",
              "  '!',\n",
              "  'guns',\n",
              "  '!',\n",
              "  'guns',\n",
              "  '!',\n",
              "  '”',\n",
              "  'to',\n",
              "  '“',\n",
              "  'guns',\n",
              "  '.',\n",
              "  'guns',\n",
              "  '.',\n",
              "  'guns',\n",
              "  '.',\n",
              "  '”'],\n",
              " ['rex',\n",
              "  'tillerson',\n",
              "  ':',\n",
              "  'state',\n",
              "  'department',\n",
              "  'can',\n",
              "  'be',\n",
              "  'cut',\n",
              "  'as',\n",
              "  'we',\n",
              "  'will',\n",
              "  'soon',\n",
              "  'solve',\n",
              "  'global',\n",
              "  'UNK'],\n",
              " ['carjackers',\n",
              "  'UNK',\n",
              "  'by',\n",
              "  'series',\n",
              "  'of',\n",
              "  'potholes',\n",
              "  'on',\n",
              "  'east',\n",
              "  'side',\n",
              "  'of',\n",
              "  'indianapolis'],\n",
              " ['jealous',\n",
              "  'paul',\n",
              "  'ryan',\n",
              "  'asks',\n",
              "  'legislator',\n",
              "  'with',\n",
              "  '37',\n",
              "  '%',\n",
              "  'approval',\n",
              "  'rating',\n",
              "  'what',\n",
              "  'his',\n",
              "  'secret',\n",
              "  'is'],\n",
              " ['customer',\n",
              "  'has',\n",
              "  'a',\n",
              "  'heart',\n",
              "  'attack',\n",
              "  'at',\n",
              "  'the',\n",
              "  'heart',\n",
              "  'attack',\n",
              "  'grill',\n",
              "  'in',\n",
              "  'vegas'],\n",
              " ['6', 'UNK', 'before', 'and', 'after', 'photos', 'of', 'meth', 'users'],\n",
              " ['trump',\n",
              "  ':',\n",
              "  'us',\n",
              "  'could',\n",
              "  'use',\n",
              "  'some',\n",
              "  '‘',\n",
              "  'good',\n",
              "  'old',\n",
              "  'global',\n",
              "  'warming',\n",
              "  '’',\n",
              "  'to',\n",
              "  'heat',\n",
              "  'up',\n",
              "  'cold',\n",
              "  'states'],\n",
              " ['teen',\n",
              "  'who',\n",
              "  'worked',\n",
              "  'in',\n",
              "  'corner',\n",
              "  'shop',\n",
              "  'for',\n",
              "  '10',\n",
              "  'weeks',\n",
              "  'to',\n",
              "  'afford',\n",
              "  'christmas',\n",
              "  'presents',\n",
              "  'told',\n",
              "  'he',\n",
              "  '’',\n",
              "  's',\n",
              "  'on',\n",
              "  'unpaid',\n",
              "  'internship'],\n",
              " ['UNK',\n",
              "  'parade',\n",
              "  'UNK',\n",
              "  'covered',\n",
              "  'in',\n",
              "  'tickets',\n",
              "  'after',\n",
              "  'parking',\n",
              "  'on',\n",
              "  '5th',\n",
              "  'UNK',\n",
              "  'over',\n",
              "  'holiday',\n",
              "  'weekend'],\n",
              " ['facebook',\n",
              "  'admits',\n",
              "  'it',\n",
              "  'did',\n",
              "  'not',\n",
              "  'read',\n",
              "  'terms',\n",
              "  'of',\n",
              "  'the',\n",
              "  'app',\n",
              "  'that',\n",
              "  'UNK',\n",
              "  'data',\n",
              "  'of',\n",
              "  '87',\n",
              "  'million'],\n",
              " ['historians',\n",
              "  'suggest',\n",
              "  '‘',\n",
              "  'UNK',\n",
              "  '’',\n",
              "  'youtube',\n",
              "  'clips',\n",
              "  'may',\n",
              "  'be',\n",
              "  'UNK',\n",
              "  'of',\n",
              "  'larger',\n",
              "  'work'],\n",
              " ['progressive', 'UNK', 'school', 'doesn', '’', 't', 'have', 'students'],\n",
              " ['dick', 'cheney', 'feared', 'assassination', 'by', 'UNK', 'hack'],\n",
              " ['3-year-old',\n",
              "  'pretending',\n",
              "  'stuffed',\n",
              "  'animals',\n",
              "  'having',\n",
              "  'big',\n",
              "  'fight',\n",
              "  'about',\n",
              "  'accidental',\n",
              "  'pregnancy'],\n",
              " ['UNK',\n",
              "  'UNK',\n",
              "  'on',\n",
              "  'welfare',\n",
              "  '?',\n",
              "  'gop',\n",
              "  'senator',\n",
              "  '’',\n",
              "  's',\n",
              "  'family',\n",
              "  'UNK',\n",
              "  '$',\n",
              "  'UNK',\n",
              "  'in',\n",
              "  'taxpayer',\n",
              "  'UNK'],\n",
              " ['pat',\n",
              "  'patriot',\n",
              "  'denies',\n",
              "  'being',\n",
              "  'mascot',\n",
              "  '#',\n",
              "  '5',\n",
              "  'in',\n",
              "  'prostitution',\n",
              "  'sting',\n",
              "  'police',\n",
              "  'report'],\n",
              " ['new',\n",
              "  'peta',\n",
              "  'ad',\n",
              "  'about',\n",
              "  'UNK',\n",
              "  'of',\n",
              "  'animal',\n",
              "  'life',\n",
              "  'shows',\n",
              "  'them',\n",
              "  'killing',\n",
              "  'fish'],\n",
              " ['UNK', 'going', 'UNK', 'to', 'avoid', 'humans'],\n",
              " ['biden',\n",
              "  ':',\n",
              "  '‘',\n",
              "  'i',\n",
              "  'may',\n",
              "  'be',\n",
              "  'a',\n",
              "  'white',\n",
              "  'boy',\n",
              "  ',',\n",
              "  'but',\n",
              "  'i',\n",
              "  'can',\n",
              "  'jump',\n",
              "  '’'],\n",
              " ['paul', 'ryan', 'UNK', 'the', 'pope'],\n",
              " ['dollar', 'tree', 'to', 'stop', 'selling', 'assault', 'weapons'],\n",
              " ['farewell',\n",
              "  'to',\n",
              "  'a',\n",
              "  'legend',\n",
              "  ':',\n",
              "  'steve',\n",
              "  'harvey',\n",
              "  'has',\n",
              "  'UNK',\n",
              "  'through',\n",
              "  'the',\n",
              "  'roof',\n",
              "  'of',\n",
              "  'his',\n",
              "  'studio',\n",
              "  'in',\n",
              "  'shock',\n",
              "  'after',\n",
              "  'a',\n",
              "  'guest',\n",
              "  'told',\n",
              "  'him',\n",
              "  'she',\n",
              "  'doesn',\n",
              "  '’',\n",
              "  't',\n",
              "  'make',\n",
              "  'her',\n",
              "  'kids',\n",
              "  'do',\n",
              "  'UNK'],\n",
              " ['michael',\n",
              "  'UNK',\n",
              "  'jordan',\n",
              "  'buys',\n",
              "  'teen',\n",
              "  'a',\n",
              "  'new',\n",
              "  'UNK',\n",
              "  'after',\n",
              "  'she',\n",
              "  'bit',\n",
              "  'through',\n",
              "  'it',\n",
              "  'during',\n",
              "  'his',\n",
              "  'shirtless',\n",
              "  'black',\n",
              "  'panther',\n",
              "  'scene'],\n",
              " ['roy',\n",
              "  'moore',\n",
              "  'on',\n",
              "  'pedophilia',\n",
              "  'accusers',\n",
              "  ':',\n",
              "  '‘',\n",
              "  'these',\n",
              "  'women',\n",
              "  'are',\n",
              "  'only',\n",
              "  'UNK',\n",
              "  'me',\n",
              "  'now',\n",
              "  'because',\n",
              "  'shifting',\n",
              "  'UNK',\n",
              "  'UNK',\n",
              "  'have',\n",
              "  'created',\n",
              "  'an',\n",
              "  'environment',\n",
              "  'in',\n",
              "  'which',\n",
              "  'assault',\n",
              "  'allegations',\n",
              "  'are',\n",
              "  'taken',\n",
              "  'seriously',\n",
              "  '’'],\n",
              " ['amazon',\n",
              "  'completes',\n",
              "  'new',\n",
              "  'suspension',\n",
              "  'tank',\n",
              "  'to',\n",
              "  'house',\n",
              "  'psychic',\n",
              "  'beings',\n",
              "  'who',\n",
              "  'UNK',\n",
              "  'customers',\n",
              "  \"'\",\n",
              "  'future',\n",
              "  'orders'],\n",
              " ['national', 'parks', 'closed', 'for', 'annual', 'UNK'],\n",
              " ['UNK', '%', 'support', 'marijuana', 'legalization'],\n",
              " ['‘',\n",
              "  'the',\n",
              "  'name',\n",
              "  'on',\n",
              "  'my',\n",
              "  'birth',\n",
              "  'certificate',\n",
              "  'is',\n",
              "  'actually',\n",
              "  '“',\n",
              "  'the',\n",
              "  '‘',\n",
              "  'UNK',\n",
              "  'johnson',\n",
              "  '’',\n",
              "  'rock',\n",
              "  ',',\n",
              "  '”',\n",
              "  'but',\n",
              "  'people',\n",
              "  'always',\n",
              "  'get',\n",
              "  'it',\n",
              "  'mixed',\n",
              "  'up',\n",
              "  '’',\n",
              "  ':',\n",
              "  '5',\n",
              "  'questions',\n",
              "  'with',\n",
              "  'UNK',\n",
              "  '‘',\n",
              "  'the',\n",
              "  'rock',\n",
              "  '’',\n",
              "  'johnson'],\n",
              " ['peta',\n",
              "  'condemns',\n",
              "  'bbc',\n",
              "  'for',\n",
              "  'UNK',\n",
              "  'thousands',\n",
              "  'of',\n",
              "  'endangered',\n",
              "  'animals',\n",
              "  'inside',\n",
              "  'tv',\n",
              "  'screens'],\n",
              " ['UNK',\n",
              "  'legendary',\n",
              "  'actor',\n",
              "  'christopher',\n",
              "  'lee',\n",
              "  'set',\n",
              "  'to',\n",
              "  'unleash',\n",
              "  'a',\n",
              "  'metal',\n",
              "  'album',\n",
              "  'next',\n",
              "  'week'],\n",
              " ['dad',\n",
              "  'unknowingly',\n",
              "  'impregnated',\n",
              "  'daughter',\n",
              "  ',',\n",
              "  'who',\n",
              "  'UNK',\n",
              "  'how',\n",
              "  'to',\n",
              "  'tell',\n",
              "  'their',\n",
              "  'son',\n",
              "  'his',\n",
              "  'grandfather',\n",
              "  'is',\n",
              "  'his',\n",
              "  'father'],\n",
              " [\"'the\",\n",
              "  'internship',\n",
              "  \"'\",\n",
              "  'poised',\n",
              "  'to',\n",
              "  'be',\n",
              "  'biggest',\n",
              "  'comedy',\n",
              "  'of',\n",
              "  '2005'],\n",
              " ['great',\n",
              "  ',',\n",
              "  'daughter',\n",
              "  'UNK',\n",
              "  'UNK',\n",
              "  'against',\n",
              "  'some',\n",
              "  '13-year-old',\n",
              "  'named',\n",
              "  'UNK',\n",
              "  'now'],\n",
              " ['mcdonald',\n",
              "  '’',\n",
              "  's',\n",
              "  'introduces',\n",
              "  'new',\n",
              "  'UNK',\n",
              "  'uniforms',\n",
              "  'for',\n",
              "  'our',\n",
              "  'modern',\n",
              "  'UNK'],\n",
              " ['scott',\n",
              "  'walker',\n",
              "  '’',\n",
              "  's',\n",
              "  'office',\n",
              "  'unable',\n",
              "  'to',\n",
              "  'provide',\n",
              "  'written',\n",
              "  'proof',\n",
              "  'of',\n",
              "  'his',\n",
              "  'communications',\n",
              "  'with',\n",
              "  'god'],\n",
              " ['double',\n",
              "  'tragedy',\n",
              "  ':',\n",
              "  'when',\n",
              "  'this',\n",
              "  'frat',\n",
              "  'pledge',\n",
              "  'had',\n",
              "  'to',\n",
              "  'be',\n",
              "  'rushed',\n",
              "  'to',\n",
              "  'the',\n",
              "  'hospital',\n",
              "  'for',\n",
              "  'alcohol',\n",
              "  'poisoning',\n",
              "  ',',\n",
              "  'frat',\n",
              "  'brothers',\n",
              "  'UNK',\n",
              "  'his',\n",
              "  'doctor',\n",
              "  'into',\n",
              "  'drinking',\n",
              "  'a',\n",
              "  'handle',\n",
              "  'of',\n",
              "  'UNK',\n",
              "  'and',\n",
              "  'they',\n",
              "  'both',\n",
              "  'died'],\n",
              " ['hundreds',\n",
              "  'of',\n",
              "  'people',\n",
              "  'gather',\n",
              "  'to',\n",
              "  'say',\n",
              "  '‘',\n",
              "  'wow',\n",
              "  '’',\n",
              "  'like',\n",
              "  'UNK',\n",
              "  'wilson'],\n",
              " ['nfl',\n",
              "  'adds',\n",
              "  'passing',\n",
              "  'concussion',\n",
              "  'UNK',\n",
              "  'to',\n",
              "  'pro',\n",
              "  'bowl',\n",
              "  'skills',\n",
              "  'competition'],\n",
              " ['news',\n",
              "  ':',\n",
              "  'helping',\n",
              "  'all',\n",
              "  'americans',\n",
              "  ':',\n",
              "  'the',\n",
              "  'gop',\n",
              "  'tax',\n",
              "  'cuts',\n",
              "  'are',\n",
              "  'set',\n",
              "  'to',\n",
              "  'benefit',\n",
              "  'working-class',\n",
              "  'mothers',\n",
              "  'earning',\n",
              "  '$',\n",
              "  '25,000',\n",
              "  'a',\n",
              "  'year',\n",
              "  'who',\n",
              "  'are',\n",
              "  'married',\n",
              "  'to',\n",
              "  'the',\n",
              "  'entire',\n",
              "  'wells',\n",
              "  'fargo',\n",
              "  'board',\n",
              "  'of',\n",
              "  'directors'],\n",
              " ['study',\n",
              "  ':',\n",
              "  'stephen',\n",
              "  'colbert',\n",
              "  'more',\n",
              "  'effective',\n",
              "  'than',\n",
              "  'journalists',\n",
              "  'at',\n",
              "  'explaining',\n",
              "  'campaign',\n",
              "  'UNK',\n",
              "  'during',\n",
              "  'last',\n",
              "  'election',\n",
              "  'cycle'],\n",
              " ['conservative', 'acquaintance', 'UNK', 'not', 'racist'],\n",
              " ['hillary',\n",
              "  'on',\n",
              "  'snapchat',\n",
              "  ':',\n",
              "  'i',\n",
              "  'love',\n",
              "  'how',\n",
              "  'messages',\n",
              "  'UNK',\n",
              "  'all',\n",
              "  'by',\n",
              "  'themselves',\n",
              "  \"'\"],\n",
              " ['fox',\n",
              "  'news',\n",
              "  'was',\n",
              "  'attacking',\n",
              "  'barack',\n",
              "  'obama',\n",
              "  'for',\n",
              "  'using',\n",
              "  'UNK',\n",
              "  'UNK',\n",
              "  'at',\n",
              "  'this',\n",
              "  'point',\n",
              "  'in',\n",
              "  'his',\n",
              "  'presidency'],\n",
              " ['2',\n",
              "  'teens',\n",
              "  'hit',\n",
              "  'while',\n",
              "  'UNK',\n",
              "  'in',\n",
              "  'the',\n",
              "  'middle',\n",
              "  'of',\n",
              "  'the',\n",
              "  'road'],\n",
              " ['UNK',\n",
              "  'UNK',\n",
              "  'rocks',\n",
              "  'back',\n",
              "  'and',\n",
              "  'UNK',\n",
              "  'in',\n",
              "  'UNK',\n",
              "  'while',\n",
              "  'watching',\n",
              "  'arby',\n",
              "  '’',\n",
              "  's',\n",
              "  'clap',\n",
              "  'back',\n",
              "  'at',\n",
              "  'burger',\n",
              "  'king',\n",
              "  'on',\n",
              "  'twitter'],\n",
              " ['scientists',\n",
              "  'make',\n",
              "  'unclear',\n",
              "  'breakthrough',\n",
              "  'after',\n",
              "  'giving',\n",
              "  'robot',\n",
              "  'cancer'],\n",
              " ['stars', 'and', 'UNK'],\n",
              " ['UNK',\n",
              "  'google',\n",
              "  'glass',\n",
              "  'UNK',\n",
              "  'to',\n",
              "  'be',\n",
              "  'donated',\n",
              "  'to',\n",
              "  'assholes',\n",
              "  'in',\n",
              "  'africa'],\n",
              " ['facebook',\n",
              "  ':',\n",
              "  '‘',\n",
              "  'UNK',\n",
              "  'hate',\n",
              "  'speech',\n",
              "  'is',\n",
              "  'difficult',\n",
              "  'because',\n",
              "  'some',\n",
              "  'posts',\n",
              "  'actually',\n",
              "  'make',\n",
              "  'pretty',\n",
              "  'interesting',\n",
              "  'points',\n",
              "  '’'],\n",
              " ['pope',\n",
              "  'UNK',\n",
              "  'through',\n",
              "  'vatican',\n",
              "  'basement',\n",
              "  'for',\n",
              "  'plastic',\n",
              "  'nativity',\n",
              "  'scene',\n",
              "  'figures'],\n",
              " ['UNK',\n",
              "  'UNK',\n",
              "  'to',\n",
              "  'be',\n",
              "  'UNK',\n",
              "  'at',\n",
              "  'night',\n",
              "  'out',\n",
              "  'of',\n",
              "  'respect',\n",
              "  'for',\n",
              "  'UNK'],\n",
              " ['not',\n",
              "  'again',\n",
              "  ':',\n",
              "  'bus',\n",
              "  'carrying',\n",
              "  'passengers',\n",
              "  'from',\n",
              "  'nightmare',\n",
              "  'cruise',\n",
              "  'breaks',\n",
              "  'down',\n",
              "  'on',\n",
              "  'way',\n",
              "  'to',\n",
              "  'new',\n",
              "  'orleans'],\n",
              " ['adult',\n",
              "  'film',\n",
              "  'industry',\n",
              "  'replaces',\n",
              "  '500',\n",
              "  'porn',\n",
              "  'stars',\n",
              "  'with',\n",
              "  'UNK',\n",
              "  'robotic',\n",
              "  'UNK',\n",
              "  'arm'],\n",
              " ['UNK',\n",
              "  'UNK',\n",
              "  'shares',\n",
              "  'laugh',\n",
              "  'with',\n",
              "  'military',\n",
              "  'leaders',\n",
              "  'over',\n",
              "  'time',\n",
              "  'he',\n",
              "  'once',\n",
              "  'wanted',\n",
              "  'to',\n",
              "  'be',\n",
              "  'a',\n",
              "  'doctor',\n",
              "  'and',\n",
              "  'help',\n",
              "  'people'],\n",
              " ['less',\n",
              "  'popular',\n",
              "  'friend',\n",
              "  'proposes',\n",
              "  'UNK',\n",
              "  'UNK',\n",
              "  'into',\n",
              "  'single',\n",
              "  'party'],\n",
              " ['the',\n",
              "  'average',\n",
              "  'american',\n",
              "  'worker',\n",
              "  'takes',\n",
              "  'less',\n",
              "  'vacation',\n",
              "  'time',\n",
              "  'than',\n",
              "  'a',\n",
              "  'medieval',\n",
              "  'UNK'],\n",
              " ['good', 'smell', 'UNK', 'new', 'yorkers'],\n",
              " ['5',\n",
              "  'unrealistic',\n",
              "  'expectations',\n",
              "  'porn',\n",
              "  'creates',\n",
              "  'in',\n",
              "  'UNK',\n",
              "  'between',\n",
              "  'horny',\n",
              "  'UNK',\n",
              "  'and',\n",
              "  'their',\n",
              "  'UNK',\n",
              "  'UNK'],\n",
              " ['large',\n",
              "  'UNK',\n",
              "  'of',\n",
              "  'UNK',\n",
              "  'maple',\n",
              "  'syrup',\n",
              "  'reserve',\n",
              "  'has',\n",
              "  'gone',\n",
              "  'missing'],\n",
              " ['UNK', 'reports', 'record', 'UNK'],\n",
              " ['heartbreaking',\n",
              "  ':',\n",
              "  'ellen',\n",
              "  'degeneres',\n",
              "  'is',\n",
              "  'trying',\n",
              "  'to',\n",
              "  'give',\n",
              "  'away',\n",
              "  '$',\n",
              "  '500,000',\n",
              "  'to',\n",
              "  'every',\n",
              "  'fan',\n",
              "  'who',\n",
              "  'comments',\n",
              "  '‘',\n",
              "  'UNK',\n",
              "  '’',\n",
              "  'on',\n",
              "  'a',\n",
              "  'poorly',\n",
              "  'UNK',\n",
              "  'facebook',\n",
              "  'post',\n",
              "  ',',\n",
              "  'but',\n",
              "  'everyone',\n",
              "  'thinks',\n",
              "  'it',\n",
              "  '’',\n",
              "  's',\n",
              "  'a',\n",
              "  'scam'],\n",
              " ['u.s.',\n",
              "  'military',\n",
              "  'defends',\n",
              "  'controversial',\n",
              "  'decision',\n",
              "  'to',\n",
              "  'test',\n",
              "  'UNK',\n",
              "  'volcano',\n",
              "  'on',\n",
              "  'UNK',\n",
              "  'civilians'],\n",
              " ['UNK', 'UNK', 'balloons', 'launched', 'at', 'north', 'korea'],\n",
              " ['kanye',\n",
              "  'west',\n",
              "  'doesn',\n",
              "  '’',\n",
              "  't',\n",
              "  'like',\n",
              "  'appearing',\n",
              "  'on',\n",
              "  'keeping',\n",
              "  'up',\n",
              "  'with',\n",
              "  'the',\n",
              "  'kardashians',\n",
              "  'because',\n",
              "  'he',\n",
              "  'has',\n",
              "  'issues',\n",
              "  'with',\n",
              "  'the',\n",
              "  'UNK'],\n",
              " ['india',\n",
              "  'continues',\n",
              "  'surge',\n",
              "  'towards',\n",
              "  'status',\n",
              "  'as',\n",
              "  'first',\n",
              "  'world',\n",
              "  'nation',\n",
              "  'by',\n",
              "  'UNK',\n",
              "  'racist',\n",
              "  ',',\n",
              "  'right-wing',\n",
              "  'UNK'],\n",
              " ['coworkers',\n",
              "  'nationwide',\n",
              "  'UNK',\n",
              "  'UNK',\n",
              "  'after',\n",
              "  'painful',\n",
              "  'UNK',\n",
              "  'separation'],\n",
              " ['‘', 'star', 'wars', 'UNK', '’', 'trailer', 'released']]"
            ]
          },
          "metadata": {},
          "execution_count": 161
        }
      ],
      "source": [
        "val_train_loop_incorrect(model, test_iterator)"
      ],
      "id": "6-azPje88iU0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUoLTGAScre7"
      },
      "source": [
        "Now that we have our incorrect sequences:   \n",
        "**Question:** *Can you describe what cases the model is getting wrong in the witheld test-set?*\n",
        "\n",
        "**Answer:** The model is having trouble recognizing unknown vocabulary since a common theme in all the sequences this model is getting wrong is the appearence of the \"UNK\" token. This means that the vocab cutoff removed many lower frequency words, which may have lead to a loss of important context and meaning in these test cases. This causes the data to have sentences that do not make coherent sense or are semintically incomplete because the cutoff threshold caused us to remove important aspects of the sentnce like proper nouns, domain specific language, or even slang.\n",
        "\n",
        "Some examples of this include, the sentence \"cops bust filthy, UNK mark zuckerberg for selling personal data on street corner,\" where it is not clear what \"UNK\" is replacing. Given the setence, it can be an important adjective or proper noun that can change the meaning of the sentence completely. Similarly, in the example \"'game of thrones' UNK demands trial by combat in new york supreme court case\" the missing word could refer to domain specific (in this case game of thrones specific) language or it could be another word that could provide context on the humor in the sentence. Those examples show that the model is struggling with unknown vocab especially with proper nouns and domain specific language."
      ],
      "id": "mUoLTGAScre7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mY8S9ZK9zuVs"
      },
      "source": [
        "## 6. Submit Your Homework\n",
        "This is the end of Project 1. Congratulations!  \n",
        "\n",
        "Now, follow the steps below to submit your homework in [Gradescope](https://www.gradescope.com/courses/939466):\n",
        "\n",
        "1. Rename this ipynb file to `CS4650_p1_GTusername.ipynb`. Make sure all cells have been run. We recommend ensuring you have removed any extraneous cells & print statements, clearing all outputs, and using the Runtime --> Run all tool to make sure all output is update to date.\n",
        "2. Click on the menu 'File' --> 'Download' --> 'Download .py'.\n",
        "3. Click on the menu 'File' --> 'Download' --> 'Download .ipynb'.\n",
        "4. Download the notebook as a .pdf document. Make sure the output from Part 4 are captured so we can see how the loss, F1, & accuracy changes while training.\n",
        "5. **Double check to make sure you answered the cell text questions (sections 5.1 and 5.2)!** Every year some students forget to answer them and get points taken off.\n",
        "5. Upload all 3 files to Gradescope. Double check the files start with `CS4650_p1_*`, capitalization matters."
      ],
      "id": "mY8S9ZK9zuVs"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}